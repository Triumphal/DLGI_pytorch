{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入必要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlginet import FirstNet, SecondNet, LastNet\n",
    "from torch import nn\n",
    "from getdataset import MyDataset\n",
    "from torch.utils import data\n",
    "from display_utils import try_gpu\n",
    "from train import train\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:0\n",
      "cudnn is available\n",
      "[epoch 1]: training loss: 2197.343262, consuming time:63.7037 s\n",
      "[epoch 2]: training loss: 2273.950195, consuming time:61.2477 s\n",
      "[epoch 3]: training loss: 2360.506348, consuming time:61.2077 s\n",
      "[epoch 4]: training loss: 1448.526611, consuming time:60.9902 s\n",
      "[epoch 5]: training loss: 2057.532227, consuming time:61.0501 s\n",
      "[epoch 6]: training loss: 2196.205322, consuming time:61.5586 s\n",
      "[epoch 7]: training loss: 2071.926758, consuming time:60.9763 s\n",
      "[epoch 8]: training loss: 2464.172363, consuming time:61.1937 s\n",
      "[epoch 9]: training loss: 1890.813232, consuming time:61.3474 s\n",
      "[epoch 10]: training loss: 2228.011230, consuming time:60.8639 s\n",
      "[epoch 11]: training loss: 2029.998779, consuming time:60.6765 s\n",
      "[epoch 12]: training loss: 2468.105469, consuming time:60.8963 s\n",
      "[epoch 13]: training loss: 1862.981445, consuming time:60.9370 s\n",
      "[epoch 14]: training loss: 1868.372803, consuming time:61.2087 s\n",
      "[epoch 15]: training loss: 3434.546631, consuming time:61.2266 s\n",
      "[epoch 16]: training loss: 2163.534180, consuming time:60.6668 s\n",
      "[epoch 17]: training loss: 1915.433594, consuming time:60.7018 s\n",
      "[epoch 18]: training loss: 2061.200684, consuming time:60.5919 s\n",
      "[epoch 19]: training loss: 2291.442383, consuming time:60.6317 s\n",
      "[epoch 20]: training loss: 1883.797485, consuming time:61.3002 s\n",
      "[epoch 21]: training loss: 1811.301514, consuming time:60.8831 s\n",
      "[epoch 22]: training loss: 1933.806641, consuming time:60.7179 s\n",
      "[epoch 23]: training loss: 2352.295898, consuming time:61.0011 s\n",
      "[epoch 24]: training loss: 1596.071045, consuming time:60.7428 s\n",
      "[epoch 25]: training loss: 1856.378418, consuming time:60.6928 s\n",
      "[epoch 26]: training loss: 1643.112793, consuming time:61.2155 s\n",
      "[epoch 27]: training loss: 1794.498901, consuming time:61.1791 s\n",
      "[epoch 28]: training loss: 1956.185425, consuming time:60.5544 s\n",
      "[epoch 29]: training loss: 1612.181152, consuming time:60.9943 s\n",
      "[epoch 30]: training loss: 2003.999512, consuming time:60.3826 s\n",
      "[epoch 31]: training loss: 1415.856567, consuming time:60.7716 s\n",
      "[epoch 32]: training loss: 1485.268311, consuming time:61.2812 s\n",
      "[epoch 33]: training loss: 1485.936401, consuming time:60.8731 s\n",
      "[epoch 34]: training loss: 1369.821777, consuming time:60.1013 s\n",
      "[epoch 35]: training loss: 1895.949219, consuming time:60.7854 s\n",
      "[epoch 36]: training loss: 1501.330078, consuming time:60.5932 s\n",
      "[epoch 37]: training loss: 1734.068848, consuming time:60.8296 s\n",
      "[epoch 38]: training loss: 1708.881592, consuming time:61.2558 s\n",
      "[epoch 39]: training loss: 2422.360596, consuming time:60.7069 s\n",
      "[epoch 40]: training loss: 1478.559326, consuming time:60.5813 s\n",
      "[epoch 41]: training loss: 1627.530029, consuming time:60.9265 s\n",
      "[epoch 42]: training loss: 2024.869141, consuming time:60.6532 s\n",
      "[epoch 43]: training loss: 1712.350098, consuming time:61.1587 s\n",
      "[epoch 44]: training loss: 1421.701416, consuming time:61.1208 s\n",
      "[epoch 45]: training loss: 1488.419434, consuming time:60.8064 s\n",
      "[epoch 46]: training loss: 1796.479248, consuming time:60.6021 s\n",
      "[epoch 47]: training loss: 1416.593750, consuming time:60.8916 s\n",
      "[epoch 48]: training loss: 1705.032227, consuming time:60.5608 s\n",
      "[epoch 49]: training loss: 2042.532471, consuming time:60.9577 s\n",
      "[epoch 50]: training loss: 1618.364990, consuming time:61.2204 s\n",
      "[epoch 51]: training loss: 900.894775, consuming time:60.8983 s\n",
      "[epoch 52]: training loss: 1467.449219, consuming time:60.2030 s\n",
      "[epoch 53]: training loss: 1601.593872, consuming time:60.6261 s\n",
      "[epoch 54]: training loss: 1654.247803, consuming time:60.8188 s\n",
      "[epoch 55]: training loss: 1688.935913, consuming time:61.2338 s\n",
      "[epoch 56]: training loss: 1554.084351, consuming time:61.3029 s\n",
      "[epoch 57]: training loss: 1390.829224, consuming time:60.9735 s\n",
      "[epoch 58]: training loss: 1141.902588, consuming time:60.7990 s\n",
      "[epoch 59]: training loss: 1599.437134, consuming time:60.8514 s\n",
      "[epoch 60]: training loss: 1700.006836, consuming time:60.8347 s\n",
      "[epoch 61]: training loss: 1538.171387, consuming time:60.9866 s\n",
      "[epoch 62]: training loss: 1348.087891, consuming time:61.2659 s\n",
      "[epoch 63]: training loss: 1821.141602, consuming time:60.7681 s\n",
      "[epoch 64]: training loss: 1802.368652, consuming time:60.3532 s\n",
      "[epoch 65]: training loss: 1609.942871, consuming time:60.6810 s\n",
      "[epoch 66]: training loss: 1506.086670, consuming time:60.6253 s\n",
      "[epoch 67]: training loss: 1328.677246, consuming time:61.0965 s\n",
      "[epoch 68]: training loss: 1621.937378, consuming time:60.8693 s\n",
      "[epoch 69]: training loss: 1487.065918, consuming time:60.6589 s\n",
      "[epoch 70]: training loss: 1851.077759, consuming time:60.3945 s\n",
      "[epoch 71]: training loss: 1917.051514, consuming time:60.7964 s\n",
      "[epoch 72]: training loss: 1648.536011, consuming time:60.6612 s\n",
      "[epoch 73]: training loss: 1593.111816, consuming time:61.0795 s\n",
      "[epoch 74]: training loss: 1547.621826, consuming time:61.0519 s\n",
      "[epoch 75]: training loss: 1375.031982, consuming time:60.7619 s\n",
      "[epoch 76]: training loss: 1447.408691, consuming time:60.5290 s\n",
      "[epoch 77]: training loss: 1435.031982, consuming time:60.7941 s\n",
      "[epoch 78]: training loss: 1662.122070, consuming time:60.7460 s\n",
      "[epoch 79]: training loss: 1584.947998, consuming time:61.0134 s\n",
      "[epoch 80]: training loss: 1478.583862, consuming time:61.1192 s\n",
      "[epoch 81]: training loss: 1807.455933, consuming time:60.6879 s\n",
      "[epoch 82]: training loss: 1887.221924, consuming time:60.5747 s\n",
      "[epoch 83]: training loss: 1624.966553, consuming time:60.2838 s\n",
      "[epoch 84]: training loss: 1448.660522, consuming time:60.7428 s\n",
      "[epoch 85]: training loss: 1396.009277, consuming time:61.2997 s\n",
      "[epoch 86]: training loss: 1943.637329, consuming time:60.8648 s\n",
      "[epoch 87]: training loss: 1820.984619, consuming time:60.6331 s\n",
      "[epoch 88]: training loss: 1720.725586, consuming time:60.5553 s\n",
      "[epoch 89]: training loss: 1494.570801, consuming time:60.7637 s\n",
      "[epoch 90]: training loss: 1385.463257, consuming time:60.9531 s\n",
      "[epoch 91]: training loss: 1408.661499, consuming time:61.0886 s\n",
      "[epoch 92]: training loss: 1536.077026, consuming time:60.8448 s\n",
      "[epoch 93]: training loss: 1725.492920, consuming time:60.5704 s\n",
      "[epoch 94]: training loss: 1294.856079, consuming time:60.7814 s\n",
      "[epoch 95]: training loss: 1640.024414, consuming time:60.8830 s\n",
      "[epoch 96]: training loss: 1767.054321, consuming time:60.9448 s\n",
      "[epoch 97]: training loss: 1501.716553, consuming time:61.0647 s\n",
      "[epoch 98]: training loss: 1709.388306, consuming time:60.9379 s\n",
      "[epoch 99]: training loss: 1737.025635, consuming time:60.7883 s\n",
      "[epoch 100]: training loss: 1540.857788, consuming time:60.6744 s\n",
      "[epoch 101]: training loss: 1398.503906, consuming time:60.8110 s\n",
      "[epoch 102]: training loss: 1578.825562, consuming time:60.8595 s\n",
      "[epoch 103]: training loss: 1665.004150, consuming time:60.9249 s\n",
      "[epoch 104]: training loss: 1418.145874, consuming time:60.9833 s\n",
      "[epoch 105]: training loss: 1400.367065, consuming time:60.1926 s\n",
      "[epoch 106]: training loss: 1501.506348, consuming time:60.6849 s\n",
      "[epoch 107]: training loss: 1539.752075, consuming time:60.7735 s\n",
      "[epoch 108]: training loss: 1392.349731, consuming time:60.8460 s\n",
      "[epoch 109]: training loss: 1496.130371, consuming time:61.0591 s\n",
      "[epoch 110]: training loss: 1211.196533, consuming time:61.0379 s\n",
      "[epoch 111]: training loss: 1698.839844, consuming time:60.3853 s\n",
      "[epoch 112]: training loss: 1575.637451, consuming time:60.8850 s\n",
      "[epoch 113]: training loss: 1350.532593, consuming time:61.2809 s\n",
      "[epoch 114]: training loss: 1495.809814, consuming time:60.8909 s\n",
      "[epoch 115]: training loss: 1575.249756, consuming time:61.1983 s\n",
      "[epoch 116]: training loss: 1283.475220, consuming time:60.9296 s\n",
      "[epoch 117]: training loss: 1249.408569, consuming time:60.6060 s\n",
      "[epoch 118]: training loss: 1454.276855, consuming time:60.7657 s\n",
      "[epoch 119]: training loss: 1701.785645, consuming time:60.8277 s\n",
      "[epoch 120]: training loss: 1317.733765, consuming time:60.6987 s\n",
      "[epoch 121]: training loss: 1526.899658, consuming time:61.1269 s\n",
      "[epoch 122]: training loss: 1736.913574, consuming time:60.7389 s\n",
      "[epoch 123]: training loss: 1607.818115, consuming time:60.6566 s\n",
      "[epoch 124]: training loss: 1452.794067, consuming time:60.7642 s\n",
      "[epoch 125]: training loss: 1271.275391, consuming time:60.7325 s\n",
      "[epoch 126]: training loss: 1463.526367, consuming time:60.9048 s\n",
      "[epoch 127]: training loss: 1201.744385, consuming time:61.2223 s\n",
      "[epoch 128]: training loss: 1189.135254, consuming time:60.8872 s\n",
      "[epoch 129]: training loss: 1734.156616, consuming time:60.4862 s\n",
      "[epoch 130]: training loss: 1225.041504, consuming time:60.7354 s\n",
      "[epoch 131]: training loss: 1501.483154, consuming time:60.2932 s\n",
      "[epoch 132]: training loss: 1496.324341, consuming time:60.6026 s\n",
      "[epoch 133]: training loss: 1628.702759, consuming time:60.9579 s\n",
      "[epoch 134]: training loss: 1465.664185, consuming time:60.8262 s\n",
      "[epoch 135]: training loss: 1249.412964, consuming time:60.2292 s\n",
      "[epoch 136]: training loss: 1138.072510, consuming time:60.5418 s\n",
      "[epoch 137]: training loss: 1704.483521, consuming time:60.7056 s\n",
      "[epoch 138]: training loss: 919.460144, consuming time:60.6194 s\n",
      "[epoch 139]: training loss: 1260.985229, consuming time:61.1590 s\n",
      "[epoch 140]: training loss: 1679.949585, consuming time:60.5993 s\n",
      "[epoch 141]: training loss: 1917.680664, consuming time:60.4476 s\n",
      "[epoch 142]: training loss: 1344.821289, consuming time:60.6845 s\n",
      "[epoch 143]: training loss: 1928.638672, consuming time:60.6277 s\n",
      "[epoch 144]: training loss: 1313.996704, consuming time:60.8120 s\n",
      "[epoch 145]: training loss: 1667.612427, consuming time:60.9699 s\n",
      "[epoch 146]: training loss: 1711.413330, consuming time:60.7153 s\n",
      "[epoch 147]: training loss: 1120.225830, consuming time:60.6287 s\n",
      "[epoch 148]: training loss: 1291.522705, consuming time:60.6451 s\n",
      "[epoch 149]: training loss: 1467.692871, consuming time:60.3701 s\n",
      "[epoch 150]: training loss: 1334.965576, consuming time:61.1712 s\n",
      "[epoch 151]: training loss: 1280.061890, consuming time:60.8551 s\n",
      "[epoch 152]: training loss: 1203.257080, consuming time:60.5475 s\n",
      "[epoch 153]: training loss: 1414.190674, consuming time:60.5345 s\n",
      "[epoch 154]: training loss: 1317.549805, consuming time:60.8324 s\n",
      "[epoch 155]: training loss: 1164.076904, consuming time:61.7693 s\n",
      "[epoch 156]: training loss: 1231.866699, consuming time:61.7681 s\n",
      "[epoch 157]: training loss: 1272.759644, consuming time:65.8560 s\n",
      "[epoch 158]: training loss: 1343.430664, consuming time:64.0024 s\n",
      "[epoch 159]: training loss: 1503.960571, consuming time:64.0384 s\n",
      "[epoch 160]: training loss: 1271.873779, consuming time:64.9061 s\n",
      "[epoch 161]: training loss: 1266.240967, consuming time:64.4843 s\n",
      "[epoch 162]: training loss: 1218.874878, consuming time:63.4976 s\n",
      "[epoch 163]: training loss: 1672.814697, consuming time:63.9529 s\n",
      "[epoch 164]: training loss: 1546.597900, consuming time:68.3448 s\n",
      "[epoch 165]: training loss: 1412.976074, consuming time:66.1688 s\n",
      "[epoch 166]: training loss: 1553.269775, consuming time:63.5790 s\n",
      "[epoch 167]: training loss: 1379.634766, consuming time:63.3352 s\n",
      "[epoch 168]: training loss: 1431.141113, consuming time:63.0876 s\n",
      "[epoch 169]: training loss: 1343.235962, consuming time:63.0257 s\n",
      "[epoch 170]: training loss: 1342.621216, consuming time:63.4705 s\n",
      "[epoch 171]: training loss: 1859.233398, consuming time:64.4252 s\n",
      "[epoch 172]: training loss: 1327.108032, consuming time:63.9452 s\n",
      "[epoch 173]: training loss: 1502.855225, consuming time:63.3152 s\n",
      "[epoch 174]: training loss: 1176.584717, consuming time:64.1811 s\n",
      "[epoch 175]: training loss: 1503.160767, consuming time:63.8797 s\n",
      "[epoch 176]: training loss: 1313.192383, consuming time:65.1156 s\n",
      "[epoch 177]: training loss: 1300.468506, consuming time:68.5393 s\n",
      "[epoch 178]: training loss: 1831.706055, consuming time:64.7376 s\n",
      "[epoch 179]: training loss: 1516.397583, consuming time:63.7355 s\n",
      "[epoch 180]: training loss: 854.728882, consuming time:64.2534 s\n",
      "[epoch 181]: training loss: 1585.145264, consuming time:62.4163 s\n",
      "[epoch 182]: training loss: 1274.064819, consuming time:60.8705 s\n",
      "[epoch 183]: training loss: 1744.362549, consuming time:61.3270 s\n",
      "[epoch 184]: training loss: 1542.925781, consuming time:60.8972 s\n",
      "[epoch 185]: training loss: 1576.921265, consuming time:60.6289 s\n",
      "[epoch 186]: training loss: 1444.556763, consuming time:60.4515 s\n",
      "[epoch 187]: training loss: 1484.889648, consuming time:60.7373 s\n",
      "[epoch 188]: training loss: 1092.361328, consuming time:60.6902 s\n",
      "[epoch 189]: training loss: 1490.569824, consuming time:60.3083 s\n",
      "[epoch 190]: training loss: 1510.627319, consuming time:60.7205 s\n",
      "[epoch 191]: training loss: 1396.368164, consuming time:60.9906 s\n",
      "[epoch 192]: training loss: 1560.302612, consuming time:60.7501 s\n",
      "[epoch 193]: training loss: 1383.399902, consuming time:60.5190 s\n",
      "[epoch 194]: training loss: 1488.340576, consuming time:61.0461 s\n",
      "[epoch 195]: training loss: 1550.568848, consuming time:60.9176 s\n",
      "[epoch 196]: training loss: 1316.284424, consuming time:60.7824 s\n",
      "[epoch 197]: training loss: 1517.386963, consuming time:60.8197 s\n",
      "[epoch 198]: training loss: 1538.956055, consuming time:61.0540 s\n",
      "[epoch 199]: training loss: 1230.541260, consuming time:60.9413 s\n",
      "[epoch 200]: training loss: 1423.543457, consuming time:61.2746 s\n",
      "[epoch 201]: training loss: 1340.782715, consuming time:60.8499 s\n",
      "[epoch 202]: training loss: 1467.188477, consuming time:62.5715 s\n",
      "[epoch 203]: training loss: 1075.983765, consuming time:61.4876 s\n",
      "[epoch 204]: training loss: 1428.638184, consuming time:61.3261 s\n",
      "[epoch 205]: training loss: 1428.134033, consuming time:60.8990 s\n",
      "[epoch 206]: training loss: 1047.515869, consuming time:61.2597 s\n",
      "[epoch 207]: training loss: 1401.849854, consuming time:61.1549 s\n",
      "[epoch 208]: training loss: 1310.752808, consuming time:60.8287 s\n",
      "[epoch 209]: training loss: 1187.868164, consuming time:60.3952 s\n",
      "[epoch 210]: training loss: 1196.085693, consuming time:60.6086 s\n",
      "[epoch 211]: training loss: 1047.831787, consuming time:61.1202 s\n",
      "[epoch 212]: training loss: 1620.977295, consuming time:60.9245 s\n",
      "[epoch 213]: training loss: 1398.861572, consuming time:60.8770 s\n",
      "[epoch 214]: training loss: 1272.182251, consuming time:60.8939 s\n",
      "[epoch 215]: training loss: 1525.848145, consuming time:60.5619 s\n",
      "[epoch 216]: training loss: 1152.452881, consuming time:60.6678 s\n",
      "[epoch 217]: training loss: 1396.021851, consuming time:60.6084 s\n",
      "[epoch 218]: training loss: 1424.091797, consuming time:60.8966 s\n",
      "[epoch 219]: training loss: 1402.168091, consuming time:61.0287 s\n",
      "[epoch 220]: training loss: 1519.575684, consuming time:61.0690 s\n",
      "[epoch 221]: training loss: 1397.411621, consuming time:60.8621 s\n",
      "[epoch 222]: training loss: 1281.942383, consuming time:60.8282 s\n",
      "[epoch 223]: training loss: 1367.832764, consuming time:62.2269 s\n",
      "[epoch 224]: training loss: 1499.141357, consuming time:62.1746 s\n",
      "[epoch 225]: training loss: 1136.884766, consuming time:61.9994 s\n",
      "[epoch 226]: training loss: 1040.074097, consuming time:61.0780 s\n",
      "[epoch 227]: training loss: 1400.093750, consuming time:61.1641 s\n",
      "[epoch 228]: training loss: 1381.372803, consuming time:60.7227 s\n",
      "[epoch 229]: training loss: 1459.945312, consuming time:60.6850 s\n",
      "[epoch 230]: training loss: 1453.882812, consuming time:61.1094 s\n",
      "[epoch 231]: training loss: 1106.021606, consuming time:60.9817 s\n",
      "[epoch 232]: training loss: 1083.163818, consuming time:61.1092 s\n",
      "[epoch 233]: training loss: 1288.839478, consuming time:60.8604 s\n",
      "[epoch 234]: training loss: 1678.751953, consuming time:60.9740 s\n",
      "[epoch 235]: training loss: 1616.899170, consuming time:61.5366 s\n",
      "[epoch 236]: training loss: 1274.502319, consuming time:60.6231 s\n",
      "[epoch 237]: training loss: 1542.911621, consuming time:60.9944 s\n",
      "[epoch 238]: training loss: 1782.090576, consuming time:61.2018 s\n",
      "[epoch 239]: training loss: 1382.083740, consuming time:61.4954 s\n",
      "[epoch 240]: training loss: 1266.032593, consuming time:60.7381 s\n",
      "[epoch 241]: training loss: 1334.008179, consuming time:60.5936 s\n",
      "[epoch 242]: training loss: 1215.614746, consuming time:60.8289 s\n",
      "[epoch 243]: training loss: 1049.197754, consuming time:60.8387 s\n",
      "[epoch 244]: training loss: 1555.680664, consuming time:60.9982 s\n",
      "[epoch 245]: training loss: 1093.908813, consuming time:60.6894 s\n",
      "[epoch 246]: training loss: 1174.736206, consuming time:60.7149 s\n",
      "[epoch 247]: training loss: 922.785400, consuming time:60.6295 s\n",
      "[epoch 248]: training loss: 1301.691284, consuming time:60.4180 s\n",
      "[epoch 249]: training loss: 1511.369385, consuming time:60.7571 s\n",
      "[epoch 250]: training loss: 1376.856567, consuming time:60.9427 s\n",
      "[epoch 251]: training loss: 1093.399414, consuming time:59.9964 s\n",
      "[epoch 252]: training loss: 1061.856689, consuming time:60.8505 s\n",
      "[epoch 253]: training loss: 1625.804810, consuming time:60.5618 s\n",
      "[epoch 254]: training loss: 1005.799805, consuming time:61.0498 s\n",
      "[epoch 255]: training loss: 1226.515747, consuming time:61.1026 s\n",
      "[epoch 256]: training loss: 1367.433838, consuming time:61.1695 s\n",
      "[epoch 257]: training loss: 1063.548584, consuming time:61.1710 s\n",
      "[epoch 258]: training loss: 1123.854492, consuming time:62.4760 s\n",
      "[epoch 259]: training loss: 1484.123291, consuming time:62.3397 s\n",
      "[epoch 260]: training loss: 834.223145, consuming time:62.1825 s\n",
      "[epoch 261]: training loss: 1291.052734, consuming time:61.5702 s\n",
      "[epoch 262]: training loss: 1512.866577, consuming time:61.0703 s\n",
      "[epoch 263]: training loss: 1203.701904, consuming time:60.1591 s\n",
      "[epoch 264]: training loss: 1226.812134, consuming time:60.6900 s\n",
      "[epoch 265]: training loss: 1378.861938, consuming time:60.5019 s\n",
      "[epoch 266]: training loss: 1491.399048, consuming time:60.3174 s\n",
      "[epoch 267]: training loss: 1467.013550, consuming time:60.8599 s\n",
      "[epoch 268]: training loss: 1374.182739, consuming time:60.6402 s\n",
      "[epoch 269]: training loss: 1315.215820, consuming time:60.0962 s\n",
      "[epoch 270]: training loss: 1143.480713, consuming time:60.7817 s\n",
      "[epoch 271]: training loss: 1251.163208, consuming time:62.9529 s\n",
      "[epoch 272]: training loss: 1142.790039, consuming time:61.7464 s\n",
      "[epoch 273]: training loss: 1171.268555, consuming time:61.1166 s\n",
      "[epoch 274]: training loss: 1243.271118, consuming time:60.6717 s\n",
      "[epoch 275]: training loss: 1305.541382, consuming time:60.6472 s\n",
      "[epoch 276]: training loss: 1002.221313, consuming time:61.0144 s\n",
      "[epoch 277]: training loss: 1150.934448, consuming time:61.4776 s\n",
      "[epoch 278]: training loss: 1441.766968, consuming time:63.2036 s\n",
      "[epoch 279]: training loss: 1210.899658, consuming time:61.8902 s\n",
      "[epoch 280]: training loss: 1670.076050, consuming time:61.0541 s\n",
      "[epoch 281]: training loss: 1128.160889, consuming time:60.7620 s\n",
      "[epoch 282]: training loss: 1261.624634, consuming time:60.8526 s\n",
      "[epoch 283]: training loss: 1456.682617, consuming time:60.8720 s\n",
      "[epoch 284]: training loss: 1114.679199, consuming time:60.9997 s\n",
      "[epoch 285]: training loss: 1459.952393, consuming time:60.8395 s\n",
      "[epoch 286]: training loss: 1135.712158, consuming time:60.7022 s\n",
      "[epoch 287]: training loss: 1264.199097, consuming time:60.6063 s\n",
      "[epoch 288]: training loss: 1227.671021, consuming time:62.2843 s\n",
      "[epoch 289]: training loss: 1094.761841, consuming time:61.7642 s\n",
      "[epoch 290]: training loss: 1241.894287, consuming time:60.8973 s\n",
      "[epoch 291]: training loss: 1431.951904, consuming time:60.9084 s\n",
      "[epoch 292]: training loss: 1025.740967, consuming time:60.6327 s\n",
      "[epoch 293]: training loss: 1011.515564, consuming time:60.5827 s\n",
      "[epoch 294]: training loss: 1116.005371, consuming time:60.9991 s\n",
      "[epoch 295]: training loss: 1045.569946, consuming time:62.9228 s\n",
      "[epoch 296]: training loss: 1357.409302, consuming time:62.1847 s\n",
      "[epoch 297]: training loss: 1010.494080, consuming time:61.4373 s\n",
      "[epoch 298]: training loss: 1070.331787, consuming time:60.7962 s\n",
      "[epoch 299]: training loss: 1283.765869, consuming time:60.8002 s\n",
      "[epoch 300]: training loss: 1333.104004, consuming time:60.9797 s\n",
      "[epoch 301]: training loss: 1157.860840, consuming time:61.1113 s\n",
      "[epoch 302]: training loss: 948.795044, consuming time:61.0789 s\n",
      "[epoch 303]: training loss: 1402.281616, consuming time:60.9557 s\n",
      "[epoch 304]: training loss: 1399.177490, consuming time:60.6710 s\n",
      "[epoch 305]: training loss: 1264.285522, consuming time:60.5762 s\n",
      "[epoch 306]: training loss: 1488.268799, consuming time:61.0264 s\n",
      "[epoch 307]: training loss: 1307.205322, consuming time:60.9084 s\n",
      "[epoch 308]: training loss: 841.396851, consuming time:60.8877 s\n",
      "[epoch 309]: training loss: 1292.409180, consuming time:60.2724 s\n",
      "[epoch 310]: training loss: 1355.542480, consuming time:60.7150 s\n",
      "[epoch 311]: training loss: 1271.062256, consuming time:60.0828 s\n",
      "[epoch 312]: training loss: 1015.004028, consuming time:60.7213 s\n",
      "[epoch 313]: training loss: 1121.500732, consuming time:62.0726 s\n",
      "[epoch 314]: training loss: 1059.518555, consuming time:61.1097 s\n",
      "[epoch 315]: training loss: 1239.104736, consuming time:60.7771 s\n",
      "[epoch 316]: training loss: 1311.178467, consuming time:60.7038 s\n",
      "[epoch 317]: training loss: 1318.687256, consuming time:61.2931 s\n",
      "[epoch 318]: training loss: 1155.439941, consuming time:63.5481 s\n",
      "[epoch 319]: training loss: 1141.222168, consuming time:61.1790 s\n",
      "[epoch 320]: training loss: 1383.085205, consuming time:61.1792 s\n",
      "[epoch 321]: training loss: 1425.796631, consuming time:62.6340 s\n",
      "[epoch 322]: training loss: 1254.978638, consuming time:60.8111 s\n",
      "[epoch 323]: training loss: 1182.655029, consuming time:60.4133 s\n",
      "[epoch 324]: training loss: 1267.133179, consuming time:60.4747 s\n",
      "[epoch 325]: training loss: 1291.047729, consuming time:60.5503 s\n",
      "[epoch 326]: training loss: 1329.327881, consuming time:60.5996 s\n",
      "[epoch 327]: training loss: 1243.104004, consuming time:61.3059 s\n",
      "[epoch 328]: training loss: 1352.636475, consuming time:61.2800 s\n",
      "[epoch 329]: training loss: 1555.738525, consuming time:61.2201 s\n",
      "[epoch 330]: training loss: 1453.091064, consuming time:60.7331 s\n",
      "[epoch 331]: training loss: 1102.406494, consuming time:60.6597 s\n",
      "[epoch 332]: training loss: 1370.325439, consuming time:61.0586 s\n",
      "[epoch 333]: training loss: 1190.164062, consuming time:61.1008 s\n",
      "[epoch 334]: training loss: 1652.680420, consuming time:60.6134 s\n",
      "[epoch 335]: training loss: 1371.416016, consuming time:61.0354 s\n",
      "[epoch 336]: training loss: 1325.640625, consuming time:61.2509 s\n",
      "[epoch 337]: training loss: 1205.839111, consuming time:61.0179 s\n",
      "[epoch 338]: training loss: 1564.867188, consuming time:61.0651 s\n",
      "[epoch 339]: training loss: 1459.802246, consuming time:60.7038 s\n",
      "[epoch 340]: training loss: 1369.973755, consuming time:60.5195 s\n",
      "[epoch 341]: training loss: 1288.188721, consuming time:61.0982 s\n",
      "[epoch 342]: training loss: 1552.964111, consuming time:60.8742 s\n",
      "[epoch 343]: training loss: 1434.397705, consuming time:61.2055 s\n",
      "[epoch 344]: training loss: 1210.010986, consuming time:60.8122 s\n",
      "[epoch 345]: training loss: 1254.021240, consuming time:60.8847 s\n",
      "[epoch 346]: training loss: 1520.767944, consuming time:60.2223 s\n",
      "[epoch 347]: training loss: 821.942139, consuming time:60.6131 s\n",
      "[epoch 348]: training loss: 1343.026855, consuming time:60.7429 s\n",
      "[epoch 349]: training loss: 1280.643311, consuming time:60.6456 s\n",
      "[epoch 350]: training loss: 1730.181763, consuming time:60.5635 s\n",
      "[epoch 351]: training loss: 1132.957764, consuming time:60.3071 s\n",
      "[epoch 352]: training loss: 1402.873535, consuming time:60.3249 s\n",
      "[epoch 353]: training loss: 1371.139893, consuming time:60.5665 s\n",
      "[epoch 354]: training loss: 1251.237427, consuming time:60.8322 s\n",
      "[epoch 355]: training loss: 1059.632812, consuming time:61.1473 s\n",
      "[epoch 356]: training loss: 867.905945, consuming time:60.9077 s\n",
      "[epoch 357]: training loss: 1206.731567, consuming time:61.1479 s\n",
      "[epoch 358]: training loss: 1384.167725, consuming time:61.1743 s\n",
      "[epoch 359]: training loss: 1166.294556, consuming time:61.2358 s\n",
      "[epoch 360]: training loss: 1214.176880, consuming time:61.3503 s\n",
      "[epoch 361]: training loss: 1291.010254, consuming time:61.3328 s\n",
      "[epoch 362]: training loss: 1188.930908, consuming time:61.0559 s\n",
      "[epoch 363]: training loss: 1653.187256, consuming time:60.9946 s\n",
      "[epoch 364]: training loss: 1283.880371, consuming time:61.1871 s\n",
      "[epoch 365]: training loss: 1228.316162, consuming time:61.0887 s\n",
      "[epoch 366]: training loss: 1068.063477, consuming time:61.1544 s\n",
      "[epoch 367]: training loss: 1044.309082, consuming time:60.9494 s\n",
      "[epoch 368]: training loss: 1130.686279, consuming time:61.0001 s\n",
      "[epoch 369]: training loss: 1366.468262, consuming time:60.6723 s\n",
      "[epoch 370]: training loss: 1059.804443, consuming time:61.2280 s\n",
      "[epoch 371]: training loss: 1451.127441, consuming time:61.3568 s\n",
      "[epoch 372]: training loss: 1147.749512, consuming time:61.0723 s\n",
      "[epoch 373]: training loss: 825.248230, consuming time:60.8915 s\n",
      "[epoch 374]: training loss: 1371.853882, consuming time:60.5518 s\n",
      "[epoch 375]: training loss: 1423.365967, consuming time:60.8570 s\n",
      "[epoch 376]: training loss: 1279.004028, consuming time:60.8933 s\n",
      "[epoch 377]: training loss: 1617.425781, consuming time:61.2228 s\n",
      "[epoch 378]: training loss: 1299.423218, consuming time:61.0849 s\n",
      "[epoch 379]: training loss: 1048.889893, consuming time:61.1090 s\n",
      "[epoch 380]: training loss: 922.704956, consuming time:61.0975 s\n",
      "[epoch 381]: training loss: 1200.768066, consuming time:61.1697 s\n",
      "[epoch 382]: training loss: 1238.050049, consuming time:61.2901 s\n",
      "[epoch 383]: training loss: 1257.076050, consuming time:60.8212 s\n",
      "[epoch 384]: training loss: 968.244995, consuming time:61.2883 s\n",
      "[epoch 385]: training loss: 944.202087, consuming time:60.9175 s\n",
      "[epoch 386]: training loss: 1140.401123, consuming time:61.1119 s\n",
      "[epoch 387]: training loss: 1219.473877, consuming time:61.1058 s\n",
      "[epoch 388]: training loss: 1457.072632, consuming time:61.0805 s\n",
      "[epoch 389]: training loss: 1079.819824, consuming time:61.1634 s\n",
      "[epoch 390]: training loss: 1016.024414, consuming time:61.0006 s\n",
      "[epoch 391]: training loss: 1375.344482, consuming time:60.8305 s\n",
      "[epoch 392]: training loss: 1359.460815, consuming time:61.0399 s\n",
      "[epoch 393]: training loss: 1512.231201, consuming time:61.0936 s\n",
      "[epoch 394]: training loss: 1246.071655, consuming time:60.8266 s\n",
      "[epoch 395]: training loss: 1227.058838, consuming time:61.0518 s\n",
      "[epoch 396]: training loss: 1501.125244, consuming time:65.5822 s\n",
      "[epoch 397]: training loss: 1195.405029, consuming time:61.4653 s\n",
      "[epoch 398]: training loss: 1169.928711, consuming time:60.9007 s\n",
      "[epoch 399]: training loss: 1031.308105, consuming time:60.8760 s\n",
      "[epoch 400]: training loss: 1115.671631, consuming time:60.8826 s\n",
      "[epoch 401]: training loss: 1285.625732, consuming time:61.1673 s\n",
      "[epoch 402]: training loss: 1416.723633, consuming time:62.0017 s\n",
      "[epoch 403]: training loss: 1488.557617, consuming time:61.7418 s\n",
      "[epoch 404]: training loss: 1136.979004, consuming time:61.1330 s\n",
      "[epoch 405]: training loss: 1410.756836, consuming time:61.7000 s\n",
      "[epoch 406]: training loss: 1145.882812, consuming time:61.5416 s\n",
      "[epoch 407]: training loss: 1569.274902, consuming time:61.0916 s\n",
      "[epoch 408]: training loss: 1247.123535, consuming time:62.0659 s\n",
      "[epoch 409]: training loss: 1291.882812, consuming time:61.2757 s\n",
      "[epoch 410]: training loss: 1341.811279, consuming time:62.8190 s\n",
      "[epoch 411]: training loss: 1125.341309, consuming time:61.1735 s\n",
      "[epoch 412]: training loss: 1443.418701, consuming time:60.9903 s\n",
      "[epoch 413]: training loss: 1371.354980, consuming time:60.9212 s\n",
      "[epoch 414]: training loss: 1045.746826, consuming time:61.5063 s\n",
      "[epoch 415]: training loss: 1199.328979, consuming time:61.3327 s\n",
      "[epoch 416]: training loss: 1135.626465, consuming time:61.7644 s\n",
      "[epoch 417]: training loss: 798.543396, consuming time:61.3388 s\n",
      "[epoch 418]: training loss: 1070.684570, consuming time:62.1132 s\n",
      "[epoch 419]: training loss: 1080.324707, consuming time:61.9608 s\n",
      "[epoch 420]: training loss: 1750.221313, consuming time:62.0522 s\n",
      "[epoch 421]: training loss: 1239.495850, consuming time:61.0251 s\n",
      "[epoch 422]: training loss: 870.761475, consuming time:60.5327 s\n",
      "[epoch 423]: training loss: 1251.137939, consuming time:59.7241 s\n",
      "[epoch 424]: training loss: 1128.872803, consuming time:60.1526 s\n",
      "[epoch 425]: training loss: 1018.608093, consuming time:60.2497 s\n",
      "[epoch 426]: training loss: 1595.343750, consuming time:60.3489 s\n",
      "[epoch 427]: training loss: 1269.201416, consuming time:60.1981 s\n",
      "[epoch 428]: training loss: 1281.255005, consuming time:60.3112 s\n",
      "[epoch 429]: training loss: 1594.965454, consuming time:60.0561 s\n",
      "[epoch 430]: training loss: 1274.043457, consuming time:60.5071 s\n",
      "[epoch 431]: training loss: 1183.673340, consuming time:60.4779 s\n",
      "[epoch 432]: training loss: 1146.921875, consuming time:59.9700 s\n",
      "[epoch 433]: training loss: 1191.031738, consuming time:60.4599 s\n",
      "[epoch 434]: training loss: 1423.371826, consuming time:59.2422 s\n",
      "[epoch 435]: training loss: 1486.054932, consuming time:59.8850 s\n",
      "[epoch 436]: training loss: 1446.661377, consuming time:61.5474 s\n",
      "[epoch 437]: training loss: 1237.528809, consuming time:63.4258 s\n",
      "[epoch 438]: training loss: 1290.012939, consuming time:63.7010 s\n",
      "[epoch 439]: training loss: 1431.042358, consuming time:64.7585 s\n",
      "[epoch 440]: training loss: 1112.856934, consuming time:64.4695 s\n",
      "[epoch 441]: training loss: 1316.584473, consuming time:63.9951 s\n",
      "[epoch 442]: training loss: 1239.604370, consuming time:62.8263 s\n",
      "[epoch 443]: training loss: 925.772278, consuming time:61.2614 s\n",
      "[epoch 444]: training loss: 911.596558, consuming time:61.3991 s\n",
      "[epoch 445]: training loss: 977.957153, consuming time:61.2695 s\n",
      "[epoch 446]: training loss: 1064.038330, consuming time:61.2903 s\n",
      "[epoch 447]: training loss: 1130.269653, consuming time:61.5918 s\n",
      "[epoch 448]: training loss: 1074.206055, consuming time:61.6346 s\n",
      "[epoch 449]: training loss: 1219.630127, consuming time:61.2021 s\n",
      "[epoch 450]: training loss: 1304.655273, consuming time:61.1813 s\n",
      "[epoch 451]: training loss: 1246.765991, consuming time:61.1462 s\n",
      "[epoch 452]: training loss: 1165.331787, consuming time:63.0727 s\n",
      "[epoch 453]: training loss: 901.610840, consuming time:62.7735 s\n",
      "[epoch 454]: training loss: 1270.029053, consuming time:61.4702 s\n",
      "[epoch 455]: training loss: 973.944824, consuming time:61.6530 s\n",
      "[epoch 456]: training loss: 1226.370117, consuming time:61.6621 s\n",
      "[epoch 457]: training loss: 1063.070801, consuming time:61.6630 s\n",
      "[epoch 458]: training loss: 1057.606689, consuming time:61.5634 s\n",
      "[epoch 459]: training loss: 1817.989746, consuming time:61.6242 s\n",
      "[epoch 460]: training loss: 1336.205444, consuming time:61.5711 s\n",
      "[epoch 461]: training loss: 846.232117, consuming time:61.6297 s\n",
      "[epoch 462]: training loss: 1171.415894, consuming time:61.8444 s\n",
      "[epoch 463]: training loss: 1207.171997, consuming time:61.6195 s\n",
      "[epoch 464]: training loss: 1019.233521, consuming time:61.6676 s\n",
      "[epoch 465]: training loss: 1379.741577, consuming time:61.4815 s\n",
      "[epoch 466]: training loss: 1305.257324, consuming time:61.6629 s\n",
      "[epoch 467]: training loss: 1141.556396, consuming time:61.6817 s\n",
      "[epoch 468]: training loss: 1505.264648, consuming time:61.6778 s\n",
      "[epoch 469]: training loss: 1416.015137, consuming time:61.3451 s\n",
      "[epoch 470]: training loss: 1221.871094, consuming time:61.6849 s\n",
      "[epoch 471]: training loss: 1137.641357, consuming time:61.6433 s\n",
      "[epoch 472]: training loss: 1282.552979, consuming time:61.6372 s\n",
      "[epoch 473]: training loss: 1250.590942, consuming time:61.5752 s\n",
      "[epoch 474]: training loss: 1222.798828, consuming time:61.5440 s\n",
      "[epoch 475]: training loss: 1134.431030, consuming time:61.5985 s\n",
      "[epoch 476]: training loss: 1254.911255, consuming time:61.5959 s\n",
      "[epoch 477]: training loss: 1126.590088, consuming time:61.4450 s\n",
      "[epoch 478]: training loss: 1037.986206, consuming time:61.6694 s\n",
      "[epoch 479]: training loss: 1076.524170, consuming time:61.6715 s\n",
      "[epoch 480]: training loss: 984.237915, consuming time:61.6940 s\n",
      "[epoch 481]: training loss: 1041.718262, consuming time:61.4765 s\n",
      "[epoch 482]: training loss: 1635.125488, consuming time:61.5799 s\n",
      "[epoch 483]: training loss: 1134.035156, consuming time:61.5912 s\n",
      "[epoch 484]: training loss: 1094.800293, consuming time:61.6113 s\n",
      "[epoch 485]: training loss: 1438.266602, consuming time:61.5854 s\n",
      "[epoch 486]: training loss: 1157.926758, consuming time:61.5701 s\n",
      "[epoch 487]: training loss: 1104.779053, consuming time:61.6009 s\n",
      "[epoch 488]: training loss: 929.045837, consuming time:61.6242 s\n",
      "[epoch 489]: training loss: 1638.091064, consuming time:61.4914 s\n",
      "[epoch 490]: training loss: 1142.363037, consuming time:61.7495 s\n",
      "[epoch 491]: training loss: 1181.190430, consuming time:61.7232 s\n",
      "[epoch 492]: training loss: 1188.679688, consuming time:61.4837 s\n",
      "[epoch 493]: training loss: 1237.499146, consuming time:61.5274 s\n",
      "[epoch 494]: training loss: 990.985657, consuming time:61.5168 s\n",
      "[epoch 495]: training loss: 1119.920166, consuming time:61.5488 s\n",
      "[epoch 496]: training loss: 1221.041260, consuming time:61.5377 s\n",
      "[epoch 497]: training loss: 1295.685913, consuming time:61.6869 s\n",
      "[epoch 498]: training loss: 1093.784424, consuming time:61.5806 s\n",
      "[epoch 499]: training loss: 1207.415039, consuming time:61.6352 s\n",
      "[epoch 500]: training loss: 1255.830811, consuming time:61.5520 s\n",
      "[epoch 501]: training loss: 1177.687744, consuming time:61.5795 s\n",
      "[epoch 502]: training loss: 1398.080566, consuming time:61.6064 s\n",
      "[epoch 503]: training loss: 1181.212402, consuming time:61.5421 s\n",
      "[epoch 504]: training loss: 1223.075317, consuming time:61.4326 s\n",
      "[epoch 505]: training loss: 1267.825439, consuming time:61.5523 s\n",
      "[epoch 506]: training loss: 1306.947754, consuming time:61.4767 s\n",
      "[epoch 507]: training loss: 896.128052, consuming time:61.8113 s\n",
      "[epoch 508]: training loss: 1235.999390, consuming time:61.7022 s\n",
      "[epoch 509]: training loss: 1069.346924, consuming time:61.5301 s\n",
      "[epoch 510]: training loss: 968.569092, consuming time:61.5863 s\n",
      "[epoch 511]: training loss: 1191.683716, consuming time:61.6408 s\n",
      "[epoch 512]: training loss: 1008.522949, consuming time:61.7508 s\n",
      "[epoch 513]: training loss: 1230.185547, consuming time:61.7453 s\n",
      "[epoch 514]: training loss: 1211.177246, consuming time:61.6473 s\n",
      "[epoch 515]: training loss: 1163.668579, consuming time:61.6305 s\n",
      "[epoch 516]: training loss: 1042.660156, consuming time:61.5811 s\n",
      "[epoch 517]: training loss: 1191.415771, consuming time:61.7430 s\n",
      "[epoch 518]: training loss: 982.029846, consuming time:61.6278 s\n",
      "[epoch 519]: training loss: 1112.138672, consuming time:61.5482 s\n",
      "[epoch 520]: training loss: 1603.229492, consuming time:61.6461 s\n",
      "[epoch 521]: training loss: 1148.411133, consuming time:61.4814 s\n",
      "[epoch 522]: training loss: 1198.833252, consuming time:61.6378 s\n",
      "[epoch 523]: training loss: 1238.383667, consuming time:61.5612 s\n",
      "[epoch 524]: training loss: 1419.765381, consuming time:61.6297 s\n",
      "[epoch 525]: training loss: 1119.276733, consuming time:61.7045 s\n",
      "[epoch 526]: training loss: 1033.218018, consuming time:61.6775 s\n",
      "[epoch 527]: training loss: 1186.561646, consuming time:61.4491 s\n",
      "[epoch 528]: training loss: 1210.841431, consuming time:61.6572 s\n",
      "[epoch 529]: training loss: 1366.881592, consuming time:61.5406 s\n",
      "[epoch 530]: training loss: 1044.977783, consuming time:61.6859 s\n",
      "[epoch 531]: training loss: 943.145630, consuming time:61.8308 s\n",
      "[epoch 532]: training loss: 882.080322, consuming time:61.6962 s\n",
      "[epoch 533]: training loss: 1169.615723, consuming time:61.4724 s\n",
      "[epoch 534]: training loss: 1380.125000, consuming time:61.6855 s\n",
      "[epoch 535]: training loss: 1275.010742, consuming time:61.4722 s\n",
      "[epoch 536]: training loss: 1114.060547, consuming time:61.7369 s\n",
      "[epoch 537]: training loss: 1214.068115, consuming time:61.8453 s\n",
      "[epoch 538]: training loss: 1114.548096, consuming time:61.4491 s\n",
      "[epoch 539]: training loss: 1465.326416, consuming time:61.5617 s\n",
      "[epoch 540]: training loss: 1370.539307, consuming time:61.4419 s\n",
      "[epoch 541]: training loss: 1164.054443, consuming time:61.7148 s\n",
      "[epoch 542]: training loss: 1171.301758, consuming time:62.5474 s\n",
      "[epoch 543]: training loss: 1009.791138, consuming time:62.5293 s\n",
      "[epoch 544]: training loss: 1456.924805, consuming time:64.2268 s\n",
      "[epoch 545]: training loss: 1172.401855, consuming time:60.8494 s\n",
      "[epoch 546]: training loss: 1187.192261, consuming time:61.2680 s\n",
      "[epoch 547]: training loss: 1550.331055, consuming time:60.8880 s\n",
      "[epoch 548]: training loss: 1538.807129, consuming time:61.2954 s\n",
      "[epoch 549]: training loss: 1474.900879, consuming time:61.6741 s\n",
      "[epoch 550]: training loss: 1077.839355, consuming time:61.9159 s\n",
      "[epoch 551]: training loss: 1202.255737, consuming time:61.1446 s\n",
      "[epoch 552]: training loss: 1427.989868, consuming time:61.1594 s\n",
      "[epoch 553]: training loss: 1124.146851, consuming time:61.1635 s\n",
      "[epoch 554]: training loss: 940.514893, consuming time:61.1718 s\n",
      "[epoch 555]: training loss: 1410.772827, consuming time:61.2285 s\n",
      "[epoch 556]: training loss: 910.968933, consuming time:61.4321 s\n",
      "[epoch 557]: training loss: 1027.976196, consuming time:61.6243 s\n",
      "[epoch 558]: training loss: 1386.344238, consuming time:61.6667 s\n",
      "[epoch 559]: training loss: 830.845947, consuming time:61.0340 s\n",
      "[epoch 560]: training loss: 1359.776367, consuming time:62.4664 s\n",
      "[epoch 561]: training loss: 1086.251953, consuming time:61.7269 s\n",
      "[epoch 562]: training loss: 1265.924683, consuming time:62.5992 s\n",
      "[epoch 563]: training loss: 1444.018921, consuming time:62.1944 s\n",
      "[epoch 564]: training loss: 1080.683594, consuming time:63.1503 s\n",
      "[epoch 565]: training loss: 1189.429810, consuming time:62.5657 s\n",
      "[epoch 566]: training loss: 807.385742, consuming time:62.4971 s\n",
      "[epoch 567]: training loss: 708.894775, consuming time:61.6801 s\n",
      "[epoch 568]: training loss: 1052.459595, consuming time:61.3220 s\n",
      "[epoch 569]: training loss: 1033.125000, consuming time:62.9772 s\n",
      "[epoch 570]: training loss: 1137.292969, consuming time:61.2373 s\n",
      "[epoch 571]: training loss: 1250.313477, consuming time:60.9993 s\n",
      "[epoch 572]: training loss: 1465.687744, consuming time:61.9183 s\n",
      "[epoch 573]: training loss: 1121.010986, consuming time:61.2339 s\n",
      "[epoch 574]: training loss: 1016.266663, consuming time:61.4706 s\n",
      "[epoch 575]: training loss: 885.984253, consuming time:61.3957 s\n",
      "[epoch 576]: training loss: 861.417603, consuming time:61.4113 s\n",
      "[epoch 577]: training loss: 1275.309204, consuming time:61.2562 s\n",
      "[epoch 578]: training loss: 1342.962891, consuming time:60.8749 s\n",
      "[epoch 579]: training loss: 923.336548, consuming time:61.0925 s\n",
      "[epoch 580]: training loss: 1251.980713, consuming time:60.9733 s\n",
      "[epoch 581]: training loss: 1769.295166, consuming time:61.1504 s\n",
      "[epoch 582]: training loss: 904.791016, consuming time:60.9158 s\n",
      "[epoch 583]: training loss: 1028.062500, consuming time:60.8833 s\n",
      "[epoch 584]: training loss: 1230.156006, consuming time:60.3298 s\n",
      "[epoch 585]: training loss: 1197.093018, consuming time:60.3847 s\n",
      "[epoch 586]: training loss: 1245.265015, consuming time:60.3378 s\n",
      "[epoch 587]: training loss: 1014.535828, consuming time:61.0109 s\n",
      "[epoch 588]: training loss: 1399.488770, consuming time:61.1335 s\n",
      "[epoch 589]: training loss: 1010.551575, consuming time:60.7147 s\n",
      "[epoch 590]: training loss: 908.265747, consuming time:61.1355 s\n",
      "[epoch 591]: training loss: 1054.205322, consuming time:61.1011 s\n",
      "[epoch 592]: training loss: 1242.110352, consuming time:60.5130 s\n",
      "[epoch 593]: training loss: 1428.317627, consuming time:60.4869 s\n",
      "[epoch 594]: training loss: 1073.607666, consuming time:60.5267 s\n",
      "[epoch 595]: training loss: 1325.590820, consuming time:60.4346 s\n",
      "[epoch 596]: training loss: 1216.282104, consuming time:60.0989 s\n",
      "[epoch 597]: training loss: 1655.673218, consuming time:60.0421 s\n",
      "[epoch 598]: training loss: 1007.538025, consuming time:60.1431 s\n",
      "[epoch 599]: training loss: 1050.380981, consuming time:61.2346 s\n",
      "[epoch 600]: training loss: 1053.970093, consuming time:61.7872 s\n",
      "[epoch 601]: training loss: 1420.861328, consuming time:61.3596 s\n",
      "[epoch 602]: training loss: 1087.975952, consuming time:61.1628 s\n",
      "[epoch 603]: training loss: 1316.241455, consuming time:60.8321 s\n",
      "[epoch 604]: training loss: 1096.319458, consuming time:60.4398 s\n",
      "[epoch 605]: training loss: 930.485718, consuming time:60.9821 s\n",
      "[epoch 606]: training loss: 1226.045410, consuming time:61.6522 s\n",
      "[epoch 607]: training loss: 1236.344727, consuming time:61.2914 s\n",
      "[epoch 608]: training loss: 1071.037231, consuming time:61.9685 s\n",
      "[epoch 609]: training loss: 1262.136108, consuming time:61.7735 s\n",
      "[epoch 610]: training loss: 1472.726685, consuming time:61.2245 s\n",
      "[epoch 611]: training loss: 1151.474854, consuming time:61.8274 s\n",
      "[epoch 612]: training loss: 854.159180, consuming time:60.8223 s\n",
      "[epoch 613]: training loss: 1186.074219, consuming time:61.0731 s\n",
      "[epoch 614]: training loss: 1317.697266, consuming time:61.0850 s\n",
      "[epoch 615]: training loss: 991.818359, consuming time:60.3146 s\n",
      "[epoch 616]: training loss: 1342.304443, consuming time:60.9096 s\n",
      "[epoch 617]: training loss: 1159.487061, consuming time:61.5305 s\n",
      "[epoch 618]: training loss: 1577.364014, consuming time:60.6161 s\n",
      "[epoch 619]: training loss: 1349.253418, consuming time:61.2690 s\n",
      "[epoch 620]: training loss: 1199.466431, consuming time:60.8940 s\n",
      "[epoch 621]: training loss: 1154.891357, consuming time:61.4060 s\n",
      "[epoch 622]: training loss: 1331.375000, consuming time:62.8132 s\n",
      "[epoch 623]: training loss: 1290.382080, consuming time:63.0195 s\n",
      "[epoch 624]: training loss: 1180.505859, consuming time:63.5981 s\n",
      "[epoch 625]: training loss: 945.383667, consuming time:62.0395 s\n",
      "[epoch 626]: training loss: 1132.295166, consuming time:63.4953 s\n",
      "[epoch 627]: training loss: 1412.735596, consuming time:60.9603 s\n",
      "[epoch 628]: training loss: 1074.661621, consuming time:61.0295 s\n",
      "[epoch 629]: training loss: 1112.134644, consuming time:60.7675 s\n",
      "[epoch 630]: training loss: 1066.623047, consuming time:61.2554 s\n",
      "[epoch 631]: training loss: 651.108521, consuming time:60.9408 s\n",
      "[epoch 632]: training loss: 1098.229858, consuming time:60.6008 s\n",
      "[epoch 633]: training loss: 1029.531006, consuming time:60.8180 s\n",
      "[epoch 634]: training loss: 1485.124023, consuming time:60.8250 s\n",
      "[epoch 635]: training loss: 1260.780762, consuming time:60.7786 s\n",
      "[epoch 636]: training loss: 925.575012, consuming time:61.2851 s\n",
      "[epoch 637]: training loss: 1281.467529, consuming time:63.0477 s\n",
      "[epoch 638]: training loss: 1422.479492, consuming time:62.9278 s\n",
      "[epoch 639]: training loss: 1514.845215, consuming time:61.4754 s\n",
      "[epoch 640]: training loss: 1175.417480, consuming time:61.5170 s\n",
      "[epoch 641]: training loss: 1020.672607, consuming time:61.0230 s\n",
      "[epoch 642]: training loss: 1140.056274, consuming time:61.0134 s\n",
      "[epoch 643]: training loss: 1061.096436, consuming time:60.8921 s\n",
      "[epoch 644]: training loss: 893.553711, consuming time:62.3652 s\n",
      "[epoch 645]: training loss: 1380.920410, consuming time:61.7081 s\n",
      "[epoch 646]: training loss: 1348.352783, consuming time:61.0641 s\n",
      "[epoch 647]: training loss: 1087.933105, consuming time:60.8346 s\n",
      "[epoch 648]: training loss: 1361.661499, consuming time:60.9159 s\n",
      "[epoch 649]: training loss: 1285.597168, consuming time:61.0505 s\n",
      "[epoch 650]: training loss: 1188.083496, consuming time:60.2449 s\n",
      "[epoch 651]: training loss: 1260.984375, consuming time:60.1969 s\n",
      "[epoch 652]: training loss: 1118.015381, consuming time:61.0734 s\n",
      "[epoch 653]: training loss: 1035.510864, consuming time:60.3031 s\n",
      "[epoch 654]: training loss: 1080.199707, consuming time:60.5951 s\n",
      "[epoch 655]: training loss: 1265.649414, consuming time:60.5131 s\n",
      "[epoch 656]: training loss: 1293.525879, consuming time:60.3399 s\n",
      "[epoch 657]: training loss: 1431.126953, consuming time:60.1565 s\n",
      "[epoch 658]: training loss: 1187.467407, consuming time:60.0871 s\n",
      "[epoch 659]: training loss: 1137.990479, consuming time:60.6086 s\n",
      "[epoch 660]: training loss: 943.977966, consuming time:60.8115 s\n",
      "[epoch 661]: training loss: 1034.679443, consuming time:60.2380 s\n",
      "[epoch 662]: training loss: 1141.788452, consuming time:60.1343 s\n",
      "[epoch 663]: training loss: 813.732117, consuming time:59.9467 s\n",
      "[epoch 664]: training loss: 853.239868, consuming time:60.2388 s\n",
      "[epoch 665]: training loss: 1058.232910, consuming time:60.6647 s\n",
      "[epoch 666]: training loss: 1078.580566, consuming time:60.5768 s\n",
      "[epoch 667]: training loss: 1394.320557, consuming time:60.0271 s\n",
      "[epoch 668]: training loss: 1337.586426, consuming time:60.0388 s\n",
      "[epoch 669]: training loss: 1282.926514, consuming time:60.2386 s\n",
      "[epoch 670]: training loss: 1349.518433, consuming time:60.3458 s\n",
      "[epoch 671]: training loss: 1123.204834, consuming time:60.3547 s\n",
      "[epoch 672]: training loss: 862.099548, consuming time:60.7356 s\n",
      "[epoch 673]: training loss: 1082.038208, consuming time:59.9425 s\n",
      "[epoch 674]: training loss: 1003.454163, consuming time:60.0139 s\n",
      "[epoch 675]: training loss: 1372.125488, consuming time:60.1551 s\n",
      "[epoch 676]: training loss: 991.978149, consuming time:60.0110 s\n",
      "[epoch 677]: training loss: 1386.367432, consuming time:60.2676 s\n",
      "[epoch 678]: training loss: 1156.922729, consuming time:60.3670 s\n",
      "[epoch 679]: training loss: 1193.566895, consuming time:59.9031 s\n",
      "[epoch 680]: training loss: 1160.609741, consuming time:60.1263 s\n",
      "[epoch 681]: training loss: 1238.497559, consuming time:59.9732 s\n",
      "[epoch 682]: training loss: 1411.863403, consuming time:60.2157 s\n",
      "[epoch 683]: training loss: 1595.262451, consuming time:60.3778 s\n",
      "[epoch 684]: training loss: 1059.616211, consuming time:60.6951 s\n",
      "[epoch 685]: training loss: 1105.130859, consuming time:60.0160 s\n",
      "[epoch 686]: training loss: 1197.270508, consuming time:59.9958 s\n",
      "[epoch 687]: training loss: 788.987610, consuming time:60.0149 s\n",
      "[epoch 688]: training loss: 878.679138, consuming time:60.2162 s\n",
      "[epoch 689]: training loss: 1395.413574, consuming time:60.3120 s\n",
      "[epoch 690]: training loss: 1284.782593, consuming time:60.4850 s\n",
      "[epoch 691]: training loss: 785.652954, consuming time:60.0139 s\n",
      "[epoch 692]: training loss: 901.044067, consuming time:60.2001 s\n",
      "[epoch 693]: training loss: 1254.835449, consuming time:60.4369 s\n",
      "[epoch 694]: training loss: 1350.734375, consuming time:60.2349 s\n",
      "[epoch 695]: training loss: 979.619873, consuming time:60.3590 s\n",
      "[epoch 696]: training loss: 1195.300781, consuming time:60.5457 s\n",
      "[epoch 697]: training loss: 1209.920410, consuming time:60.1711 s\n",
      "[epoch 698]: training loss: 955.221069, consuming time:59.8940 s\n",
      "[epoch 699]: training loss: 1004.448853, consuming time:60.2114 s\n",
      "[epoch 700]: training loss: 1267.662842, consuming time:60.4984 s\n",
      "[epoch 701]: training loss: 818.630981, consuming time:60.4384 s\n",
      "[epoch 702]: training loss: 1141.598877, consuming time:60.3811 s\n",
      "[epoch 703]: training loss: 955.075378, consuming time:60.4940 s\n",
      "[epoch 704]: training loss: 1215.283447, consuming time:60.1503 s\n",
      "[epoch 705]: training loss: 1028.416382, consuming time:60.3394 s\n",
      "[epoch 706]: training loss: 1307.959229, consuming time:60.3610 s\n",
      "[epoch 707]: training loss: 899.570618, consuming time:60.5211 s\n",
      "[epoch 708]: training loss: 1281.939575, consuming time:60.3954 s\n",
      "[epoch 709]: training loss: 991.874512, consuming time:60.1961 s\n",
      "[epoch 710]: training loss: 1211.230347, consuming time:59.9918 s\n",
      "[epoch 711]: training loss: 925.563965, consuming time:59.9722 s\n",
      "[epoch 712]: training loss: 1104.683838, consuming time:60.3974 s\n",
      "[epoch 713]: training loss: 985.912598, consuming time:60.1937 s\n",
      "[epoch 714]: training loss: 932.582764, consuming time:60.4313 s\n",
      "[epoch 715]: training loss: 1069.214966, consuming time:60.3096 s\n",
      "[epoch 716]: training loss: 1236.660156, consuming time:59.8533 s\n",
      "[epoch 717]: training loss: 990.330811, consuming time:60.0339 s\n",
      "[epoch 718]: training loss: 1530.905029, consuming time:60.0953 s\n",
      "[epoch 719]: training loss: 1068.335938, consuming time:60.4593 s\n",
      "[epoch 720]: training loss: 1003.585815, consuming time:60.3312 s\n",
      "[epoch 721]: training loss: 872.459167, consuming time:59.9923 s\n",
      "[epoch 722]: training loss: 1060.967041, consuming time:59.7785 s\n",
      "[epoch 723]: training loss: 1438.121582, consuming time:60.2411 s\n",
      "[epoch 724]: training loss: 1552.103271, consuming time:60.2076 s\n",
      "[epoch 725]: training loss: 1047.811279, consuming time:60.3516 s\n",
      "[epoch 726]: training loss: 1191.860352, consuming time:60.5467 s\n",
      "[epoch 727]: training loss: 1151.777954, consuming time:60.2457 s\n",
      "[epoch 728]: training loss: 1469.907959, consuming time:59.8444 s\n",
      "[epoch 729]: training loss: 1098.334717, consuming time:59.9556 s\n",
      "[epoch 730]: training loss: 1364.922119, consuming time:60.0532 s\n",
      "[epoch 731]: training loss: 1176.395264, consuming time:60.1747 s\n",
      "[epoch 732]: training loss: 1036.147583, consuming time:60.2696 s\n",
      "[epoch 733]: training loss: 1199.137085, consuming time:60.2696 s\n",
      "[epoch 734]: training loss: 1078.072754, consuming time:59.6756 s\n",
      "[epoch 735]: training loss: 1519.014771, consuming time:60.2507 s\n",
      "[epoch 736]: training loss: 1227.335938, consuming time:60.1848 s\n",
      "[epoch 737]: training loss: 1149.964355, consuming time:60.3643 s\n",
      "[epoch 738]: training loss: 1032.073975, consuming time:60.2823 s\n",
      "[epoch 739]: training loss: 1090.507935, consuming time:60.4697 s\n",
      "[epoch 740]: training loss: 1314.323608, consuming time:59.8057 s\n",
      "[epoch 741]: training loss: 1409.334229, consuming time:60.0316 s\n",
      "[epoch 742]: training loss: 1119.204712, consuming time:60.5250 s\n",
      "[epoch 743]: training loss: 1293.795044, consuming time:60.5163 s\n",
      "[epoch 744]: training loss: 1425.378662, consuming time:60.2612 s\n",
      "[epoch 745]: training loss: 1163.479370, consuming time:60.3771 s\n",
      "[epoch 746]: training loss: 1215.520996, consuming time:59.8086 s\n",
      "[epoch 747]: training loss: 1225.510498, consuming time:60.1461 s\n",
      "[epoch 748]: training loss: 1067.448975, consuming time:60.1966 s\n",
      "[epoch 749]: training loss: 1033.780029, consuming time:60.3777 s\n",
      "[epoch 750]: training loss: 897.875366, consuming time:60.3944 s\n",
      "[epoch 751]: training loss: 1108.388916, consuming time:60.3289 s\n",
      "[epoch 752]: training loss: 670.915344, consuming time:60.0313 s\n",
      "[epoch 753]: training loss: 1415.948242, consuming time:60.2676 s\n",
      "[epoch 754]: training loss: 1345.347046, consuming time:60.1943 s\n",
      "[epoch 755]: training loss: 1098.423462, consuming time:60.6144 s\n",
      "[epoch 756]: training loss: 941.228882, consuming time:60.5894 s\n",
      "[epoch 757]: training loss: 967.325317, consuming time:60.4230 s\n",
      "[epoch 758]: training loss: 1090.821655, consuming time:59.9705 s\n",
      "[epoch 759]: training loss: 1407.821533, consuming time:60.4651 s\n",
      "[epoch 760]: training loss: 1237.224365, consuming time:60.3255 s\n",
      "[epoch 761]: training loss: 1034.815674, consuming time:60.5669 s\n",
      "[epoch 762]: training loss: 965.501343, consuming time:60.4575 s\n",
      "[epoch 763]: training loss: 1063.843262, consuming time:60.1210 s\n",
      "[epoch 764]: training loss: 1170.543579, consuming time:60.0005 s\n",
      "[epoch 765]: training loss: 1495.988037, consuming time:60.1751 s\n",
      "[epoch 766]: training loss: 1021.591431, consuming time:60.5256 s\n",
      "[epoch 767]: training loss: 1195.469727, consuming time:60.2319 s\n",
      "[epoch 768]: training loss: 1146.231934, consuming time:60.3835 s\n",
      "[epoch 769]: training loss: 1011.726135, consuming time:60.4913 s\n",
      "[epoch 770]: training loss: 803.377441, consuming time:59.8343 s\n",
      "[epoch 771]: training loss: 1092.151123, consuming time:60.1706 s\n",
      "[epoch 772]: training loss: 1011.997925, consuming time:60.5403 s\n",
      "[epoch 773]: training loss: 1155.221802, consuming time:60.5606 s\n",
      "[epoch 774]: training loss: 1303.159180, consuming time:60.4372 s\n",
      "[epoch 775]: training loss: 1073.938232, consuming time:60.3867 s\n",
      "[epoch 776]: training loss: 1331.690308, consuming time:60.1289 s\n",
      "[epoch 777]: training loss: 1099.843018, consuming time:60.4216 s\n",
      "[epoch 778]: training loss: 1139.536987, consuming time:60.8985 s\n",
      "[epoch 779]: training loss: 1092.275879, consuming time:60.4579 s\n",
      "[epoch 780]: training loss: 1055.387695, consuming time:60.5715 s\n",
      "[epoch 781]: training loss: 1220.412476, consuming time:60.2550 s\n",
      "[epoch 782]: training loss: 1259.380737, consuming time:59.8880 s\n",
      "[epoch 783]: training loss: 1108.428711, consuming time:60.3683 s\n",
      "[epoch 784]: training loss: 1298.982056, consuming time:60.0516 s\n",
      "[epoch 785]: training loss: 893.311401, consuming time:60.5329 s\n",
      "[epoch 786]: training loss: 1217.593506, consuming time:60.4156 s\n",
      "[epoch 787]: training loss: 1237.952148, consuming time:60.5388 s\n",
      "[epoch 788]: training loss: 1024.368774, consuming time:60.0147 s\n",
      "[epoch 789]: training loss: 1447.011230, consuming time:60.2082 s\n",
      "[epoch 790]: training loss: 1133.042358, consuming time:60.5619 s\n",
      "[epoch 791]: training loss: 964.609131, consuming time:60.7017 s\n",
      "[epoch 792]: training loss: 1019.078308, consuming time:60.3366 s\n",
      "[epoch 793]: training loss: 860.777344, consuming time:60.2198 s\n",
      "[epoch 794]: training loss: 942.928650, consuming time:59.5448 s\n",
      "[epoch 795]: training loss: 832.297241, consuming time:60.1878 s\n",
      "[epoch 796]: training loss: 848.328979, consuming time:60.4882 s\n",
      "[epoch 797]: training loss: 1398.104492, consuming time:60.4894 s\n",
      "[epoch 798]: training loss: 667.692627, consuming time:60.2536 s\n",
      "[epoch 799]: training loss: 1276.904053, consuming time:60.0983 s\n",
      "[epoch 800]: training loss: 963.191772, consuming time:59.6846 s\n",
      "[epoch 801]: training loss: 1196.003418, consuming time:60.1980 s\n",
      "[epoch 802]: training loss: 1144.032227, consuming time:60.3925 s\n",
      "[epoch 803]: training loss: 1167.762817, consuming time:60.4237 s\n",
      "[epoch 804]: training loss: 1511.244751, consuming time:60.4733 s\n",
      "[epoch 805]: training loss: 1003.758301, consuming time:60.5472 s\n",
      "[epoch 806]: training loss: 1070.358765, consuming time:60.2682 s\n",
      "[epoch 807]: training loss: 1006.504639, consuming time:60.5763 s\n",
      "[epoch 808]: training loss: 1045.794434, consuming time:60.6724 s\n",
      "[epoch 809]: training loss: 949.437744, consuming time:60.9259 s\n",
      "[epoch 810]: training loss: 1189.050171, consuming time:60.5959 s\n",
      "[epoch 811]: training loss: 1057.003174, consuming time:60.3110 s\n",
      "[epoch 812]: training loss: 1175.225342, consuming time:60.4948 s\n",
      "[epoch 813]: training loss: 1523.232666, consuming time:60.4460 s\n",
      "[epoch 814]: training loss: 927.207397, consuming time:60.3270 s\n",
      "[epoch 815]: training loss: 948.683472, consuming time:60.4020 s\n",
      "[epoch 816]: training loss: 1272.689941, consuming time:60.6147 s\n",
      "[epoch 817]: training loss: 1135.137939, consuming time:60.4010 s\n",
      "[epoch 818]: training loss: 991.681519, consuming time:59.7415 s\n",
      "[epoch 819]: training loss: 1123.192871, consuming time:60.5619 s\n",
      "[epoch 820]: training loss: 1025.077148, consuming time:60.2674 s\n",
      "[epoch 821]: training loss: 1021.162109, consuming time:60.5427 s\n",
      "[epoch 822]: training loss: 1036.516602, consuming time:60.4298 s\n",
      "[epoch 823]: training loss: 1217.833862, consuming time:60.2745 s\n",
      "[epoch 824]: training loss: 1083.745850, consuming time:59.8391 s\n",
      "[epoch 825]: training loss: 1052.110107, consuming time:60.2773 s\n",
      "[epoch 826]: training loss: 1119.051025, consuming time:60.4125 s\n",
      "[epoch 827]: training loss: 1113.350342, consuming time:60.6280 s\n",
      "[epoch 828]: training loss: 799.447388, consuming time:60.4304 s\n",
      "[epoch 829]: training loss: 780.287231, consuming time:60.0492 s\n",
      "[epoch 830]: training loss: 1105.144775, consuming time:60.0234 s\n",
      "[epoch 831]: training loss: 958.746155, consuming time:60.4602 s\n",
      "[epoch 832]: training loss: 1331.484497, consuming time:60.6574 s\n",
      "[epoch 833]: training loss: 1067.318970, consuming time:60.5318 s\n",
      "[epoch 834]: training loss: 995.142395, consuming time:60.3727 s\n",
      "[epoch 835]: training loss: 1088.175171, consuming time:60.0584 s\n",
      "[epoch 836]: training loss: 1104.433594, consuming time:59.8235 s\n",
      "[epoch 837]: training loss: 986.308472, consuming time:60.3752 s\n",
      "[epoch 838]: training loss: 1340.128296, consuming time:60.2775 s\n",
      "[epoch 839]: training loss: 1096.154297, consuming time:60.5810 s\n",
      "[epoch 840]: training loss: 930.493042, consuming time:60.5351 s\n",
      "[epoch 841]: training loss: 1051.313232, consuming time:59.9314 s\n",
      "[epoch 842]: training loss: 1041.387451, consuming time:60.1225 s\n",
      "[epoch 843]: training loss: 1296.366943, consuming time:60.2978 s\n",
      "[epoch 844]: training loss: 990.846130, consuming time:60.4357 s\n",
      "[epoch 845]: training loss: 1205.092163, consuming time:60.5412 s\n",
      "[epoch 846]: training loss: 1307.588257, consuming time:60.3487 s\n",
      "[epoch 847]: training loss: 915.625000, consuming time:60.0649 s\n",
      "[epoch 848]: training loss: 1446.496826, consuming time:60.1656 s\n",
      "[epoch 849]: training loss: 1178.737549, consuming time:60.2358 s\n",
      "[epoch 850]: training loss: 1715.590820, consuming time:60.5295 s\n",
      "[epoch 851]: training loss: 1183.369019, consuming time:60.6325 s\n",
      "[epoch 852]: training loss: 1327.868652, consuming time:60.5390 s\n",
      "[epoch 853]: training loss: 1005.191162, consuming time:59.9760 s\n",
      "[epoch 854]: training loss: 952.597290, consuming time:60.1051 s\n",
      "[epoch 855]: training loss: 1290.672119, consuming time:60.2886 s\n",
      "[epoch 856]: training loss: 1287.898926, consuming time:60.3518 s\n",
      "[epoch 857]: training loss: 1288.115967, consuming time:60.4141 s\n",
      "[epoch 858]: training loss: 1236.091797, consuming time:60.4330 s\n",
      "[epoch 859]: training loss: 1133.668091, consuming time:60.3481 s\n",
      "[epoch 860]: training loss: 1158.783936, consuming time:60.2852 s\n",
      "[epoch 861]: training loss: 1192.745361, consuming time:60.3682 s\n",
      "[epoch 862]: training loss: 1624.121338, consuming time:60.5685 s\n",
      "[epoch 863]: training loss: 924.870056, consuming time:60.3247 s\n",
      "[epoch 864]: training loss: 847.681152, consuming time:60.3284 s\n",
      "[epoch 865]: training loss: 906.132935, consuming time:60.2843 s\n",
      "[epoch 866]: training loss: 1017.746277, consuming time:59.9946 s\n",
      "[epoch 867]: training loss: 1018.286011, consuming time:60.3309 s\n",
      "[epoch 868]: training loss: 858.936523, consuming time:60.2414 s\n",
      "[epoch 869]: training loss: 1326.932373, consuming time:60.5422 s\n",
      "[epoch 870]: training loss: 1117.712891, consuming time:60.4905 s\n",
      "[epoch 871]: training loss: 1117.704834, consuming time:59.9178 s\n",
      "[epoch 872]: training loss: 859.744263, consuming time:59.9200 s\n",
      "[epoch 873]: training loss: 1208.267578, consuming time:60.6220 s\n",
      "[epoch 874]: training loss: 1179.935547, consuming time:60.3508 s\n",
      "[epoch 875]: training loss: 1216.800049, consuming time:60.4863 s\n",
      "[epoch 876]: training loss: 1043.541870, consuming time:60.2965 s\n",
      "[epoch 877]: training loss: 1407.266113, consuming time:60.1395 s\n",
      "[epoch 878]: training loss: 1066.360107, consuming time:60.2117 s\n",
      "[epoch 879]: training loss: 959.260986, consuming time:60.2739 s\n",
      "[epoch 880]: training loss: 888.247986, consuming time:60.2014 s\n",
      "[epoch 881]: training loss: 1132.259033, consuming time:60.5899 s\n",
      "[epoch 882]: training loss: 1011.466980, consuming time:60.4255 s\n",
      "[epoch 883]: training loss: 1010.860168, consuming time:59.9265 s\n",
      "[epoch 884]: training loss: 857.829102, consuming time:59.9307 s\n",
      "[epoch 885]: training loss: 870.891113, consuming time:60.5509 s\n",
      "[epoch 886]: training loss: 1079.330322, consuming time:60.1084 s\n",
      "[epoch 887]: training loss: 1041.682861, consuming time:60.8190 s\n",
      "[epoch 888]: training loss: 1233.030151, consuming time:60.5826 s\n",
      "[epoch 889]: training loss: 956.607422, consuming time:59.9624 s\n",
      "[epoch 890]: training loss: 1337.179199, consuming time:60.1453 s\n",
      "[epoch 891]: training loss: 1159.034546, consuming time:60.5265 s\n",
      "[epoch 892]: training loss: 1275.675049, consuming time:60.5101 s\n",
      "[epoch 893]: training loss: 1233.289795, consuming time:60.3732 s\n",
      "[epoch 894]: training loss: 1136.615723, consuming time:60.1478 s\n",
      "[epoch 895]: training loss: 1035.375366, consuming time:60.1486 s\n",
      "[epoch 896]: training loss: 890.174500, consuming time:59.9866 s\n",
      "[epoch 897]: training loss: 1262.727051, consuming time:60.3312 s\n",
      "[epoch 898]: training loss: 1238.769287, consuming time:60.2388 s\n",
      "[epoch 899]: training loss: 1499.628662, consuming time:60.2380 s\n",
      "[epoch 900]: training loss: 882.339539, consuming time:60.2741 s\n",
      "[epoch 901]: training loss: 1085.882812, consuming time:59.8078 s\n",
      "[epoch 902]: training loss: 1166.333862, consuming time:60.3608 s\n",
      "[epoch 903]: training loss: 975.581909, consuming time:60.3322 s\n",
      "[epoch 904]: training loss: 796.218567, consuming time:60.3828 s\n",
      "[epoch 905]: training loss: 1214.157837, consuming time:60.7140 s\n",
      "[epoch 906]: training loss: 1106.511719, consuming time:60.2651 s\n",
      "[epoch 907]: training loss: 978.230103, consuming time:59.6918 s\n",
      "[epoch 908]: training loss: 851.391113, consuming time:60.1597 s\n",
      "[epoch 909]: training loss: 996.711975, consuming time:60.1842 s\n",
      "[epoch 910]: training loss: 1311.927856, consuming time:60.4549 s\n",
      "[epoch 911]: training loss: 994.717407, consuming time:60.4611 s\n",
      "[epoch 912]: training loss: 1132.484375, consuming time:60.0440 s\n",
      "[epoch 913]: training loss: 1058.817871, consuming time:59.7098 s\n",
      "[epoch 914]: training loss: 788.399536, consuming time:60.0952 s\n",
      "[epoch 915]: training loss: 904.093628, consuming time:60.2470 s\n",
      "[epoch 916]: training loss: 1055.522827, consuming time:60.0150 s\n",
      "[epoch 917]: training loss: 993.573120, consuming time:60.4332 s\n",
      "[epoch 918]: training loss: 687.867310, consuming time:60.4022 s\n",
      "[epoch 919]: training loss: 1072.119141, consuming time:59.9129 s\n",
      "[epoch 920]: training loss: 1039.883423, consuming time:60.1077 s\n",
      "[epoch 921]: training loss: 994.967957, consuming time:60.0211 s\n",
      "[epoch 922]: training loss: 1002.923462, consuming time:60.2614 s\n",
      "[epoch 923]: training loss: 987.076904, consuming time:60.2773 s\n",
      "[epoch 924]: training loss: 1323.450928, consuming time:60.2668 s\n",
      "[epoch 925]: training loss: 1135.328247, consuming time:59.4871 s\n",
      "[epoch 926]: training loss: 994.596863, consuming time:60.5074 s\n",
      "[epoch 927]: training loss: 1170.620972, consuming time:60.3602 s\n",
      "[epoch 928]: training loss: 1276.285522, consuming time:60.4570 s\n",
      "[epoch 929]: training loss: 1232.270020, consuming time:60.4100 s\n",
      "[epoch 930]: training loss: 1119.237549, consuming time:60.1920 s\n",
      "[epoch 931]: training loss: 1219.760498, consuming time:60.1997 s\n",
      "[epoch 932]: training loss: 1177.118652, consuming time:60.0728 s\n",
      "[epoch 933]: training loss: 901.888062, consuming time:60.4253 s\n",
      "[epoch 934]: training loss: 888.822876, consuming time:60.2098 s\n",
      "[epoch 935]: training loss: 1219.625732, consuming time:60.8833 s\n",
      "[epoch 936]: training loss: 1157.314697, consuming time:60.0348 s\n",
      "[epoch 937]: training loss: 1050.070923, consuming time:59.5690 s\n",
      "[epoch 938]: training loss: 1001.686707, consuming time:60.1261 s\n",
      "[epoch 939]: training loss: 1416.177368, consuming time:60.1380 s\n",
      "[epoch 940]: training loss: 1162.207397, consuming time:60.8166 s\n",
      "[epoch 941]: training loss: 1014.275635, consuming time:60.3156 s\n",
      "[epoch 942]: training loss: 1053.287231, consuming time:60.2149 s\n",
      "[epoch 943]: training loss: 1000.742676, consuming time:59.7909 s\n",
      "[epoch 944]: training loss: 1150.822266, consuming time:60.1386 s\n",
      "[epoch 945]: training loss: 1083.705566, consuming time:60.3396 s\n",
      "[epoch 946]: training loss: 1125.777100, consuming time:60.3292 s\n",
      "[epoch 947]: training loss: 1004.349365, consuming time:60.4284 s\n",
      "[epoch 948]: training loss: 1160.307007, consuming time:59.9985 s\n",
      "[epoch 949]: training loss: 901.473389, consuming time:60.0971 s\n",
      "[epoch 950]: training loss: 1037.887451, consuming time:59.9022 s\n",
      "[epoch 951]: training loss: 958.388062, consuming time:60.2486 s\n",
      "[epoch 952]: training loss: 1089.122559, consuming time:60.5017 s\n",
      "[epoch 953]: training loss: 1325.971558, consuming time:60.5293 s\n",
      "[epoch 954]: training loss: 1389.863525, consuming time:60.3502 s\n",
      "[epoch 955]: training loss: 1036.049927, consuming time:59.9393 s\n",
      "[epoch 956]: training loss: 1242.914673, consuming time:60.1377 s\n",
      "[epoch 957]: training loss: 784.417542, consuming time:60.1050 s\n",
      "[epoch 958]: training loss: 926.953003, consuming time:60.5890 s\n",
      "[epoch 959]: training loss: 1204.676758, consuming time:60.6917 s\n",
      "[epoch 960]: training loss: 1305.793579, consuming time:60.1332 s\n",
      "[epoch 961]: training loss: 951.203918, consuming time:59.7695 s\n",
      "[epoch 962]: training loss: 1171.061035, consuming time:60.0506 s\n",
      "[epoch 963]: training loss: 986.250183, consuming time:60.2685 s\n",
      "[epoch 964]: training loss: 1132.223877, consuming time:60.3667 s\n",
      "[epoch 965]: training loss: 841.143555, consuming time:60.3825 s\n",
      "[epoch 966]: training loss: 949.448059, consuming time:60.5104 s\n",
      "[epoch 967]: training loss: 914.092407, consuming time:63.1541 s\n",
      "[epoch 968]: training loss: 1263.049805, consuming time:60.2643 s\n",
      "[epoch 969]: training loss: 977.907349, consuming time:60.0955 s\n",
      "[epoch 970]: training loss: 1803.047729, consuming time:60.4479 s\n",
      "[epoch 971]: training loss: 1439.544434, consuming time:60.5306 s\n",
      "[epoch 972]: training loss: 873.236572, consuming time:60.2243 s\n",
      "[epoch 973]: training loss: 811.911133, consuming time:59.7875 s\n",
      "[epoch 974]: training loss: 1310.829224, consuming time:59.9856 s\n",
      "[epoch 975]: training loss: 736.462585, consuming time:60.5950 s\n",
      "[epoch 976]: training loss: 1228.705322, consuming time:60.4133 s\n",
      "[epoch 977]: training loss: 1116.211060, consuming time:60.0997 s\n",
      "[epoch 978]: training loss: 1274.255859, consuming time:60.1505 s\n",
      "[epoch 979]: training loss: 1027.226807, consuming time:60.2187 s\n",
      "[epoch 980]: training loss: 1444.459229, consuming time:60.3619 s\n",
      "[epoch 981]: training loss: 1193.822388, consuming time:60.7249 s\n",
      "[epoch 982]: training loss: 889.658569, consuming time:60.8195 s\n",
      "[epoch 983]: training loss: 1293.777344, consuming time:60.7077 s\n",
      "[epoch 984]: training loss: 1002.802368, consuming time:60.5043 s\n",
      "[epoch 985]: training loss: 1570.197510, consuming time:60.2693 s\n",
      "[epoch 986]: training loss: 1051.082275, consuming time:60.5405 s\n",
      "[epoch 987]: training loss: 1128.068726, consuming time:60.7940 s\n",
      "[epoch 988]: training loss: 1398.489746, consuming time:60.7585 s\n",
      "[epoch 989]: training loss: 862.628418, consuming time:60.4389 s\n",
      "[epoch 990]: training loss: 1056.400879, consuming time:60.2929 s\n",
      "[epoch 991]: training loss: 1183.017700, consuming time:59.8352 s\n",
      "[epoch 992]: training loss: 1145.059326, consuming time:60.5108 s\n",
      "[epoch 993]: training loss: 1308.190430, consuming time:60.5696 s\n",
      "[epoch 994]: training loss: 727.920898, consuming time:60.8421 s\n",
      "[epoch 995]: training loss: 1150.916992, consuming time:60.3959 s\n",
      "[epoch 996]: training loss: 882.883179, consuming time:60.2710 s\n",
      "[epoch 997]: training loss: 1003.563904, consuming time:59.9670 s\n",
      "[epoch 998]: training loss: 1158.405396, consuming time:60.6532 s\n",
      "[epoch 999]: training loss: 960.704102, consuming time:60.5110 s\n",
      "[epoch 1000]: training loss: 1075.265381, consuming time:60.6017 s\n",
      "[epoch 1001]: training loss: 966.986511, consuming time:60.6218 s\n",
      "[epoch 1002]: training loss: 986.254517, consuming time:60.2844 s\n",
      "[epoch 1003]: training loss: 960.021606, consuming time:59.9214 s\n",
      "[epoch 1004]: training loss: 1197.346802, consuming time:60.1643 s\n",
      "[epoch 1005]: training loss: 1137.228882, consuming time:60.5246 s\n",
      "[epoch 1006]: training loss: 1141.213623, consuming time:60.7207 s\n",
      "[epoch 1007]: training loss: 770.215820, consuming time:60.7000 s\n",
      "[epoch 1008]: training loss: 864.953857, consuming time:60.5052 s\n",
      "[epoch 1009]: training loss: 1244.575439, consuming time:60.0521 s\n",
      "[epoch 1010]: training loss: 1248.189453, consuming time:60.4264 s\n",
      "[epoch 1011]: training loss: 1030.039307, consuming time:60.4385 s\n",
      "[epoch 1012]: training loss: 881.134155, consuming time:60.3851 s\n",
      "[epoch 1013]: training loss: 946.605469, consuming time:60.2646 s\n",
      "[epoch 1014]: training loss: 958.301086, consuming time:60.1884 s\n",
      "[epoch 1015]: training loss: 1094.984619, consuming time:59.4488 s\n",
      "[epoch 1016]: training loss: 1360.619507, consuming time:60.1628 s\n",
      "[epoch 1017]: training loss: 868.026978, consuming time:60.2744 s\n",
      "[epoch 1018]: training loss: 1141.316040, consuming time:60.2775 s\n",
      "[epoch 1019]: training loss: 902.851440, consuming time:60.3979 s\n",
      "[epoch 1020]: training loss: 1053.611450, consuming time:60.1392 s\n",
      "[epoch 1021]: training loss: 1268.195312, consuming time:59.5905 s\n",
      "[epoch 1022]: training loss: 1463.389404, consuming time:60.1449 s\n",
      "[epoch 1023]: training loss: 1315.370728, consuming time:60.1201 s\n",
      "[epoch 1024]: training loss: 1071.854980, consuming time:60.5431 s\n",
      "[epoch 1025]: training loss: 1393.661011, consuming time:60.6961 s\n",
      "[epoch 1026]: training loss: 723.325684, consuming time:60.2984 s\n",
      "[epoch 1027]: training loss: 1170.426270, consuming time:60.0411 s\n",
      "[epoch 1028]: training loss: 1136.560791, consuming time:60.7101 s\n",
      "[epoch 1029]: training loss: 1294.487915, consuming time:60.5238 s\n",
      "[epoch 1030]: training loss: 884.521484, consuming time:60.7707 s\n",
      "[epoch 1031]: training loss: 720.387878, consuming time:60.6384 s\n",
      "[epoch 1032]: training loss: 982.889282, consuming time:60.3369 s\n",
      "[epoch 1033]: training loss: 1279.637939, consuming time:60.1211 s\n",
      "[epoch 1034]: training loss: 952.521729, consuming time:60.4802 s\n",
      "[epoch 1035]: training loss: 963.329773, consuming time:60.6630 s\n",
      "[epoch 1036]: training loss: 1024.825073, consuming time:60.4565 s\n",
      "[epoch 1037]: training loss: 1053.606201, consuming time:60.4531 s\n",
      "[epoch 1038]: training loss: 1048.552490, consuming time:60.3971 s\n",
      "[epoch 1039]: training loss: 1031.092285, consuming time:60.2634 s\n",
      "[epoch 1040]: training loss: 1118.644897, consuming time:60.6111 s\n",
      "[epoch 1041]: training loss: 1222.370728, consuming time:60.6563 s\n",
      "[epoch 1042]: training loss: 1200.289795, consuming time:60.3507 s\n",
      "[epoch 1043]: training loss: 848.697083, consuming time:60.5816 s\n",
      "[epoch 1044]: training loss: 861.793945, consuming time:60.3622 s\n",
      "[epoch 1045]: training loss: 647.612427, consuming time:60.1211 s\n",
      "[epoch 1046]: training loss: 1171.475342, consuming time:60.6192 s\n",
      "[epoch 1047]: training loss: 1283.650513, consuming time:60.3029 s\n",
      "[epoch 1048]: training loss: 1139.035889, consuming time:60.7582 s\n",
      "[epoch 1049]: training loss: 1145.826172, consuming time:60.7183 s\n",
      "[epoch 1050]: training loss: 962.870850, consuming time:60.1344 s\n",
      "[epoch 1051]: training loss: 851.559143, consuming time:60.1972 s\n",
      "[epoch 1052]: training loss: 861.928833, consuming time:60.5771 s\n",
      "[epoch 1053]: training loss: 1172.903320, consuming time:60.5695 s\n",
      "[epoch 1054]: training loss: 1001.428711, consuming time:60.7034 s\n",
      "[epoch 1055]: training loss: 959.868652, consuming time:60.6776 s\n",
      "[epoch 1056]: training loss: 1272.125732, consuming time:60.6505 s\n",
      "[epoch 1057]: training loss: 856.295837, consuming time:59.9143 s\n",
      "[epoch 1058]: training loss: 1113.187500, consuming time:60.5626 s\n",
      "[epoch 1059]: training loss: 1132.704346, consuming time:60.4347 s\n",
      "[epoch 1060]: training loss: 1343.583496, consuming time:60.5629 s\n",
      "[epoch 1061]: training loss: 943.143677, consuming time:60.2957 s\n",
      "[epoch 1062]: training loss: 1350.363037, consuming time:60.2649 s\n",
      "[epoch 1063]: training loss: 1322.750244, consuming time:60.6751 s\n",
      "[epoch 1064]: training loss: 1138.256104, consuming time:60.6956 s\n",
      "[epoch 1065]: training loss: 1007.379639, consuming time:60.8173 s\n",
      "[epoch 1066]: training loss: 808.621399, consuming time:60.8077 s\n",
      "[epoch 1067]: training loss: 916.191956, consuming time:60.4747 s\n",
      "[epoch 1068]: training loss: 1122.848633, consuming time:60.4026 s\n",
      "[epoch 1069]: training loss: 871.327759, consuming time:60.2759 s\n",
      "[epoch 1070]: training loss: 1091.230469, consuming time:60.5870 s\n",
      "[epoch 1071]: training loss: 1052.176514, consuming time:60.6162 s\n",
      "[epoch 1072]: training loss: 1171.166992, consuming time:60.6027 s\n",
      "[epoch 1073]: training loss: 1191.112427, consuming time:60.5187 s\n",
      "[epoch 1074]: training loss: 1045.894287, consuming time:59.9903 s\n",
      "[epoch 1075]: training loss: 978.601196, consuming time:60.4251 s\n",
      "[epoch 1076]: training loss: 1171.739136, consuming time:60.6055 s\n",
      "[epoch 1077]: training loss: 857.149414, consuming time:60.4385 s\n",
      "[epoch 1078]: training loss: 957.827087, consuming time:60.6929 s\n",
      "[epoch 1079]: training loss: 819.172852, consuming time:60.9088 s\n",
      "[epoch 1080]: training loss: 1257.243042, consuming time:60.0774 s\n",
      "[epoch 1081]: training loss: 1277.865601, consuming time:60.4411 s\n",
      "[epoch 1082]: training loss: 1129.797119, consuming time:60.4407 s\n",
      "[epoch 1083]: training loss: 1370.810547, consuming time:60.5706 s\n",
      "[epoch 1084]: training loss: 676.318604, consuming time:60.5417 s\n",
      "[epoch 1085]: training loss: 1155.321289, consuming time:60.4886 s\n",
      "[epoch 1086]: training loss: 1128.797607, consuming time:60.4820 s\n",
      "[epoch 1087]: training loss: 1257.529175, consuming time:60.1298 s\n",
      "[epoch 1088]: training loss: 1220.057251, consuming time:60.2943 s\n",
      "[epoch 1089]: training loss: 1060.184570, consuming time:60.6922 s\n",
      "[epoch 1090]: training loss: 1044.334961, consuming time:60.7056 s\n",
      "[epoch 1091]: training loss: 988.817261, consuming time:60.6001 s\n",
      "[epoch 1092]: training loss: 1388.369141, consuming time:60.0898 s\n",
      "[epoch 1093]: training loss: 1170.655029, consuming time:60.5868 s\n",
      "[epoch 1094]: training loss: 978.331055, consuming time:60.4488 s\n",
      "[epoch 1095]: training loss: 1077.906372, consuming time:60.7374 s\n",
      "[epoch 1096]: training loss: 1298.623047, consuming time:60.6020 s\n",
      "[epoch 1097]: training loss: 1198.208008, consuming time:60.2929 s\n",
      "[epoch 1098]: training loss: 1088.598389, consuming time:60.4887 s\n",
      "[epoch 1099]: training loss: 1382.243286, consuming time:60.3492 s\n",
      "[epoch 1100]: training loss: 1082.803711, consuming time:60.7473 s\n",
      "[epoch 1101]: training loss: 1032.963379, consuming time:60.7242 s\n",
      "[epoch 1102]: training loss: 1371.576904, consuming time:60.9360 s\n",
      "[epoch 1103]: training loss: 763.249023, consuming time:60.6642 s\n",
      "[epoch 1104]: training loss: 1187.841919, consuming time:60.3298 s\n",
      "[epoch 1105]: training loss: 1025.434814, consuming time:60.4425 s\n",
      "[epoch 1106]: training loss: 1142.670898, consuming time:60.4100 s\n",
      "[epoch 1107]: training loss: 1192.232300, consuming time:60.8988 s\n",
      "[epoch 1108]: training loss: 1195.389404, consuming time:60.9095 s\n",
      "[epoch 1109]: training loss: 1159.340576, consuming time:60.3327 s\n",
      "[epoch 1110]: training loss: 1043.625854, consuming time:60.1619 s\n",
      "[epoch 1111]: training loss: 1091.239014, consuming time:60.6670 s\n",
      "[epoch 1112]: training loss: 1058.016357, consuming time:60.6568 s\n",
      "[epoch 1113]: training loss: 1034.198120, consuming time:60.6445 s\n",
      "[epoch 1114]: training loss: 1065.417969, consuming time:60.6750 s\n",
      "[epoch 1115]: training loss: 659.405701, consuming time:60.4718 s\n",
      "[epoch 1116]: training loss: 904.995300, consuming time:60.1813 s\n",
      "[epoch 1117]: training loss: 1151.216064, consuming time:60.3124 s\n",
      "[epoch 1118]: training loss: 1001.530884, consuming time:60.3422 s\n",
      "[epoch 1119]: training loss: 1153.745605, consuming time:60.5923 s\n",
      "[epoch 1120]: training loss: 1063.488403, consuming time:60.5555 s\n",
      "[epoch 1121]: training loss: 1133.891113, consuming time:60.1660 s\n",
      "[epoch 1122]: training loss: 779.296753, consuming time:60.0941 s\n",
      "[epoch 1123]: training loss: 955.782959, consuming time:60.6022 s\n",
      "[epoch 1124]: training loss: 940.582764, consuming time:60.5414 s\n",
      "[epoch 1125]: training loss: 1002.726440, consuming time:60.7399 s\n",
      "[epoch 1126]: training loss: 1448.826660, consuming time:60.7074 s\n",
      "[epoch 1127]: training loss: 1193.070923, consuming time:60.5370 s\n",
      "[epoch 1128]: training loss: 937.385864, consuming time:60.3570 s\n",
      "[epoch 1129]: training loss: 1114.236328, consuming time:60.6570 s\n",
      "[epoch 1130]: training loss: 1033.931763, consuming time:60.2502 s\n",
      "[epoch 1131]: training loss: 981.539917, consuming time:60.7330 s\n",
      "[epoch 1132]: training loss: 1204.177368, consuming time:60.3987 s\n",
      "[epoch 1133]: training loss: 866.229858, consuming time:60.5646 s\n",
      "[epoch 1134]: training loss: 1118.146484, consuming time:60.1868 s\n",
      "[epoch 1135]: training loss: 1038.326660, consuming time:60.5201 s\n",
      "[epoch 1136]: training loss: 920.676392, consuming time:60.2658 s\n",
      "[epoch 1137]: training loss: 1237.453369, consuming time:60.4662 s\n",
      "[epoch 1138]: training loss: 984.487183, consuming time:60.3970 s\n",
      "[epoch 1139]: training loss: 838.341248, consuming time:60.4909 s\n",
      "[epoch 1140]: training loss: 1210.463623, consuming time:60.1431 s\n",
      "[epoch 1141]: training loss: 854.961060, consuming time:60.6703 s\n",
      "[epoch 1142]: training loss: 1104.608887, consuming time:60.7552 s\n",
      "[epoch 1143]: training loss: 1413.532959, consuming time:60.6007 s\n",
      "[epoch 1144]: training loss: 889.761353, consuming time:60.9273 s\n",
      "[epoch 1145]: training loss: 1053.069458, consuming time:60.4832 s\n",
      "[epoch 1146]: training loss: 1157.321045, consuming time:60.1301 s\n",
      "[epoch 1147]: training loss: 859.599487, consuming time:60.5785 s\n",
      "[epoch 1148]: training loss: 1198.900635, consuming time:60.6190 s\n",
      "[epoch 1149]: training loss: 1170.057861, consuming time:60.8659 s\n",
      "[epoch 1150]: training loss: 895.218933, consuming time:60.7663 s\n",
      "[epoch 1151]: training loss: 976.896973, consuming time:60.4277 s\n",
      "[epoch 1152]: training loss: 982.451538, consuming time:60.1285 s\n",
      "[epoch 1153]: training loss: 1001.287231, consuming time:60.6256 s\n",
      "[epoch 1154]: training loss: 1242.474365, consuming time:60.7512 s\n",
      "[epoch 1155]: training loss: 959.208496, consuming time:60.7369 s\n",
      "[epoch 1156]: training loss: 888.108032, consuming time:60.5074 s\n",
      "[epoch 1157]: training loss: 1278.427368, consuming time:60.4386 s\n",
      "[epoch 1158]: training loss: 1155.080933, consuming time:60.1250 s\n",
      "[epoch 1159]: training loss: 1072.114136, consuming time:60.5318 s\n",
      "[epoch 1160]: training loss: 1228.005737, consuming time:60.4932 s\n",
      "[epoch 1161]: training loss: 1175.318848, consuming time:61.0015 s\n",
      "[epoch 1162]: training loss: 1209.738281, consuming time:60.8900 s\n",
      "[epoch 1163]: training loss: 1165.647217, consuming time:60.3379 s\n",
      "[epoch 1164]: training loss: 1249.028809, consuming time:60.4645 s\n",
      "[epoch 1165]: training loss: 1147.845215, consuming time:60.6866 s\n",
      "[epoch 1166]: training loss: 1124.518799, consuming time:60.8123 s\n",
      "[epoch 1167]: training loss: 1005.717346, consuming time:60.7246 s\n",
      "[epoch 1168]: training loss: 1101.206055, consuming time:60.7301 s\n",
      "[epoch 1169]: training loss: 933.239380, consuming time:60.2039 s\n",
      "[epoch 1170]: training loss: 914.747314, consuming time:60.4277 s\n",
      "[epoch 1171]: training loss: 859.097900, consuming time:60.6107 s\n",
      "[epoch 1172]: training loss: 896.803833, consuming time:60.7475 s\n",
      "[epoch 1173]: training loss: 957.748169, consuming time:60.6180 s\n",
      "[epoch 1174]: training loss: 851.416931, consuming time:60.5053 s\n",
      "[epoch 1175]: training loss: 1098.107910, consuming time:60.3596 s\n",
      "[epoch 1176]: training loss: 936.479614, consuming time:60.3533 s\n",
      "[epoch 1177]: training loss: 939.484619, consuming time:60.6162 s\n",
      "[epoch 1178]: training loss: 1062.772217, consuming time:60.6861 s\n",
      "[epoch 1179]: training loss: 1051.334717, consuming time:60.5034 s\n",
      "[epoch 1180]: training loss: 922.520508, consuming time:60.7216 s\n",
      "[epoch 1181]: training loss: 1268.108521, consuming time:60.3679 s\n",
      "[epoch 1182]: training loss: 989.576050, consuming time:60.2199 s\n",
      "[epoch 1183]: training loss: 960.047729, consuming time:60.5229 s\n",
      "[epoch 1184]: training loss: 850.158020, consuming time:60.5119 s\n",
      "[epoch 1185]: training loss: 1234.512695, consuming time:60.4459 s\n",
      "[epoch 1186]: training loss: 1114.800659, consuming time:60.7900 s\n",
      "[epoch 1187]: training loss: 1239.951660, consuming time:60.3325 s\n",
      "[epoch 1188]: training loss: 938.539001, consuming time:59.8852 s\n",
      "[epoch 1189]: training loss: 1428.772095, consuming time:60.6289 s\n",
      "[epoch 1190]: training loss: 972.799805, consuming time:60.7898 s\n",
      "[epoch 1191]: training loss: 1186.773682, consuming time:60.6165 s\n",
      "[epoch 1192]: training loss: 969.481995, consuming time:60.6684 s\n",
      "[epoch 1193]: training loss: 1030.541748, consuming time:60.5025 s\n",
      "[epoch 1194]: training loss: 1068.388428, consuming time:60.0856 s\n",
      "[epoch 1195]: training loss: 1375.187378, consuming time:60.8128 s\n",
      "[epoch 1196]: training loss: 914.390625, consuming time:60.5543 s\n",
      "[epoch 1197]: training loss: 921.150879, consuming time:60.6108 s\n",
      "[epoch 1198]: training loss: 1169.654053, consuming time:60.4005 s\n",
      "[epoch 1199]: training loss: 1070.544189, consuming time:60.2762 s\n",
      "[epoch 1200]: training loss: 1033.780518, consuming time:60.3249 s\n",
      "[epoch 1201]: training loss: 898.537292, consuming time:60.4175 s\n",
      "[epoch 1202]: training loss: 1122.144409, consuming time:60.7599 s\n",
      "[epoch 1203]: training loss: 1075.748901, consuming time:60.4804 s\n",
      "[epoch 1204]: training loss: 869.714111, consuming time:60.5810 s\n",
      "[epoch 1205]: training loss: 1171.641602, consuming time:60.2469 s\n",
      "[epoch 1206]: training loss: 1422.688477, consuming time:60.3319 s\n",
      "[epoch 1207]: training loss: 1359.507935, consuming time:60.5306 s\n",
      "[epoch 1208]: training loss: 1002.407898, consuming time:60.5652 s\n",
      "[epoch 1209]: training loss: 1253.202637, consuming time:60.6381 s\n",
      "[epoch 1210]: training loss: 1040.161377, consuming time:60.4469 s\n",
      "[epoch 1211]: training loss: 1004.804077, consuming time:60.0805 s\n",
      "[epoch 1212]: training loss: 980.434570, consuming time:60.3490 s\n",
      "[epoch 1213]: training loss: 1428.582764, consuming time:60.4164 s\n",
      "[epoch 1214]: training loss: 870.687622, consuming time:60.5837 s\n",
      "[epoch 1215]: training loss: 1409.705811, consuming time:60.7471 s\n",
      "[epoch 1216]: training loss: 778.067139, consuming time:60.5338 s\n",
      "[epoch 1217]: training loss: 794.540283, consuming time:60.1802 s\n",
      "[epoch 1218]: training loss: 1181.181152, consuming time:60.3529 s\n",
      "[epoch 1219]: training loss: 1322.649536, consuming time:60.4404 s\n",
      "[epoch 1220]: training loss: 1325.383667, consuming time:60.8460 s\n",
      "[epoch 1221]: training loss: 1086.511475, consuming time:60.7082 s\n",
      "[epoch 1222]: training loss: 1064.130737, consuming time:60.4996 s\n",
      "[epoch 1223]: training loss: 853.385498, consuming time:59.9778 s\n",
      "[epoch 1224]: training loss: 936.278076, consuming time:60.4606 s\n",
      "[epoch 1225]: training loss: 1451.539551, consuming time:60.5449 s\n",
      "[epoch 1226]: training loss: 1070.006348, consuming time:60.3352 s\n",
      "[epoch 1227]: training loss: 1075.448975, consuming time:60.8158 s\n",
      "[epoch 1228]: training loss: 884.944946, consuming time:60.4283 s\n",
      "[epoch 1229]: training loss: 1239.879761, consuming time:60.1924 s\n",
      "[epoch 1230]: training loss: 1292.910645, consuming time:60.3983 s\n",
      "[epoch 1231]: training loss: 1193.177002, consuming time:60.4595 s\n",
      "[epoch 1232]: training loss: 857.338745, consuming time:60.6926 s\n",
      "[epoch 1233]: training loss: 1320.982544, consuming time:60.6772 s\n",
      "[epoch 1234]: training loss: 1140.822021, consuming time:60.2715 s\n",
      "[epoch 1235]: training loss: 793.668396, consuming time:60.0220 s\n",
      "[epoch 1236]: training loss: 1049.572388, consuming time:60.5591 s\n",
      "[epoch 1237]: training loss: 1004.937012, consuming time:60.3429 s\n",
      "[epoch 1238]: training loss: 1042.535034, consuming time:60.9129 s\n",
      "[epoch 1239]: training loss: 809.313232, consuming time:60.8938 s\n",
      "[epoch 1240]: training loss: 1124.563477, consuming time:60.3050 s\n",
      "[epoch 1241]: training loss: 920.144653, consuming time:59.9155 s\n",
      "[epoch 1242]: training loss: 1071.593018, consuming time:60.3848 s\n",
      "[epoch 1243]: training loss: 1055.189575, consuming time:60.3467 s\n",
      "[epoch 1244]: training loss: 928.825806, consuming time:60.7599 s\n",
      "[epoch 1245]: training loss: 1029.474365, consuming time:60.3484 s\n",
      "[epoch 1246]: training loss: 1357.782837, consuming time:60.4519 s\n",
      "[epoch 1247]: training loss: 1004.909424, consuming time:60.2908 s\n",
      "[epoch 1248]: training loss: 1035.525635, consuming time:60.3648 s\n",
      "[epoch 1249]: training loss: 920.184082, consuming time:60.4462 s\n",
      "[epoch 1250]: training loss: 945.296509, consuming time:60.5251 s\n",
      "[epoch 1251]: training loss: 989.514038, consuming time:60.8814 s\n",
      "[epoch 1252]: training loss: 986.603821, consuming time:60.2500 s\n",
      "[epoch 1253]: training loss: 1109.924683, consuming time:59.8466 s\n",
      "[epoch 1254]: training loss: 1088.747070, consuming time:60.3692 s\n",
      "[epoch 1255]: training loss: 979.749756, consuming time:60.3710 s\n",
      "[epoch 1256]: training loss: 1179.089966, consuming time:60.7219 s\n",
      "[epoch 1257]: training loss: 1040.691406, consuming time:60.7612 s\n",
      "[epoch 1258]: training loss: 885.891846, consuming time:60.3617 s\n",
      "[epoch 1259]: training loss: 1457.343994, consuming time:60.2412 s\n",
      "[epoch 1260]: training loss: 927.583252, consuming time:60.4299 s\n",
      "[epoch 1261]: training loss: 1174.909302, consuming time:60.2261 s\n",
      "[epoch 1262]: training loss: 873.868713, consuming time:60.8453 s\n",
      "[epoch 1263]: training loss: 1292.822876, consuming time:60.7551 s\n",
      "[epoch 1264]: training loss: 922.896973, consuming time:60.3630 s\n",
      "[epoch 1265]: training loss: 897.663086, consuming time:60.3951 s\n",
      "[epoch 1266]: training loss: 997.521545, consuming time:60.6395 s\n",
      "[epoch 1267]: training loss: 1047.007446, consuming time:60.4504 s\n",
      "[epoch 1268]: training loss: 954.515869, consuming time:60.8732 s\n",
      "[epoch 1269]: training loss: 1202.062012, consuming time:60.6853 s\n",
      "[epoch 1270]: training loss: 1222.135620, consuming time:60.1840 s\n",
      "[epoch 1271]: training loss: 1030.641357, consuming time:60.3213 s\n",
      "[epoch 1272]: training loss: 1312.076172, consuming time:60.5218 s\n",
      "[epoch 1273]: training loss: 1240.573730, consuming time:60.4879 s\n",
      "[epoch 1274]: training loss: 1011.390564, consuming time:60.8791 s\n",
      "[epoch 1275]: training loss: 1326.670288, consuming time:60.6732 s\n",
      "[epoch 1276]: training loss: 1215.544922, consuming time:60.5402 s\n",
      "[epoch 1277]: training loss: 1347.421143, consuming time:60.4628 s\n",
      "[epoch 1278]: training loss: 1166.631836, consuming time:60.5085 s\n",
      "[epoch 1279]: training loss: 1235.601929, consuming time:60.6737 s\n",
      "[epoch 1280]: training loss: 903.453064, consuming time:60.3051 s\n",
      "[epoch 1281]: training loss: 1119.617065, consuming time:60.6055 s\n",
      "[epoch 1282]: training loss: 865.950562, consuming time:60.2017 s\n",
      "[epoch 1283]: training loss: 1040.309814, consuming time:60.1670 s\n",
      "[epoch 1284]: training loss: 807.481079, consuming time:60.6249 s\n",
      "[epoch 1285]: training loss: 1383.996704, consuming time:60.3399 s\n",
      "[epoch 1286]: training loss: 965.009033, consuming time:60.7311 s\n",
      "[epoch 1287]: training loss: 869.618591, consuming time:60.6083 s\n",
      "[epoch 1288]: training loss: 1140.068237, consuming time:59.9724 s\n",
      "[epoch 1289]: training loss: 909.888123, consuming time:60.3077 s\n",
      "[epoch 1290]: training loss: 918.131470, consuming time:60.2803 s\n",
      "[epoch 1291]: training loss: 1048.109375, consuming time:60.6830 s\n",
      "[epoch 1292]: training loss: 1469.453857, consuming time:60.7568 s\n",
      "[epoch 1293]: training loss: 1198.161133, consuming time:60.2822 s\n",
      "[epoch 1294]: training loss: 965.870422, consuming time:60.0310 s\n",
      "[epoch 1295]: training loss: 1313.476074, consuming time:60.2158 s\n",
      "[epoch 1296]: training loss: 884.496094, consuming time:60.6832 s\n",
      "[epoch 1297]: training loss: 1010.233521, consuming time:60.5768 s\n",
      "[epoch 1298]: training loss: 952.446411, consuming time:60.8169 s\n",
      "[epoch 1299]: training loss: 1047.557617, consuming time:60.7072 s\n",
      "[epoch 1300]: training loss: 1079.632080, consuming time:60.3101 s\n",
      "[epoch 1301]: training loss: 1032.521851, consuming time:60.2887 s\n",
      "[epoch 1302]: training loss: 1107.472656, consuming time:60.7365 s\n",
      "[epoch 1303]: training loss: 711.380188, consuming time:60.9610 s\n",
      "[epoch 1304]: training loss: 911.296509, consuming time:60.7403 s\n",
      "[epoch 1305]: training loss: 1502.456421, consuming time:60.1997 s\n",
      "[epoch 1306]: training loss: 1125.701172, consuming time:60.6049 s\n",
      "[epoch 1307]: training loss: 980.256104, consuming time:60.4179 s\n",
      "[epoch 1308]: training loss: 956.358398, consuming time:60.5850 s\n",
      "[epoch 1309]: training loss: 881.437500, consuming time:60.5293 s\n",
      "[epoch 1310]: training loss: 1387.302979, consuming time:60.6848 s\n",
      "[epoch 1311]: training loss: 724.922241, consuming time:60.2612 s\n",
      "[epoch 1312]: training loss: 1212.515015, consuming time:60.3158 s\n",
      "[epoch 1313]: training loss: 893.655029, consuming time:60.4194 s\n",
      "[epoch 1314]: training loss: 970.237000, consuming time:60.9973 s\n",
      "[epoch 1315]: training loss: 782.704468, consuming time:60.4164 s\n",
      "[epoch 1316]: training loss: 846.133850, consuming time:60.4369 s\n",
      "[epoch 1317]: training loss: 1139.454590, consuming time:60.1364 s\n",
      "[epoch 1318]: training loss: 1210.998291, consuming time:60.1776 s\n",
      "[epoch 1319]: training loss: 1105.998047, consuming time:60.3876 s\n",
      "[epoch 1320]: training loss: 1119.383545, consuming time:60.7109 s\n",
      "[epoch 1321]: training loss: 875.504395, consuming time:60.5606 s\n",
      "[epoch 1322]: training loss: 1286.156006, consuming time:60.6617 s\n",
      "[epoch 1323]: training loss: 1319.477051, consuming time:60.7176 s\n",
      "[epoch 1324]: training loss: 937.189697, consuming time:61.1352 s\n",
      "[epoch 1325]: training loss: 1246.768677, consuming time:61.0896 s\n",
      "[epoch 1326]: training loss: 1101.218262, consuming time:60.7498 s\n",
      "[epoch 1327]: training loss: 1165.253784, consuming time:61.0042 s\n",
      "[epoch 1328]: training loss: 1061.188599, consuming time:60.8101 s\n",
      "[epoch 1329]: training loss: 1230.658203, consuming time:60.4576 s\n",
      "[epoch 1330]: training loss: 967.291016, consuming time:62.3867 s\n",
      "[epoch 1331]: training loss: 803.892822, consuming time:67.4993 s\n",
      "[epoch 1332]: training loss: 1056.336914, consuming time:62.8695 s\n",
      "[epoch 1333]: training loss: 941.767334, consuming time:61.4971 s\n",
      "[epoch 1334]: training loss: 905.606079, consuming time:60.6709 s\n",
      "[epoch 1335]: training loss: 977.654358, consuming time:60.8456 s\n",
      "[epoch 1336]: training loss: 1197.301636, consuming time:60.9260 s\n",
      "[epoch 1337]: training loss: 967.522827, consuming time:60.7759 s\n",
      "[epoch 1338]: training loss: 1002.495972, consuming time:60.7679 s\n",
      "[epoch 1339]: training loss: 782.137146, consuming time:60.6148 s\n",
      "[epoch 1340]: training loss: 920.556274, consuming time:60.5014 s\n",
      "[epoch 1341]: training loss: 876.157227, consuming time:60.7320 s\n",
      "[epoch 1342]: training loss: 773.680542, consuming time:60.5681 s\n",
      "[epoch 1343]: training loss: 1299.477295, consuming time:60.9302 s\n",
      "[epoch 1344]: training loss: 1027.220337, consuming time:61.1753 s\n",
      "[epoch 1345]: training loss: 1066.116943, consuming time:60.6687 s\n",
      "[epoch 1346]: training loss: 995.388062, consuming time:60.5792 s\n",
      "[epoch 1347]: training loss: 912.554688, consuming time:61.0329 s\n",
      "[epoch 1348]: training loss: 1046.479736, consuming time:60.5160 s\n",
      "[epoch 1349]: training loss: 929.668945, consuming time:60.7903 s\n",
      "[epoch 1350]: training loss: 1394.419800, consuming time:60.8439 s\n",
      "[epoch 1351]: training loss: 1090.176758, consuming time:60.6893 s\n",
      "[epoch 1352]: training loss: 871.917969, consuming time:60.5358 s\n",
      "[epoch 1353]: training loss: 902.008301, consuming time:60.8951 s\n",
      "[epoch 1354]: training loss: 760.372314, consuming time:60.6561 s\n",
      "[epoch 1355]: training loss: 1007.237732, consuming time:61.0809 s\n",
      "[epoch 1356]: training loss: 872.139709, consuming time:60.7913 s\n",
      "[epoch 1357]: training loss: 1117.628906, consuming time:62.0840 s\n",
      "[epoch 1358]: training loss: 1053.140137, consuming time:60.7466 s\n",
      "[epoch 1359]: training loss: 1440.289795, consuming time:60.9761 s\n",
      "[epoch 1360]: training loss: 1048.444580, consuming time:63.3765 s\n",
      "[epoch 1361]: training loss: 997.871826, consuming time:60.8498 s\n",
      "[epoch 1362]: training loss: 1109.532715, consuming time:63.6159 s\n",
      "[epoch 1363]: training loss: 922.859497, consuming time:63.2273 s\n",
      "[epoch 1364]: training loss: 1259.742432, consuming time:61.7980 s\n",
      "[epoch 1365]: training loss: 1052.802490, consuming time:61.3759 s\n",
      "[epoch 1366]: training loss: 991.025635, consuming time:60.5738 s\n",
      "[epoch 1367]: training loss: 1226.440552, consuming time:61.4763 s\n",
      "[epoch 1368]: training loss: 1289.879883, consuming time:60.9264 s\n",
      "[epoch 1369]: training loss: 1077.464844, consuming time:61.0427 s\n",
      "[epoch 1370]: training loss: 893.362183, consuming time:60.9863 s\n",
      "[epoch 1371]: training loss: 996.671326, consuming time:60.3906 s\n",
      "[epoch 1372]: training loss: 924.553772, consuming time:60.7799 s\n",
      "[epoch 1373]: training loss: 974.142334, consuming time:63.0301 s\n",
      "[epoch 1374]: training loss: 968.125549, consuming time:61.3267 s\n",
      "[epoch 1375]: training loss: 986.903015, consuming time:62.9256 s\n",
      "[epoch 1376]: training loss: 1200.747803, consuming time:61.7220 s\n",
      "[epoch 1377]: training loss: 896.890991, consuming time:61.9353 s\n",
      "[epoch 1378]: training loss: 977.732910, consuming time:63.6055 s\n",
      "[epoch 1379]: training loss: 1106.413574, consuming time:62.3427 s\n",
      "[epoch 1380]: training loss: 864.289856, consuming time:61.2304 s\n",
      "[epoch 1381]: training loss: 1342.175293, consuming time:61.1794 s\n",
      "[epoch 1382]: training loss: 877.355591, consuming time:60.9421 s\n",
      "[epoch 1383]: training loss: 1194.746338, consuming time:60.7723 s\n",
      "[epoch 1384]: training loss: 1035.560791, consuming time:62.4358 s\n",
      "[epoch 1385]: training loss: 1098.716797, consuming time:62.4724 s\n",
      "[epoch 1386]: training loss: 926.631226, consuming time:60.9444 s\n",
      "[epoch 1387]: training loss: 1340.060669, consuming time:60.8611 s\n",
      "[epoch 1388]: training loss: 773.521606, consuming time:60.9761 s\n",
      "[epoch 1389]: training loss: 841.500793, consuming time:61.3896 s\n",
      "[epoch 1390]: training loss: 1081.041992, consuming time:60.6881 s\n",
      "[epoch 1391]: training loss: 866.944214, consuming time:60.9233 s\n",
      "[epoch 1392]: training loss: 1087.777588, consuming time:61.0137 s\n",
      "[epoch 1393]: training loss: 962.147583, consuming time:61.2316 s\n",
      "[epoch 1394]: training loss: 1134.247559, consuming time:61.4898 s\n",
      "[epoch 1395]: training loss: 1008.186768, consuming time:61.1514 s\n",
      "[epoch 1396]: training loss: 921.840271, consuming time:60.3336 s\n",
      "[epoch 1397]: training loss: 1217.621826, consuming time:60.6423 s\n",
      "[epoch 1398]: training loss: 854.686523, consuming time:60.4353 s\n",
      "[epoch 1399]: training loss: 1025.384644, consuming time:60.4537 s\n",
      "[epoch 1400]: training loss: 963.814514, consuming time:60.6419 s\n",
      "[epoch 1401]: training loss: 1029.427246, consuming time:60.7124 s\n",
      "[epoch 1402]: training loss: 1118.674316, consuming time:60.6554 s\n",
      "[epoch 1403]: training loss: 1066.376221, consuming time:60.8702 s\n",
      "[epoch 1404]: training loss: 1124.922485, consuming time:64.7365 s\n",
      "[epoch 1405]: training loss: 1028.140381, consuming time:61.0890 s\n",
      "[epoch 1406]: training loss: 1251.969238, consuming time:60.3909 s\n",
      "[epoch 1407]: training loss: 888.133057, consuming time:60.6001 s\n",
      "[epoch 1408]: training loss: 1122.735474, consuming time:60.9149 s\n",
      "[epoch 1409]: training loss: 1117.603394, consuming time:62.5382 s\n",
      "[epoch 1410]: training loss: 973.470886, consuming time:61.7989 s\n",
      "[epoch 1411]: training loss: 1058.309937, consuming time:60.6610 s\n",
      "[epoch 1412]: training loss: 1238.046143, consuming time:60.7517 s\n",
      "[epoch 1413]: training loss: 1217.430908, consuming time:60.8556 s\n",
      "[epoch 1414]: training loss: 900.108093, consuming time:60.9201 s\n",
      "[epoch 1415]: training loss: 1172.322876, consuming time:61.0638 s\n",
      "[epoch 1416]: training loss: 1215.099609, consuming time:60.8811 s\n",
      "[epoch 1417]: training loss: 948.322388, consuming time:61.3549 s\n",
      "[epoch 1418]: training loss: 909.419006, consuming time:63.5548 s\n",
      "[epoch 1419]: training loss: 875.041748, consuming time:64.3995 s\n",
      "[epoch 1420]: training loss: 841.358704, consuming time:61.6925 s\n",
      "[epoch 1421]: training loss: 1071.798096, consuming time:61.0446 s\n",
      "[epoch 1422]: training loss: 1275.336792, consuming time:60.1640 s\n",
      "[epoch 1423]: training loss: 895.336853, consuming time:60.3814 s\n",
      "[epoch 1424]: training loss: 965.072876, consuming time:59.8324 s\n",
      "[epoch 1425]: training loss: 1100.692383, consuming time:60.2711 s\n",
      "[epoch 1426]: training loss: 912.934326, consuming time:60.0881 s\n",
      "[epoch 1427]: training loss: 1214.484863, consuming time:60.1609 s\n",
      "[epoch 1428]: training loss: 1183.282471, consuming time:60.2950 s\n",
      "[epoch 1429]: training loss: 1069.758423, consuming time:60.2285 s\n",
      "[epoch 1430]: training loss: 1345.591553, consuming time:59.7886 s\n",
      "[epoch 1431]: training loss: 750.710999, consuming time:60.2622 s\n",
      "[epoch 1432]: training loss: 1011.364990, consuming time:60.1725 s\n",
      "[epoch 1433]: training loss: 1053.974121, consuming time:60.4547 s\n",
      "[epoch 1434]: training loss: 1186.113281, consuming time:60.4745 s\n",
      "[epoch 1435]: training loss: 1108.801270, consuming time:60.4035 s\n",
      "[epoch 1436]: training loss: 1075.104492, consuming time:59.5760 s\n",
      "[epoch 1437]: training loss: 845.141541, consuming time:60.1998 s\n",
      "[epoch 1438]: training loss: 1138.824585, consuming time:60.4996 s\n",
      "[epoch 1439]: training loss: 1038.985474, consuming time:60.4391 s\n",
      "[epoch 1440]: training loss: 1059.335083, consuming time:60.6411 s\n",
      "[epoch 1441]: training loss: 882.081848, consuming time:60.2982 s\n",
      "[epoch 1442]: training loss: 1223.063232, consuming time:60.2137 s\n",
      "[epoch 1443]: training loss: 1260.720215, consuming time:60.5699 s\n",
      "[epoch 1444]: training loss: 774.827759, consuming time:60.8368 s\n",
      "[epoch 1445]: training loss: 1147.210327, consuming time:60.8083 s\n",
      "[epoch 1446]: training loss: 1019.086182, consuming time:60.4332 s\n",
      "[epoch 1447]: training loss: 914.753418, consuming time:60.5041 s\n",
      "[epoch 1448]: training loss: 869.606812, consuming time:60.2536 s\n",
      "[epoch 1449]: training loss: 721.877319, consuming time:60.9558 s\n",
      "[epoch 1450]: training loss: 648.410522, consuming time:60.5273 s\n",
      "[epoch 1451]: training loss: 726.004883, consuming time:60.5598 s\n",
      "[epoch 1452]: training loss: 1331.431641, consuming time:60.4003 s\n",
      "[epoch 1453]: training loss: 1237.976807, consuming time:60.3023 s\n",
      "[epoch 1454]: training loss: 854.898926, consuming time:59.8198 s\n",
      "[epoch 1455]: training loss: 1088.295898, consuming time:60.2688 s\n",
      "[epoch 1456]: training loss: 870.179260, consuming time:60.1489 s\n",
      "[epoch 1457]: training loss: 861.271484, consuming time:60.7613 s\n",
      "[epoch 1458]: training loss: 1023.140442, consuming time:60.4700 s\n",
      "[epoch 1459]: training loss: 1047.655518, consuming time:60.6343 s\n",
      "[epoch 1460]: training loss: 1166.247192, consuming time:60.0543 s\n",
      "[epoch 1461]: training loss: 982.412842, consuming time:60.4656 s\n",
      "[epoch 1462]: training loss: 1120.414062, consuming time:60.6713 s\n",
      "[epoch 1463]: training loss: 1086.957275, consuming time:60.4106 s\n",
      "[epoch 1464]: training loss: 970.600891, consuming time:60.4342 s\n",
      "[epoch 1465]: training loss: 1018.697876, consuming time:60.7546 s\n",
      "[epoch 1466]: training loss: 1395.745117, consuming time:60.1171 s\n",
      "[epoch 1467]: training loss: 1149.754150, consuming time:60.3500 s\n",
      "[epoch 1468]: training loss: 1041.631348, consuming time:60.6064 s\n",
      "[epoch 1469]: training loss: 1230.596436, consuming time:60.4930 s\n",
      "[epoch 1470]: training loss: 772.260498, consuming time:60.5171 s\n",
      "[epoch 1471]: training loss: 1193.859375, consuming time:60.5898 s\n",
      "[epoch 1472]: training loss: 939.035522, consuming time:60.3353 s\n",
      "[epoch 1473]: training loss: 895.297241, consuming time:60.5657 s\n",
      "[epoch 1474]: training loss: 909.895386, consuming time:60.6412 s\n",
      "[epoch 1475]: training loss: 770.756897, consuming time:60.5612 s\n",
      "[epoch 1476]: training loss: 1077.370972, consuming time:60.5358 s\n",
      "[epoch 1477]: training loss: 1014.489441, consuming time:60.4800 s\n",
      "[epoch 1478]: training loss: 1019.856689, consuming time:60.1586 s\n",
      "[epoch 1479]: training loss: 913.422668, consuming time:60.6027 s\n",
      "[epoch 1480]: training loss: 1116.151855, consuming time:60.6239 s\n",
      "[epoch 1481]: training loss: 1132.111328, consuming time:60.7145 s\n",
      "[epoch 1482]: training loss: 1001.723877, consuming time:60.4114 s\n",
      "[epoch 1483]: training loss: 966.401978, consuming time:60.5845 s\n",
      "[epoch 1484]: training loss: 905.475708, consuming time:60.1750 s\n",
      "[epoch 1485]: training loss: 1324.590942, consuming time:60.3713 s\n",
      "[epoch 1486]: training loss: 890.436035, consuming time:60.4852 s\n",
      "[epoch 1487]: training loss: 1056.326538, consuming time:60.4498 s\n",
      "[epoch 1488]: training loss: 1001.755432, consuming time:60.5720 s\n",
      "[epoch 1489]: training loss: 1579.733398, consuming time:60.2642 s\n",
      "[epoch 1490]: training loss: 919.672424, consuming time:60.0382 s\n",
      "[epoch 1491]: training loss: 921.217346, consuming time:60.6888 s\n",
      "[epoch 1492]: training loss: 1110.341064, consuming time:60.5028 s\n",
      "[epoch 1493]: training loss: 1039.773438, consuming time:60.6588 s\n",
      "[epoch 1494]: training loss: 1122.789429, consuming time:60.3712 s\n",
      "[epoch 1495]: training loss: 1440.093750, consuming time:60.2988 s\n",
      "[epoch 1496]: training loss: 1232.259521, consuming time:60.1605 s\n",
      "[epoch 1497]: training loss: 871.281860, consuming time:60.5481 s\n",
      "[epoch 1498]: training loss: 1139.536621, consuming time:60.7880 s\n",
      "[epoch 1499]: training loss: 934.037781, consuming time:60.5393 s\n",
      "[epoch 1500]: training loss: 1198.954346, consuming time:60.6221 s\n",
      "[epoch 1501]: training loss: 1065.298706, consuming time:60.3782 s\n",
      "[epoch 1502]: training loss: 1028.102051, consuming time:60.4021 s\n",
      "[epoch 1503]: training loss: 1198.179688, consuming time:60.5294 s\n",
      "[epoch 1504]: training loss: 911.548950, consuming time:60.6904 s\n",
      "[epoch 1505]: training loss: 840.725037, consuming time:60.8153 s\n",
      "[epoch 1506]: training loss: 960.927063, consuming time:60.2920 s\n",
      "[epoch 1507]: training loss: 1202.455322, consuming time:60.5734 s\n",
      "[epoch 1508]: training loss: 1014.761230, consuming time:60.5519 s\n",
      "[epoch 1509]: training loss: 1010.624268, consuming time:60.8686 s\n",
      "[epoch 1510]: training loss: 916.403625, consuming time:60.6236 s\n",
      "[epoch 1511]: training loss: 1052.969727, consuming time:60.5675 s\n",
      "[epoch 1512]: training loss: 988.687500, consuming time:60.4809 s\n",
      "[epoch 1513]: training loss: 892.389648, consuming time:60.3554 s\n",
      "[epoch 1514]: training loss: 992.881042, consuming time:60.5554 s\n",
      "[epoch 1515]: training loss: 774.521729, consuming time:60.6370 s\n",
      "[epoch 1516]: training loss: 994.801270, consuming time:60.5917 s\n",
      "[epoch 1517]: training loss: 1054.047363, consuming time:60.7514 s\n",
      "[epoch 1518]: training loss: 988.556580, consuming time:60.7065 s\n",
      "[epoch 1519]: training loss: 1088.627930, consuming time:60.3158 s\n",
      "[epoch 1520]: training loss: 1091.647095, consuming time:60.4061 s\n",
      "[epoch 1521]: training loss: 1074.604248, consuming time:60.3878 s\n",
      "[epoch 1522]: training loss: 1203.325195, consuming time:60.5449 s\n",
      "[epoch 1523]: training loss: 1092.542236, consuming time:60.7620 s\n",
      "[epoch 1524]: training loss: 847.044434, consuming time:60.4171 s\n",
      "[epoch 1525]: training loss: 1068.638916, consuming time:60.3372 s\n",
      "[epoch 1526]: training loss: 1095.378662, consuming time:60.5132 s\n",
      "[epoch 1527]: training loss: 1349.683838, consuming time:60.4057 s\n",
      "[epoch 1528]: training loss: 1091.031982, consuming time:60.7345 s\n",
      "[epoch 1529]: training loss: 945.995728, consuming time:60.5659 s\n",
      "[epoch 1530]: training loss: 785.089722, consuming time:60.3123 s\n",
      "[epoch 1531]: training loss: 1022.136780, consuming time:60.3860 s\n",
      "[epoch 1532]: training loss: 955.672180, consuming time:60.6212 s\n",
      "[epoch 1533]: training loss: 885.210144, consuming time:60.7527 s\n",
      "[epoch 1534]: training loss: 984.437744, consuming time:60.3908 s\n",
      "[epoch 1535]: training loss: 1428.010986, consuming time:60.6821 s\n",
      "[epoch 1536]: training loss: 1019.393188, consuming time:60.4209 s\n",
      "[epoch 1537]: training loss: 1153.999023, consuming time:60.4692 s\n",
      "[epoch 1538]: training loss: 713.530762, consuming time:60.4281 s\n",
      "[epoch 1539]: training loss: 967.941040, consuming time:60.3445 s\n",
      "[epoch 1540]: training loss: 996.945435, consuming time:60.4959 s\n",
      "[epoch 1541]: training loss: 860.266479, consuming time:60.6067 s\n",
      "[epoch 1542]: training loss: 1247.057861, consuming time:60.4641 s\n",
      "[epoch 1543]: training loss: 1154.256592, consuming time:60.0036 s\n",
      "[epoch 1544]: training loss: 1288.433594, consuming time:60.3722 s\n",
      "[epoch 1545]: training loss: 1026.929688, consuming time:60.7711 s\n",
      "[epoch 1546]: training loss: 935.513428, consuming time:60.4798 s\n",
      "[epoch 1547]: training loss: 937.511597, consuming time:60.3554 s\n",
      "[epoch 1548]: training loss: 1044.237915, consuming time:60.5073 s\n",
      "[epoch 1549]: training loss: 1140.385742, consuming time:59.9647 s\n",
      "[epoch 1550]: training loss: 924.545288, consuming time:60.5883 s\n",
      "[epoch 1551]: training loss: 1236.166016, consuming time:60.7655 s\n",
      "[epoch 1552]: training loss: 1170.733154, consuming time:60.5252 s\n",
      "[epoch 1553]: training loss: 1307.349121, consuming time:60.6977 s\n",
      "[epoch 1554]: training loss: 854.298645, consuming time:60.4249 s\n",
      "[epoch 1555]: training loss: 737.617615, consuming time:60.2197 s\n",
      "[epoch 1556]: training loss: 846.392822, consuming time:60.7379 s\n",
      "[epoch 1557]: training loss: 1057.537231, consuming time:60.4710 s\n",
      "[epoch 1558]: training loss: 1063.381348, consuming time:60.8134 s\n",
      "[epoch 1559]: training loss: 845.542603, consuming time:60.5705 s\n",
      "[epoch 1560]: training loss: 1023.457031, consuming time:60.3029 s\n",
      "[epoch 1561]: training loss: 1025.038574, consuming time:59.9058 s\n",
      "[epoch 1562]: training loss: 1137.926758, consuming time:60.5348 s\n",
      "[epoch 1563]: training loss: 1196.218506, consuming time:60.6084 s\n",
      "[epoch 1564]: training loss: 1061.360718, consuming time:60.5753 s\n",
      "[epoch 1565]: training loss: 1070.506592, consuming time:60.5539 s\n",
      "[epoch 1566]: training loss: 1035.535400, consuming time:60.7653 s\n",
      "[epoch 1567]: training loss: 1331.891846, consuming time:60.1830 s\n",
      "[epoch 1568]: training loss: 892.377502, consuming time:60.3893 s\n",
      "[epoch 1569]: training loss: 881.175049, consuming time:60.2959 s\n",
      "[epoch 1570]: training loss: 976.117676, consuming time:60.7150 s\n",
      "[epoch 1571]: training loss: 954.700623, consuming time:60.4898 s\n",
      "[epoch 1572]: training loss: 1115.215576, consuming time:60.4030 s\n",
      "[epoch 1573]: training loss: 973.491699, consuming time:59.9913 s\n",
      "[epoch 1574]: training loss: 857.592773, consuming time:60.4039 s\n",
      "[epoch 1575]: training loss: 1112.312256, consuming time:60.3302 s\n",
      "[epoch 1576]: training loss: 1046.759155, consuming time:60.6176 s\n",
      "[epoch 1577]: training loss: 1402.493652, consuming time:60.2137 s\n",
      "[epoch 1578]: training loss: 998.886230, consuming time:60.6510 s\n",
      "[epoch 1579]: training loss: 974.015747, consuming time:60.0093 s\n",
      "[epoch 1580]: training loss: 715.201843, consuming time:60.7136 s\n",
      "[epoch 1581]: training loss: 1240.947632, consuming time:60.6509 s\n",
      "[epoch 1582]: training loss: 1372.356079, consuming time:60.6697 s\n",
      "[epoch 1583]: training loss: 792.265503, consuming time:60.4795 s\n",
      "[epoch 1584]: training loss: 1032.354248, consuming time:60.4220 s\n",
      "[epoch 1585]: training loss: 597.148743, consuming time:60.0612 s\n",
      "[epoch 1586]: training loss: 1192.892822, consuming time:60.5233 s\n",
      "[epoch 1587]: training loss: 871.470947, consuming time:60.6695 s\n",
      "[epoch 1588]: training loss: 1320.295776, consuming time:60.5086 s\n",
      "[epoch 1589]: training loss: 934.087769, consuming time:60.7116 s\n",
      "[epoch 1590]: training loss: 978.544556, consuming time:60.5400 s\n",
      "[epoch 1591]: training loss: 1406.475586, consuming time:59.9805 s\n",
      "[epoch 1592]: training loss: 1055.043579, consuming time:60.5991 s\n",
      "[epoch 1593]: training loss: 1263.671631, consuming time:60.6778 s\n",
      "[epoch 1594]: training loss: 999.960571, consuming time:60.4358 s\n",
      "[epoch 1595]: training loss: 823.756287, consuming time:60.4909 s\n",
      "[epoch 1596]: training loss: 999.875366, consuming time:60.3183 s\n",
      "[epoch 1597]: training loss: 1075.847412, consuming time:59.9913 s\n",
      "[epoch 1598]: training loss: 1051.819702, consuming time:60.6830 s\n",
      "[epoch 1599]: training loss: 1286.472290, consuming time:60.4194 s\n",
      "[epoch 1600]: training loss: 897.964600, consuming time:60.8272 s\n",
      "[epoch 1601]: training loss: 818.745544, consuming time:60.5261 s\n",
      "[epoch 1602]: training loss: 1325.567627, consuming time:60.6137 s\n",
      "[epoch 1603]: training loss: 1105.318604, consuming time:60.1538 s\n",
      "[epoch 1604]: training loss: 915.807251, consuming time:60.3065 s\n",
      "[epoch 1605]: training loss: 1087.731934, consuming time:60.6893 s\n",
      "[epoch 1606]: training loss: 724.913086, consuming time:60.8024 s\n",
      "[epoch 1607]: training loss: 1204.868042, consuming time:60.5300 s\n",
      "[epoch 1608]: training loss: 1073.763672, consuming time:60.4190 s\n",
      "[epoch 1609]: training loss: 796.155884, consuming time:60.2026 s\n",
      "[epoch 1610]: training loss: 1022.366821, consuming time:60.2836 s\n",
      "[epoch 1611]: training loss: 1230.892090, consuming time:60.6046 s\n",
      "[epoch 1612]: training loss: 933.116272, consuming time:60.7691 s\n",
      "[epoch 1613]: training loss: 694.640869, consuming time:60.9301 s\n",
      "[epoch 1614]: training loss: 973.522888, consuming time:60.6477 s\n",
      "[epoch 1615]: training loss: 690.294128, consuming time:59.8614 s\n",
      "[epoch 1616]: training loss: 1003.267456, consuming time:60.2474 s\n",
      "[epoch 1617]: training loss: 1383.866333, consuming time:60.6752 s\n",
      "[epoch 1618]: training loss: 1038.761719, consuming time:60.6365 s\n",
      "[epoch 1619]: training loss: 1349.208862, consuming time:60.6640 s\n",
      "[epoch 1620]: training loss: 1104.998291, consuming time:60.3450 s\n",
      "[epoch 1621]: training loss: 1380.432861, consuming time:60.2402 s\n",
      "[epoch 1622]: training loss: 1106.666748, consuming time:60.4522 s\n",
      "[epoch 1623]: training loss: 987.608398, consuming time:60.7648 s\n",
      "[epoch 1624]: training loss: 1171.366211, consuming time:60.6134 s\n",
      "[epoch 1625]: training loss: 1184.523804, consuming time:60.7125 s\n",
      "[epoch 1626]: training loss: 1013.665955, consuming time:60.5251 s\n",
      "[epoch 1627]: training loss: 1407.344482, consuming time:60.5421 s\n",
      "[epoch 1628]: training loss: 1207.964233, consuming time:60.5441 s\n",
      "[epoch 1629]: training loss: 1291.721680, consuming time:60.4824 s\n",
      "[epoch 1630]: training loss: 752.754517, consuming time:60.5643 s\n",
      "[epoch 1631]: training loss: 974.211670, consuming time:60.7388 s\n",
      "[epoch 1632]: training loss: 1019.587891, consuming time:60.3669 s\n",
      "[epoch 1633]: training loss: 1374.635864, consuming time:60.2173 s\n",
      "[epoch 1634]: training loss: 890.509583, consuming time:60.7378 s\n",
      "[epoch 1635]: training loss: 967.762329, consuming time:60.4311 s\n",
      "[epoch 1636]: training loss: 1032.285400, consuming time:60.6376 s\n",
      "[epoch 1637]: training loss: 1103.112427, consuming time:60.4857 s\n",
      "[epoch 1638]: training loss: 997.793396, consuming time:60.3118 s\n",
      "[epoch 1639]: training loss: 1247.028320, consuming time:60.4524 s\n",
      "[epoch 1640]: training loss: 1060.131226, consuming time:60.2764 s\n",
      "[epoch 1641]: training loss: 1027.528564, consuming time:62.0276 s\n",
      "[epoch 1642]: training loss: 1062.422607, consuming time:62.0064 s\n",
      "[epoch 1643]: training loss: 1057.679321, consuming time:61.8746 s\n",
      "[epoch 1644]: training loss: 704.111633, consuming time:62.1141 s\n",
      "[epoch 1645]: training loss: 1061.351318, consuming time:62.3849 s\n",
      "[epoch 1646]: training loss: 1187.666260, consuming time:61.9592 s\n",
      "[epoch 1647]: training loss: 1259.502441, consuming time:61.5321 s\n",
      "[epoch 1648]: training loss: 713.611328, consuming time:61.5727 s\n",
      "[epoch 1649]: training loss: 826.434326, consuming time:62.1492 s\n",
      "[epoch 1650]: training loss: 1457.979858, consuming time:61.3791 s\n",
      "[epoch 1651]: training loss: 1149.718018, consuming time:61.8415 s\n",
      "[epoch 1652]: training loss: 726.257690, consuming time:61.6488 s\n",
      "[epoch 1653]: training loss: 1175.459473, consuming time:61.7923 s\n",
      "[epoch 1654]: training loss: 1134.482788, consuming time:61.3644 s\n",
      "[epoch 1655]: training loss: 756.082581, consuming time:60.5009 s\n",
      "[epoch 1656]: training loss: 981.942322, consuming time:61.4163 s\n",
      "[epoch 1657]: training loss: 1124.848145, consuming time:61.1920 s\n",
      "[epoch 1658]: training loss: 932.369263, consuming time:62.0721 s\n",
      "[epoch 1659]: training loss: 1214.493896, consuming time:61.4830 s\n",
      "[epoch 1660]: training loss: 1109.534058, consuming time:61.4643 s\n",
      "[epoch 1661]: training loss: 876.278687, consuming time:61.5538 s\n",
      "[epoch 1662]: training loss: 1163.768799, consuming time:60.6428 s\n",
      "[epoch 1663]: training loss: 891.828796, consuming time:60.4349 s\n",
      "[epoch 1664]: training loss: 710.899048, consuming time:60.4607 s\n",
      "[epoch 1665]: training loss: 859.225037, consuming time:60.6972 s\n",
      "[epoch 1666]: training loss: 845.974487, consuming time:60.6723 s\n",
      "[epoch 1667]: training loss: 1141.219238, consuming time:60.9406 s\n",
      "[epoch 1668]: training loss: 1007.955994, consuming time:60.7951 s\n",
      "[epoch 1669]: training loss: 971.143311, consuming time:60.9649 s\n",
      "[epoch 1670]: training loss: 1026.824829, consuming time:61.7619 s\n",
      "[epoch 1671]: training loss: 850.412109, consuming time:63.2888 s\n",
      "[epoch 1672]: training loss: 1260.009033, consuming time:64.3989 s\n",
      "[epoch 1673]: training loss: 917.220886, consuming time:62.1739 s\n",
      "[epoch 1674]: training loss: 1035.357544, consuming time:61.4273 s\n",
      "[epoch 1675]: training loss: 1098.610352, consuming time:61.2339 s\n",
      "[epoch 1676]: training loss: 996.282349, consuming time:61.1581 s\n",
      "[epoch 1677]: training loss: 748.281311, consuming time:61.3874 s\n",
      "[epoch 1678]: training loss: 885.785645, consuming time:61.2585 s\n",
      "[epoch 1679]: training loss: 1071.274536, consuming time:60.9665 s\n",
      "[epoch 1680]: training loss: 1038.782471, consuming time:61.0148 s\n",
      "[epoch 1681]: training loss: 1277.877563, consuming time:61.2707 s\n",
      "[epoch 1682]: training loss: 1116.885376, consuming time:61.0537 s\n",
      "[epoch 1683]: training loss: 1201.331299, consuming time:61.0566 s\n",
      "[epoch 1684]: training loss: 811.763123, consuming time:61.1959 s\n",
      "[epoch 1685]: training loss: 985.054810, consuming time:60.8119 s\n",
      "[epoch 1686]: training loss: 1056.219727, consuming time:61.4370 s\n",
      "[epoch 1687]: training loss: 1088.915771, consuming time:60.8885 s\n",
      "[epoch 1688]: training loss: 1016.260620, consuming time:61.1668 s\n",
      "[epoch 1689]: training loss: 1093.621094, consuming time:61.2720 s\n",
      "[epoch 1690]: training loss: 1007.167297, consuming time:61.3349 s\n",
      "[epoch 1691]: training loss: 979.019470, consuming time:62.5704 s\n",
      "[epoch 1692]: training loss: 1039.596191, consuming time:60.8974 s\n",
      "[epoch 1693]: training loss: 804.967773, consuming time:60.9614 s\n",
      "[epoch 1694]: training loss: 1150.780273, consuming time:61.0165 s\n",
      "[epoch 1695]: training loss: 912.646606, consuming time:60.9030 s\n",
      "[epoch 1696]: training loss: 1274.856934, consuming time:60.9546 s\n",
      "[epoch 1697]: training loss: 851.628418, consuming time:60.7281 s\n",
      "[epoch 1698]: training loss: 1020.429932, consuming time:61.0833 s\n",
      "[epoch 1699]: training loss: 1240.965210, consuming time:60.9398 s\n",
      "[epoch 1700]: training loss: 1155.302368, consuming time:61.0287 s\n",
      "[epoch 1701]: training loss: 832.171021, consuming time:62.6891 s\n",
      "[epoch 1702]: training loss: 1081.846802, consuming time:63.9432 s\n",
      "[epoch 1703]: training loss: 861.256165, consuming time:62.7326 s\n",
      "[epoch 1704]: training loss: 682.105835, consuming time:62.3579 s\n",
      "[epoch 1705]: training loss: 813.332458, consuming time:63.4028 s\n",
      "[epoch 1706]: training loss: 1035.492065, consuming time:61.0310 s\n",
      "[epoch 1707]: training loss: 929.351440, consuming time:61.0208 s\n",
      "[epoch 1708]: training loss: 1161.953613, consuming time:60.9029 s\n",
      "[epoch 1709]: training loss: 789.506226, consuming time:60.5294 s\n",
      "[epoch 1710]: training loss: 938.954590, consuming time:61.2686 s\n",
      "[epoch 1711]: training loss: 1090.489014, consuming time:60.3310 s\n",
      "[epoch 1712]: training loss: 1200.465332, consuming time:60.4490 s\n",
      "[epoch 1713]: training loss: 1421.174561, consuming time:60.9417 s\n",
      "[epoch 1714]: training loss: 1091.450195, consuming time:60.5784 s\n",
      "[epoch 1715]: training loss: 801.752808, consuming time:60.9846 s\n",
      "[epoch 1716]: training loss: 1118.678223, consuming time:60.7509 s\n",
      "[epoch 1717]: training loss: 1180.795166, consuming time:61.0401 s\n",
      "[epoch 1718]: training loss: 1122.443848, consuming time:61.1391 s\n",
      "[epoch 1719]: training loss: 1231.570923, consuming time:60.5531 s\n",
      "[epoch 1720]: training loss: 1051.674561, consuming time:60.6551 s\n",
      "[epoch 1721]: training loss: 910.788086, consuming time:61.0524 s\n",
      "[epoch 1722]: training loss: 1013.458740, consuming time:61.9052 s\n",
      "[epoch 1723]: training loss: 1121.441284, consuming time:62.3847 s\n",
      "[epoch 1724]: training loss: 1018.444031, consuming time:62.7609 s\n",
      "[epoch 1725]: training loss: 909.081787, consuming time:61.1225 s\n",
      "[epoch 1726]: training loss: 1290.251709, consuming time:61.5295 s\n",
      "[epoch 1727]: training loss: 1042.080566, consuming time:62.5985 s\n",
      "[epoch 1728]: training loss: 948.436768, consuming time:60.8447 s\n",
      "[epoch 1729]: training loss: 969.594666, consuming time:61.3491 s\n",
      "[epoch 1730]: training loss: 980.321533, consuming time:62.1534 s\n",
      "[epoch 1731]: training loss: 939.040161, consuming time:61.5459 s\n",
      "[epoch 1732]: training loss: 901.056519, consuming time:61.4076 s\n",
      "[epoch 1733]: training loss: 1020.819946, consuming time:61.3664 s\n",
      "[epoch 1734]: training loss: 1081.243408, consuming time:61.1819 s\n",
      "[epoch 1735]: training loss: 1014.112061, consuming time:62.5972 s\n",
      "[epoch 1736]: training loss: 932.640137, consuming time:62.0227 s\n",
      "[epoch 1737]: training loss: 1101.844238, consuming time:64.1824 s\n",
      "[epoch 1738]: training loss: 849.240723, consuming time:61.5134 s\n",
      "[epoch 1739]: training loss: 846.224854, consuming time:61.7231 s\n",
      "[epoch 1740]: training loss: 1129.133545, consuming time:61.8436 s\n",
      "[epoch 1741]: training loss: 988.997437, consuming time:61.8479 s\n",
      "[epoch 1742]: training loss: 1165.302002, consuming time:61.8423 s\n",
      "[epoch 1743]: training loss: 818.074829, consuming time:61.7007 s\n",
      "[epoch 1744]: training loss: 1087.952881, consuming time:61.5670 s\n",
      "[epoch 1745]: training loss: 984.189087, consuming time:62.7375 s\n",
      "[epoch 1746]: training loss: 1146.795654, consuming time:62.9621 s\n",
      "[epoch 1747]: training loss: 1320.103149, consuming time:63.9264 s\n",
      "[epoch 1748]: training loss: 901.866211, consuming time:61.7957 s\n",
      "[epoch 1749]: training loss: 1475.916260, consuming time:60.8136 s\n",
      "[epoch 1750]: training loss: 838.724365, consuming time:60.8012 s\n",
      "[epoch 1751]: training loss: 997.116943, consuming time:60.8252 s\n",
      "[epoch 1752]: training loss: 929.401367, consuming time:60.9808 s\n",
      "[epoch 1753]: training loss: 1473.834473, consuming time:60.6610 s\n",
      "[epoch 1754]: training loss: 972.689941, consuming time:60.7888 s\n",
      "[epoch 1755]: training loss: 1158.970703, consuming time:60.7507 s\n",
      "[epoch 1756]: training loss: 1133.996338, consuming time:60.8824 s\n",
      "[epoch 1757]: training loss: 1318.225830, consuming time:60.8805 s\n",
      "[epoch 1758]: training loss: 936.764038, consuming time:61.0038 s\n",
      "[epoch 1759]: training loss: 1043.371582, consuming time:60.9229 s\n",
      "[epoch 1760]: training loss: 889.385620, consuming time:61.1523 s\n",
      "[epoch 1761]: training loss: 906.991638, consuming time:60.5069 s\n",
      "[epoch 1762]: training loss: 1147.539062, consuming time:60.6905 s\n",
      "[epoch 1763]: training loss: 828.325073, consuming time:61.3327 s\n",
      "[epoch 1764]: training loss: 1082.497437, consuming time:61.0797 s\n",
      "[epoch 1765]: training loss: 1059.134766, consuming time:60.9070 s\n",
      "[epoch 1766]: training loss: 837.576721, consuming time:61.0823 s\n",
      "[epoch 1767]: training loss: 1176.917236, consuming time:60.1701 s\n",
      "[epoch 1768]: training loss: 952.345032, consuming time:60.6603 s\n",
      "[epoch 1769]: training loss: 912.001526, consuming time:60.9901 s\n",
      "[epoch 1770]: training loss: 909.756714, consuming time:61.8039 s\n",
      "[epoch 1771]: training loss: 1199.902954, consuming time:63.1107 s\n",
      "[epoch 1772]: training loss: 1083.826904, consuming time:60.9706 s\n",
      "[epoch 1773]: training loss: 1105.431152, consuming time:60.6572 s\n",
      "[epoch 1774]: training loss: 906.425049, consuming time:64.6741 s\n",
      "[epoch 1775]: training loss: 608.741699, consuming time:61.4428 s\n",
      "[epoch 1776]: training loss: 1102.726318, consuming time:61.3287 s\n",
      "[epoch 1777]: training loss: 810.096863, consuming time:61.1429 s\n",
      "[epoch 1778]: training loss: 889.732605, consuming time:60.3232 s\n",
      "[epoch 1779]: training loss: 1206.410400, consuming time:59.8324 s\n",
      "[epoch 1780]: training loss: 1067.378662, consuming time:60.1898 s\n",
      "[epoch 1781]: training loss: 872.491455, consuming time:60.0506 s\n",
      "[epoch 1782]: training loss: 861.250671, consuming time:60.0964 s\n",
      "[epoch 1783]: training loss: 1082.057739, consuming time:60.3252 s\n",
      "[epoch 1784]: training loss: 1402.038330, consuming time:60.1642 s\n",
      "[epoch 1785]: training loss: 1223.916016, consuming time:59.7291 s\n",
      "[epoch 1786]: training loss: 1049.452393, consuming time:60.4516 s\n",
      "[epoch 1787]: training loss: 1290.622803, consuming time:60.2823 s\n",
      "[epoch 1788]: training loss: 907.675354, consuming time:60.3890 s\n",
      "[epoch 1789]: training loss: 888.206909, consuming time:60.0555 s\n",
      "[epoch 1790]: training loss: 1178.488525, consuming time:60.0346 s\n",
      "[epoch 1791]: training loss: 972.875916, consuming time:59.7129 s\n",
      "[epoch 1792]: training loss: 849.760498, consuming time:60.1044 s\n",
      "[epoch 1793]: training loss: 1064.182495, consuming time:60.1822 s\n",
      "[epoch 1794]: training loss: 1058.762573, consuming time:59.8980 s\n",
      "[epoch 1795]: training loss: 888.643188, consuming time:60.3228 s\n",
      "[epoch 1796]: training loss: 940.406982, consuming time:60.3399 s\n",
      "[epoch 1797]: training loss: 1283.402832, consuming time:60.1941 s\n",
      "[epoch 1798]: training loss: 773.685059, consuming time:60.6507 s\n",
      "[epoch 1799]: training loss: 1122.651245, consuming time:60.5614 s\n",
      "[epoch 1800]: training loss: 986.237427, consuming time:60.5628 s\n",
      "[epoch 1801]: training loss: 1232.105713, consuming time:60.7679 s\n",
      "[epoch 1802]: training loss: 1173.246460, consuming time:60.6639 s\n",
      "[epoch 1803]: training loss: 1032.146240, consuming time:60.3718 s\n",
      "[epoch 1804]: training loss: 889.836548, consuming time:60.5296 s\n",
      "[epoch 1805]: training loss: 868.404358, consuming time:60.4341 s\n",
      "[epoch 1806]: training loss: 1217.808350, consuming time:60.5113 s\n",
      "[epoch 1807]: training loss: 803.658081, consuming time:60.5963 s\n",
      "[epoch 1808]: training loss: 799.998901, consuming time:60.2948 s\n",
      "[epoch 1809]: training loss: 1254.145508, consuming time:60.1980 s\n",
      "[epoch 1810]: training loss: 943.474243, consuming time:60.3673 s\n",
      "[epoch 1811]: training loss: 1179.378174, consuming time:60.4152 s\n",
      "[epoch 1812]: training loss: 1168.048828, consuming time:60.7261 s\n",
      "[epoch 1813]: training loss: 1018.787781, consuming time:60.7261 s\n",
      "[epoch 1814]: training loss: 1425.649658, consuming time:62.9857 s\n",
      "[epoch 1815]: training loss: 982.389771, consuming time:62.4493 s\n",
      "[epoch 1816]: training loss: 856.438477, consuming time:66.1248 s\n",
      "[epoch 1817]: training loss: 1014.466187, consuming time:61.4048 s\n",
      "[epoch 1818]: training loss: 971.621216, consuming time:63.3268 s\n",
      "[epoch 1819]: training loss: 1068.677002, consuming time:63.1086 s\n",
      "[epoch 1820]: training loss: 889.582642, consuming time:66.9252 s\n",
      "[epoch 1821]: training loss: 925.338135, consuming time:82.8939 s\n",
      "[epoch 1822]: training loss: 1514.239502, consuming time:79.9618 s\n",
      "[epoch 1823]: training loss: 941.032349, consuming time:137.4943 s\n",
      "[epoch 1824]: training loss: 1171.186890, consuming time:133.1882 s\n",
      "[epoch 1825]: training loss: 1122.284058, consuming time:130.0381 s\n",
      "[epoch 1826]: training loss: 780.789673, consuming time:125.0108 s\n",
      "[epoch 1827]: training loss: 812.648193, consuming time:130.9166 s\n",
      "[epoch 1828]: training loss: 1194.309570, consuming time:133.6285 s\n",
      "[epoch 1829]: training loss: 940.619690, consuming time:132.2590 s\n",
      "[epoch 1830]: training loss: 893.291626, consuming time:131.5518 s\n",
      "[epoch 1831]: training loss: 1113.063477, consuming time:124.6092 s\n",
      "[epoch 1832]: training loss: 1159.300049, consuming time:125.7634 s\n",
      "[epoch 1833]: training loss: 853.035400, consuming time:114.2917 s\n",
      "[epoch 1834]: training loss: 975.359558, consuming time:82.2214 s\n",
      "[epoch 1835]: training loss: 962.004578, consuming time:79.9399 s\n",
      "[epoch 1836]: training loss: 1140.420654, consuming time:146.1921 s\n",
      "[epoch 1837]: training loss: 1169.268799, consuming time:144.8862 s\n",
      "[epoch 1838]: training loss: 1120.381104, consuming time:145.5322 s\n",
      "[epoch 1839]: training loss: 999.891113, consuming time:118.6075 s\n",
      "[epoch 1840]: training loss: 847.356445, consuming time:81.4449 s\n",
      "[epoch 1841]: training loss: 1270.273804, consuming time:144.9611 s\n",
      "[epoch 1842]: training loss: 748.353516, consuming time:142.1072 s\n",
      "[epoch 1843]: training loss: 1999.780396, consuming time:138.5593 s\n",
      "[epoch 1844]: training loss: 1211.315674, consuming time:137.1784 s\n",
      "[epoch 1845]: training loss: 786.971802, consuming time:142.9872 s\n",
      "[epoch 1846]: training loss: 1020.032776, consuming time:137.3788 s\n",
      "[epoch 1847]: training loss: 908.833008, consuming time:136.8241 s\n",
      "[epoch 1848]: training loss: 975.742798, consuming time:82.5527 s\n",
      "[epoch 1849]: training loss: 1176.738159, consuming time:114.4549 s\n",
      "[epoch 1850]: training loss: 963.435608, consuming time:147.1463 s\n",
      "[epoch 1851]: training loss: 1242.551270, consuming time:141.0368 s\n",
      "[epoch 1852]: training loss: 963.730530, consuming time:139.6734 s\n",
      "[epoch 1853]: training loss: 981.235352, consuming time:143.9646 s\n",
      "[epoch 1854]: training loss: 994.344727, consuming time:140.6826 s\n",
      "[epoch 1855]: training loss: 895.823853, consuming time:141.1895 s\n",
      "[epoch 1856]: training loss: 880.810425, consuming time:146.4061 s\n",
      "[epoch 1857]: training loss: 919.625793, consuming time:142.1401 s\n",
      "[epoch 1858]: training loss: 817.425903, consuming time:141.4289 s\n",
      "[epoch 1859]: training loss: 1129.898438, consuming time:141.5672 s\n",
      "[epoch 1860]: training loss: 817.347595, consuming time:138.5185 s\n",
      "[epoch 1861]: training loss: 957.998047, consuming time:84.2598 s\n",
      "[epoch 1862]: training loss: 1063.578857, consuming time:60.7490 s\n",
      "[epoch 1863]: training loss: 1031.004395, consuming time:60.3275 s\n",
      "[epoch 1864]: training loss: 943.071106, consuming time:60.0588 s\n",
      "[epoch 1865]: training loss: 846.554565, consuming time:60.3546 s\n",
      "[epoch 1866]: training loss: 949.941589, consuming time:60.4303 s\n",
      "[epoch 1867]: training loss: 1023.361877, consuming time:60.3810 s\n",
      "[epoch 1868]: training loss: 884.904053, consuming time:60.7367 s\n",
      "[epoch 1869]: training loss: 1067.996094, consuming time:60.7190 s\n",
      "[epoch 1870]: training loss: 929.565979, consuming time:61.9381 s\n",
      "[epoch 1871]: training loss: 714.911987, consuming time:61.9774 s\n",
      "[epoch 1872]: training loss: 1107.152100, consuming time:60.9273 s\n",
      "[epoch 1873]: training loss: 818.285706, consuming time:62.6391 s\n",
      "[epoch 1874]: training loss: 772.372925, consuming time:61.4781 s\n",
      "[epoch 1875]: training loss: 1205.990967, consuming time:61.3024 s\n",
      "[epoch 1876]: training loss: 1183.955322, consuming time:61.3591 s\n",
      "[epoch 1877]: training loss: 1287.611816, consuming time:61.3394 s\n",
      "[epoch 1878]: training loss: 886.705933, consuming time:60.9995 s\n",
      "[epoch 1879]: training loss: 873.294861, consuming time:61.2314 s\n",
      "[epoch 1880]: training loss: 926.396301, consuming time:63.8690 s\n",
      "[epoch 1881]: training loss: 1198.830322, consuming time:62.5767 s\n",
      "[epoch 1882]: training loss: 1223.084595, consuming time:62.4432 s\n",
      "[epoch 1883]: training loss: 942.492554, consuming time:62.6361 s\n",
      "[epoch 1884]: training loss: 1358.098389, consuming time:61.3224 s\n",
      "[epoch 1885]: training loss: 818.940491, consuming time:62.4844 s\n",
      "[epoch 1886]: training loss: 1000.428406, consuming time:61.1166 s\n",
      "[epoch 1887]: training loss: 1019.243591, consuming time:60.7362 s\n",
      "[epoch 1888]: training loss: 931.228516, consuming time:60.5779 s\n",
      "[epoch 1889]: training loss: 1088.415527, consuming time:60.9160 s\n",
      "[epoch 1890]: training loss: 1082.831421, consuming time:60.9516 s\n",
      "[epoch 1891]: training loss: 1236.598633, consuming time:61.1218 s\n",
      "[epoch 1892]: training loss: 921.665161, consuming time:61.1417 s\n",
      "[epoch 1893]: training loss: 808.555969, consuming time:63.4507 s\n",
      "[epoch 1894]: training loss: 1047.923340, consuming time:62.1243 s\n",
      "[epoch 1895]: training loss: 814.615662, consuming time:60.8371 s\n",
      "[epoch 1896]: training loss: 1237.151001, consuming time:61.9705 s\n",
      "[epoch 1897]: training loss: 995.770203, consuming time:65.9589 s\n",
      "[epoch 1898]: training loss: 1025.044922, consuming time:63.0363 s\n",
      "[epoch 1899]: training loss: 758.573853, consuming time:61.6961 s\n",
      "[epoch 1900]: training loss: 741.949646, consuming time:60.6274 s\n",
      "[epoch 1901]: training loss: 834.479980, consuming time:60.6213 s\n",
      "[epoch 1902]: training loss: 938.622070, consuming time:61.0529 s\n",
      "[epoch 1903]: training loss: 1303.516235, consuming time:60.8357 s\n",
      "[epoch 1904]: training loss: 1270.621094, consuming time:61.9331 s\n",
      "[epoch 1905]: training loss: 983.023315, consuming time:61.4958 s\n",
      "[epoch 1906]: training loss: 1045.639893, consuming time:61.4980 s\n",
      "[epoch 1907]: training loss: 1374.883545, consuming time:61.2497 s\n",
      "[epoch 1908]: training loss: 1039.143311, consuming time:61.2206 s\n",
      "[epoch 1909]: training loss: 1031.083008, consuming time:61.6295 s\n",
      "[epoch 1910]: training loss: 1106.703979, consuming time:61.1291 s\n",
      "[epoch 1911]: training loss: 1182.778076, consuming time:61.2643 s\n",
      "[epoch 1912]: training loss: 1074.203857, consuming time:61.7342 s\n",
      "[epoch 1913]: training loss: 1190.113281, consuming time:61.6305 s\n",
      "[epoch 1914]: training loss: 851.022766, consuming time:61.1110 s\n",
      "[epoch 1915]: training loss: 1200.934204, consuming time:61.0059 s\n",
      "[epoch 1916]: training loss: 1079.975220, consuming time:61.2874 s\n",
      "[epoch 1917]: training loss: 1176.312744, consuming time:61.3557 s\n",
      "[epoch 1918]: training loss: 957.963623, consuming time:61.3298 s\n",
      "[epoch 1919]: training loss: 820.078369, consuming time:61.3993 s\n",
      "[epoch 1920]: training loss: 1197.385742, consuming time:61.7074 s\n",
      "[epoch 1921]: training loss: 1130.671387, consuming time:61.1068 s\n",
      "[epoch 1922]: training loss: 1091.821899, consuming time:61.9540 s\n",
      "[epoch 1923]: training loss: 698.558472, consuming time:61.5397 s\n",
      "[epoch 1924]: training loss: 749.384033, consuming time:61.1231 s\n",
      "[epoch 1925]: training loss: 1054.420776, consuming time:61.2439 s\n",
      "[epoch 1926]: training loss: 904.437256, consuming time:62.7300 s\n",
      "[epoch 1927]: training loss: 1211.502686, consuming time:62.4450 s\n",
      "[epoch 1928]: training loss: 1068.170288, consuming time:66.0499 s\n",
      "[epoch 1929]: training loss: 1080.349243, consuming time:67.8440 s\n",
      "[epoch 1930]: training loss: 1185.356934, consuming time:64.4706 s\n",
      "[epoch 1931]: training loss: 1047.874268, consuming time:65.3751 s\n",
      "[epoch 1932]: training loss: 1056.412598, consuming time:61.8945 s\n",
      "[epoch 1933]: training loss: 1116.922119, consuming time:61.2553 s\n",
      "[epoch 1934]: training loss: 1085.020264, consuming time:60.8988 s\n",
      "[epoch 1935]: training loss: 954.697632, consuming time:67.8978 s\n",
      "[epoch 1936]: training loss: 1011.628357, consuming time:66.1773 s\n",
      "[epoch 1937]: training loss: 1191.106689, consuming time:65.3585 s\n",
      "[epoch 1938]: training loss: 1150.176147, consuming time:67.4977 s\n",
      "[epoch 1939]: training loss: 959.257568, consuming time:63.2686 s\n",
      "[epoch 1940]: training loss: 884.260315, consuming time:62.3530 s\n",
      "[epoch 1941]: training loss: 890.703491, consuming time:65.9504 s\n",
      "[epoch 1942]: training loss: 1081.320068, consuming time:64.1532 s\n",
      "[epoch 1943]: training loss: 1279.550049, consuming time:61.6837 s\n",
      "[epoch 1944]: training loss: 828.238525, consuming time:64.3071 s\n",
      "[epoch 1945]: training loss: 1216.126587, consuming time:61.1937 s\n",
      "[epoch 1946]: training loss: 598.294983, consuming time:61.4046 s\n",
      "[epoch 1947]: training loss: 1497.248291, consuming time:61.6908 s\n",
      "[epoch 1948]: training loss: 1238.940796, consuming time:60.8537 s\n",
      "[epoch 1949]: training loss: 975.766174, consuming time:61.2544 s\n",
      "[epoch 1950]: training loss: 922.792297, consuming time:62.4667 s\n",
      "[epoch 1951]: training loss: 1027.874390, consuming time:63.8313 s\n",
      "[epoch 1952]: training loss: 1116.695557, consuming time:62.6761 s\n",
      "[epoch 1953]: training loss: 1092.495972, consuming time:64.7078 s\n",
      "[epoch 1954]: training loss: 1324.280884, consuming time:66.4340 s\n",
      "[epoch 1955]: training loss: 985.743225, consuming time:64.3631 s\n",
      "[epoch 1956]: training loss: 1109.914551, consuming time:63.3943 s\n",
      "[epoch 1957]: training loss: 1117.949707, consuming time:62.1750 s\n",
      "[epoch 1958]: training loss: 870.445923, consuming time:62.9203 s\n",
      "[epoch 1959]: training loss: 907.720581, consuming time:61.4349 s\n",
      "[epoch 1960]: training loss: 1018.122559, consuming time:61.3896 s\n",
      "[epoch 1961]: training loss: 1212.886597, consuming time:61.6690 s\n",
      "[epoch 1962]: training loss: 856.900879, consuming time:61.1307 s\n",
      "[epoch 1963]: training loss: 740.002380, consuming time:61.3663 s\n",
      "[epoch 1964]: training loss: 758.003174, consuming time:61.4528 s\n",
      "[epoch 1965]: training loss: 1144.673584, consuming time:61.1294 s\n",
      "[epoch 1966]: training loss: 1003.164062, consuming time:61.2283 s\n",
      "[epoch 1967]: training loss: 1121.873901, consuming time:60.9558 s\n",
      "[epoch 1968]: training loss: 1096.981812, consuming time:61.1669 s\n",
      "[epoch 1969]: training loss: 1053.736206, consuming time:61.0493 s\n",
      "[epoch 1970]: training loss: 937.066895, consuming time:60.9347 s\n",
      "[epoch 1971]: training loss: 1185.589111, consuming time:61.0962 s\n",
      "[epoch 1972]: training loss: 1066.034912, consuming time:60.8411 s\n",
      "[epoch 1973]: training loss: 907.013550, consuming time:60.7713 s\n",
      "[epoch 1974]: training loss: 1055.341064, consuming time:60.9189 s\n",
      "[epoch 1975]: training loss: 885.337524, consuming time:60.7103 s\n",
      "[epoch 1976]: training loss: 1045.730957, consuming time:60.5636 s\n",
      "[epoch 1977]: training loss: 1140.178345, consuming time:60.5924 s\n",
      "[epoch 1978]: training loss: 1245.308594, consuming time:60.5451 s\n",
      "[epoch 1979]: training loss: 1075.567627, consuming time:60.7234 s\n",
      "[epoch 1980]: training loss: 866.710938, consuming time:60.2193 s\n",
      "[epoch 1981]: training loss: 1171.108276, consuming time:61.6772 s\n",
      "[epoch 1982]: training loss: 989.934082, consuming time:65.3964 s\n",
      "[epoch 1983]: training loss: 762.389832, consuming time:61.0716 s\n",
      "[epoch 1984]: training loss: 910.514587, consuming time:60.9269 s\n",
      "[epoch 1985]: training loss: 825.934814, consuming time:60.5523 s\n",
      "[epoch 1986]: training loss: 846.798950, consuming time:60.5643 s\n",
      "[epoch 1987]: training loss: 1271.098633, consuming time:61.1664 s\n",
      "[epoch 1988]: training loss: 810.667664, consuming time:60.6949 s\n",
      "[epoch 1989]: training loss: 745.971558, consuming time:60.7936 s\n",
      "[epoch 1990]: training loss: 738.711304, consuming time:60.8943 s\n",
      "[epoch 1991]: training loss: 1029.113892, consuming time:61.0885 s\n",
      "[epoch 1992]: training loss: 1207.804688, consuming time:61.4274 s\n",
      "[epoch 1993]: training loss: 1089.453125, consuming time:61.3023 s\n",
      "[epoch 1994]: training loss: 885.845459, consuming time:61.3321 s\n",
      "[epoch 1995]: training loss: 962.012329, consuming time:61.4362 s\n",
      "[epoch 1996]: training loss: 950.119751, consuming time:63.0426 s\n",
      "[epoch 1997]: training loss: 972.357666, consuming time:66.7829 s\n",
      "[epoch 1998]: training loss: 1175.208740, consuming time:61.8576 s\n",
      "[epoch 1999]: training loss: 1089.948486, consuming time:61.1264 s\n",
      "[epoch 2000]: training loss: 975.396790, consuming time:68.9014 s\n",
      "[epoch 2001]: training loss: 969.742004, consuming time:65.5799 s\n",
      "[epoch 2002]: training loss: 1591.666138, consuming time:63.2684 s\n",
      "[epoch 2003]: training loss: 1073.473389, consuming time:65.3587 s\n",
      "[epoch 2004]: training loss: 935.598328, consuming time:63.9568 s\n",
      "[epoch 2005]: training loss: 863.189880, consuming time:66.4334 s\n",
      "[epoch 2006]: training loss: 974.460327, consuming time:65.2252 s\n",
      "[epoch 2007]: training loss: 1233.721924, consuming time:65.5406 s\n",
      "[epoch 2008]: training loss: 1017.297668, consuming time:67.7219 s\n",
      "[epoch 2009]: training loss: 1106.976562, consuming time:65.4109 s\n",
      "[epoch 2010]: training loss: 1183.432983, consuming time:66.5586 s\n",
      "[epoch 2011]: training loss: 1135.144409, consuming time:63.3184 s\n",
      "[epoch 2012]: training loss: 1013.481689, consuming time:65.0306 s\n",
      "[epoch 2013]: training loss: 961.342773, consuming time:60.4109 s\n",
      "[epoch 2014]: training loss: 983.958191, consuming time:61.7924 s\n",
      "[epoch 2015]: training loss: 998.078430, consuming time:61.8908 s\n",
      "[epoch 2016]: training loss: 1006.146973, consuming time:61.6299 s\n",
      "[epoch 2017]: training loss: 1032.353516, consuming time:61.3027 s\n",
      "[epoch 2018]: training loss: 842.331116, consuming time:61.3639 s\n",
      "[epoch 2019]: training loss: 992.065247, consuming time:61.7999 s\n",
      "[epoch 2020]: training loss: 1030.977417, consuming time:61.5541 s\n",
      "[epoch 2021]: training loss: 683.937927, consuming time:62.0816 s\n",
      "[epoch 2022]: training loss: 762.557129, consuming time:61.0564 s\n",
      "[epoch 2023]: training loss: 1158.240723, consuming time:61.5212 s\n",
      "[epoch 2024]: training loss: 854.565552, consuming time:61.2670 s\n",
      "[epoch 2025]: training loss: 1168.800659, consuming time:61.4596 s\n",
      "[epoch 2026]: training loss: 812.496033, consuming time:61.3049 s\n",
      "[epoch 2027]: training loss: 910.322266, consuming time:60.9278 s\n",
      "[epoch 2028]: training loss: 925.102173, consuming time:60.6672 s\n",
      "[epoch 2029]: training loss: 957.205200, consuming time:60.6107 s\n",
      "[epoch 2030]: training loss: 845.445007, consuming time:60.8020 s\n",
      "[epoch 2031]: training loss: 1071.519531, consuming time:60.7142 s\n",
      "[epoch 2032]: training loss: 741.198425, consuming time:60.5063 s\n",
      "[epoch 2033]: training loss: 907.916504, consuming time:60.3806 s\n",
      "[epoch 2034]: training loss: 891.642944, consuming time:60.7084 s\n",
      "[epoch 2035]: training loss: 1407.583252, consuming time:60.8798 s\n",
      "[epoch 2036]: training loss: 979.323425, consuming time:60.7616 s\n",
      "[epoch 2037]: training loss: 916.139160, consuming time:60.8155 s\n",
      "[epoch 2038]: training loss: 982.042114, consuming time:60.5854 s\n",
      "[epoch 2039]: training loss: 924.021545, consuming time:60.6127 s\n",
      "[epoch 2040]: training loss: 969.123779, consuming time:61.5230 s\n",
      "[epoch 2041]: training loss: 866.664062, consuming time:60.9540 s\n",
      "[epoch 2042]: training loss: 918.915771, consuming time:61.0423 s\n",
      "[epoch 2043]: training loss: 856.238892, consuming time:60.8934 s\n",
      "[epoch 2044]: training loss: 870.163696, consuming time:61.2588 s\n",
      "[epoch 2045]: training loss: 1196.619629, consuming time:69.5212 s\n",
      "[epoch 2046]: training loss: 1032.809326, consuming time:64.3785 s\n",
      "[epoch 2047]: training loss: 1092.988159, consuming time:66.2472 s\n",
      "[epoch 2048]: training loss: 873.913574, consuming time:66.1166 s\n",
      "[epoch 2049]: training loss: 879.324707, consuming time:66.5991 s\n",
      "[epoch 2050]: training loss: 972.775085, consuming time:66.3753 s\n",
      "[epoch 2051]: training loss: 1212.798828, consuming time:65.8832 s\n",
      "[epoch 2052]: training loss: 1009.103333, consuming time:67.5808 s\n",
      "[epoch 2053]: training loss: 775.026123, consuming time:65.5361 s\n",
      "[epoch 2054]: training loss: 1243.721924, consuming time:66.3050 s\n",
      "[epoch 2055]: training loss: 790.921509, consuming time:66.7698 s\n",
      "[epoch 2056]: training loss: 1265.829834, consuming time:63.1939 s\n",
      "[epoch 2057]: training loss: 1081.070557, consuming time:64.0058 s\n",
      "[epoch 2058]: training loss: 1024.015747, consuming time:61.7460 s\n",
      "[epoch 2059]: training loss: 699.994629, consuming time:61.6815 s\n",
      "[epoch 2060]: training loss: 1064.653442, consuming time:61.5737 s\n",
      "[epoch 2061]: training loss: 1115.200317, consuming time:61.4423 s\n",
      "[epoch 2062]: training loss: 929.084229, consuming time:62.0276 s\n",
      "[epoch 2063]: training loss: 1046.872070, consuming time:61.7628 s\n",
      "[epoch 2064]: training loss: 970.208008, consuming time:61.4742 s\n",
      "[epoch 2065]: training loss: 1289.557617, consuming time:61.5747 s\n",
      "[epoch 2066]: training loss: 1217.260986, consuming time:61.5145 s\n",
      "[epoch 2067]: training loss: 943.852478, consuming time:61.7357 s\n",
      "[epoch 2068]: training loss: 945.470276, consuming time:62.4630 s\n",
      "[epoch 2069]: training loss: 1089.337646, consuming time:61.9303 s\n",
      "[epoch 2070]: training loss: 1382.513550, consuming time:62.5109 s\n",
      "[epoch 2071]: training loss: 920.144287, consuming time:61.7435 s\n",
      "[epoch 2072]: training loss: 886.918945, consuming time:61.7681 s\n",
      "[epoch 2073]: training loss: 923.857239, consuming time:61.6608 s\n",
      "[epoch 2074]: training loss: 822.305054, consuming time:61.5842 s\n",
      "[epoch 2075]: training loss: 884.937927, consuming time:61.5304 s\n",
      "[epoch 2076]: training loss: 893.514160, consuming time:61.8362 s\n",
      "[epoch 2077]: training loss: 887.216553, consuming time:61.6111 s\n",
      "[epoch 2078]: training loss: 952.096802, consuming time:61.6871 s\n",
      "[epoch 2079]: training loss: 987.185913, consuming time:61.6483 s\n",
      "[epoch 2080]: training loss: 1308.047852, consuming time:61.4601 s\n",
      "[epoch 2081]: training loss: 910.992798, consuming time:61.8563 s\n",
      "[epoch 2082]: training loss: 941.495605, consuming time:61.5840 s\n",
      "[epoch 2083]: training loss: 1238.475830, consuming time:61.6261 s\n",
      "[epoch 2084]: training loss: 1144.389160, consuming time:61.9644 s\n",
      "[epoch 2085]: training loss: 1390.401855, consuming time:61.5640 s\n",
      "[epoch 2086]: training loss: 807.812012, consuming time:61.4476 s\n",
      "[epoch 2087]: training loss: 975.454102, consuming time:61.4106 s\n",
      "[epoch 2088]: training loss: 793.053955, consuming time:61.8057 s\n",
      "[epoch 2089]: training loss: 1008.151733, consuming time:61.8904 s\n",
      "[epoch 2090]: training loss: 831.432495, consuming time:61.5510 s\n",
      "[epoch 2091]: training loss: 1110.050293, consuming time:61.8516 s\n",
      "[epoch 2092]: training loss: 869.926697, consuming time:61.3489 s\n",
      "[epoch 2093]: training loss: 795.431091, consuming time:61.6551 s\n",
      "[epoch 2094]: training loss: 1044.161621, consuming time:61.9323 s\n",
      "[epoch 2095]: training loss: 1139.912109, consuming time:61.4477 s\n",
      "[epoch 2096]: training loss: 770.944580, consuming time:61.4508 s\n",
      "[epoch 2097]: training loss: 869.105957, consuming time:61.6196 s\n",
      "[epoch 2098]: training loss: 1109.119873, consuming time:61.2449 s\n",
      "[epoch 2099]: training loss: 1354.589478, consuming time:61.6900 s\n",
      "[epoch 2100]: training loss: 1077.385254, consuming time:61.3356 s\n",
      "[epoch 2101]: training loss: 941.716064, consuming time:61.3912 s\n",
      "[epoch 2102]: training loss: 1048.029175, consuming time:61.4573 s\n",
      "[epoch 2103]: training loss: 1300.634521, consuming time:61.5830 s\n",
      "[epoch 2104]: training loss: 984.935669, consuming time:61.8952 s\n",
      "[epoch 2105]: training loss: 894.055176, consuming time:61.7485 s\n",
      "[epoch 2106]: training loss: 982.438477, consuming time:61.2356 s\n",
      "[epoch 2107]: training loss: 987.086182, consuming time:61.8764 s\n",
      "[epoch 2108]: training loss: 945.794189, consuming time:61.7510 s\n",
      "[epoch 2109]: training loss: 1212.211426, consuming time:61.3542 s\n",
      "[epoch 2110]: training loss: 1556.627686, consuming time:61.9246 s\n",
      "[epoch 2111]: training loss: 1164.066650, consuming time:61.5362 s\n",
      "[epoch 2112]: training loss: 1019.355835, consuming time:61.2383 s\n",
      "[epoch 2113]: training loss: 1088.558105, consuming time:61.7658 s\n",
      "[epoch 2114]: training loss: 1345.301270, consuming time:61.4429 s\n",
      "[epoch 2115]: training loss: 910.812439, consuming time:61.8168 s\n",
      "[epoch 2116]: training loss: 1013.650085, consuming time:61.5357 s\n",
      "[epoch 2117]: training loss: 942.199951, consuming time:61.6493 s\n",
      "[epoch 2118]: training loss: 812.262939, consuming time:61.4593 s\n",
      "[epoch 2119]: training loss: 1066.785889, consuming time:61.3872 s\n",
      "[epoch 2120]: training loss: 1001.608093, consuming time:61.6346 s\n",
      "[epoch 2121]: training loss: 1091.259277, consuming time:61.5645 s\n",
      "[epoch 2122]: training loss: 987.644043, consuming time:62.3617 s\n",
      "[epoch 2123]: training loss: 1058.776367, consuming time:61.7811 s\n",
      "[epoch 2124]: training loss: 1060.482910, consuming time:61.6578 s\n",
      "[epoch 2125]: training loss: 1072.320312, consuming time:61.5783 s\n",
      "[epoch 2126]: training loss: 908.627808, consuming time:61.5659 s\n",
      "[epoch 2127]: training loss: 1074.459473, consuming time:61.6298 s\n",
      "[epoch 2128]: training loss: 1008.163696, consuming time:61.5979 s\n",
      "[epoch 2129]: training loss: 1038.608398, consuming time:61.6003 s\n",
      "[epoch 2130]: training loss: 1147.675903, consuming time:61.3446 s\n",
      "[epoch 2131]: training loss: 1007.581543, consuming time:61.5976 s\n",
      "[epoch 2132]: training loss: 1005.682251, consuming time:61.5081 s\n",
      "[epoch 2133]: training loss: 909.406372, consuming time:61.7493 s\n",
      "[epoch 2134]: training loss: 864.862671, consuming time:61.8654 s\n",
      "[epoch 2135]: training loss: 1011.333984, consuming time:61.2043 s\n",
      "[epoch 2136]: training loss: 842.494141, consuming time:61.6292 s\n",
      "[epoch 2137]: training loss: 1272.815186, consuming time:61.3941 s\n",
      "[epoch 2138]: training loss: 937.582275, consuming time:61.5796 s\n",
      "[epoch 2139]: training loss: 733.852478, consuming time:61.5657 s\n",
      "[epoch 2140]: training loss: 939.972046, consuming time:61.8023 s\n",
      "[epoch 2141]: training loss: 1079.881592, consuming time:61.3273 s\n",
      "[epoch 2142]: training loss: 980.319763, consuming time:61.7098 s\n",
      "[epoch 2143]: training loss: 1051.652832, consuming time:61.4358 s\n",
      "[epoch 2144]: training loss: 1099.755127, consuming time:61.7458 s\n",
      "[epoch 2145]: training loss: 1250.863770, consuming time:61.7181 s\n",
      "[epoch 2146]: training loss: 1056.416382, consuming time:61.4487 s\n",
      "[epoch 2147]: training loss: 1011.968872, consuming time:61.6603 s\n",
      "[epoch 2148]: training loss: 984.734558, consuming time:61.5866 s\n",
      "[epoch 2149]: training loss: 1058.864746, consuming time:61.5630 s\n",
      "[epoch 2150]: training loss: 1371.383057, consuming time:61.5355 s\n",
      "[epoch 2151]: training loss: 1217.096436, consuming time:61.5203 s\n",
      "[epoch 2152]: training loss: 892.190552, consuming time:61.7562 s\n",
      "[epoch 2153]: training loss: 811.654785, consuming time:61.4051 s\n",
      "[epoch 2154]: training loss: 1012.960999, consuming time:61.5822 s\n",
      "[epoch 2155]: training loss: 1365.642700, consuming time:61.7333 s\n",
      "[epoch 2156]: training loss: 1056.495361, consuming time:61.8436 s\n",
      "[epoch 2157]: training loss: 1301.663330, consuming time:61.5798 s\n",
      "[epoch 2158]: training loss: 855.928589, consuming time:61.5216 s\n",
      "[epoch 2159]: training loss: 1004.372314, consuming time:61.2992 s\n",
      "[epoch 2160]: training loss: 1053.997925, consuming time:61.5276 s\n",
      "[epoch 2161]: training loss: 1160.882568, consuming time:61.6460 s\n",
      "[epoch 2162]: training loss: 950.438416, consuming time:61.5419 s\n",
      "[epoch 2163]: training loss: 787.453491, consuming time:61.3229 s\n",
      "[epoch 2164]: training loss: 1074.327393, consuming time:61.5139 s\n",
      "[epoch 2165]: training loss: 884.765381, consuming time:61.5032 s\n",
      "[epoch 2166]: training loss: 881.141968, consuming time:61.8541 s\n",
      "[epoch 2167]: training loss: 711.542786, consuming time:61.8395 s\n",
      "[epoch 2168]: training loss: 1072.279053, consuming time:61.6691 s\n",
      "[epoch 2169]: training loss: 997.398132, consuming time:61.3850 s\n",
      "[epoch 2170]: training loss: 1102.754761, consuming time:61.2361 s\n",
      "[epoch 2171]: training loss: 784.499695, consuming time:61.7655 s\n",
      "[epoch 2172]: training loss: 652.288208, consuming time:62.1163 s\n",
      "[epoch 2173]: training loss: 1255.023193, consuming time:61.5332 s\n",
      "[epoch 2174]: training loss: 917.357422, consuming time:61.7338 s\n",
      "[epoch 2175]: training loss: 1211.182373, consuming time:61.3258 s\n",
      "[epoch 2176]: training loss: 885.927734, consuming time:61.5675 s\n",
      "[epoch 2177]: training loss: 801.500122, consuming time:61.6983 s\n",
      "[epoch 2178]: training loss: 1052.201904, consuming time:61.7533 s\n",
      "[epoch 2179]: training loss: 1061.287720, consuming time:61.9152 s\n",
      "[epoch 2180]: training loss: 1290.594971, consuming time:61.7259 s\n",
      "[epoch 2181]: training loss: 1243.512085, consuming time:61.8691 s\n",
      "[epoch 2182]: training loss: 843.194214, consuming time:61.3211 s\n",
      "[epoch 2183]: training loss: 1096.709351, consuming time:61.3738 s\n",
      "[epoch 2184]: training loss: 909.843018, consuming time:61.4856 s\n",
      "[epoch 2185]: training loss: 958.033630, consuming time:61.5104 s\n",
      "[epoch 2186]: training loss: 913.790649, consuming time:61.4828 s\n",
      "[epoch 2187]: training loss: 962.064880, consuming time:61.5376 s\n",
      "[epoch 2188]: training loss: 984.797913, consuming time:61.1717 s\n",
      "[epoch 2189]: training loss: 920.303833, consuming time:61.8186 s\n",
      "[epoch 2190]: training loss: 1055.737061, consuming time:62.0545 s\n",
      "[epoch 2191]: training loss: 704.174744, consuming time:61.4390 s\n",
      "[epoch 2192]: training loss: 802.357666, consuming time:61.4254 s\n",
      "[epoch 2193]: training loss: 707.167297, consuming time:61.4042 s\n",
      "[epoch 2194]: training loss: 1151.822388, consuming time:61.2448 s\n",
      "[epoch 2195]: training loss: 869.016968, consuming time:61.5708 s\n",
      "[epoch 2196]: training loss: 827.926880, consuming time:61.5518 s\n",
      "[epoch 2197]: training loss: 1236.568604, consuming time:61.5147 s\n",
      "[epoch 2198]: training loss: 856.312988, consuming time:61.5266 s\n",
      "[epoch 2199]: training loss: 1146.590332, consuming time:61.8296 s\n",
      "[epoch 2200]: training loss: 895.311768, consuming time:61.3609 s\n",
      "[epoch 2201]: training loss: 875.947021, consuming time:61.5667 s\n",
      "[epoch 2202]: training loss: 1060.915649, consuming time:61.5703 s\n",
      "[epoch 2203]: training loss: 1400.819824, consuming time:61.7884 s\n",
      "[epoch 2204]: training loss: 1100.732178, consuming time:61.4689 s\n",
      "[epoch 2205]: training loss: 1094.601318, consuming time:61.4102 s\n",
      "[epoch 2206]: training loss: 1161.435059, consuming time:61.5413 s\n",
      "[epoch 2207]: training loss: 968.638367, consuming time:61.5510 s\n",
      "[epoch 2208]: training loss: 1075.063232, consuming time:61.6623 s\n",
      "[epoch 2209]: training loss: 1004.909668, consuming time:61.4358 s\n",
      "[epoch 2210]: training loss: 1138.528687, consuming time:61.8093 s\n",
      "[epoch 2211]: training loss: 1056.016235, consuming time:61.7229 s\n",
      "[epoch 2212]: training loss: 909.555664, consuming time:61.6072 s\n",
      "[epoch 2213]: training loss: 1334.885986, consuming time:61.5567 s\n",
      "[epoch 2214]: training loss: 774.825317, consuming time:61.4108 s\n",
      "[epoch 2215]: training loss: 850.576172, consuming time:61.8597 s\n",
      "[epoch 2216]: training loss: 725.296143, consuming time:61.8836 s\n",
      "[epoch 2217]: training loss: 1243.747070, consuming time:61.3036 s\n",
      "[epoch 2218]: training loss: 1132.725098, consuming time:61.6295 s\n",
      "[epoch 2219]: training loss: 779.343872, consuming time:61.6414 s\n",
      "[epoch 2220]: training loss: 1244.236206, consuming time:61.5939 s\n",
      "[epoch 2221]: training loss: 1008.177368, consuming time:61.8725 s\n",
      "[epoch 2222]: training loss: 1031.358521, consuming time:61.6622 s\n",
      "[epoch 2223]: training loss: 759.893188, consuming time:61.4936 s\n",
      "[epoch 2224]: training loss: 1150.104126, consuming time:61.7705 s\n",
      "[epoch 2225]: training loss: 951.810303, consuming time:61.6732 s\n",
      "[epoch 2226]: training loss: 806.622498, consuming time:61.6070 s\n",
      "[epoch 2227]: training loss: 1052.022705, consuming time:61.6451 s\n",
      "[epoch 2228]: training loss: 1033.321045, consuming time:61.3832 s\n",
      "[epoch 2229]: training loss: 607.652405, consuming time:61.4864 s\n",
      "[epoch 2230]: training loss: 834.542969, consuming time:61.7766 s\n",
      "[epoch 2231]: training loss: 1376.401855, consuming time:61.7659 s\n",
      "[epoch 2232]: training loss: 1262.666016, consuming time:61.3500 s\n",
      "[epoch 2233]: training loss: 1008.391602, consuming time:61.5997 s\n",
      "[epoch 2234]: training loss: 1326.421143, consuming time:61.4852 s\n",
      "[epoch 2235]: training loss: 1133.704712, consuming time:61.2931 s\n",
      "[epoch 2236]: training loss: 1115.534302, consuming time:61.7213 s\n",
      "[epoch 2237]: training loss: 776.025818, consuming time:61.4578 s\n",
      "[epoch 2238]: training loss: 917.426453, consuming time:61.6223 s\n",
      "[epoch 2239]: training loss: 1496.035156, consuming time:61.6939 s\n",
      "[epoch 2240]: training loss: 987.346802, consuming time:61.7116 s\n",
      "[epoch 2241]: training loss: 1238.757568, consuming time:61.3488 s\n",
      "[epoch 2242]: training loss: 897.169250, consuming time:61.2765 s\n",
      "[epoch 2243]: training loss: 1149.794434, consuming time:61.4728 s\n",
      "[epoch 2244]: training loss: 1347.283569, consuming time:61.4282 s\n",
      "[epoch 2245]: training loss: 1084.615234, consuming time:61.7927 s\n",
      "[epoch 2246]: training loss: 1376.313721, consuming time:61.2536 s\n",
      "[epoch 2247]: training loss: 1122.857178, consuming time:61.6517 s\n",
      "[epoch 2248]: training loss: 1138.355713, consuming time:61.4979 s\n",
      "[epoch 2249]: training loss: 854.180664, consuming time:61.4036 s\n",
      "[epoch 2250]: training loss: 1331.133057, consuming time:61.5874 s\n",
      "[epoch 2251]: training loss: 1178.398438, consuming time:62.0751 s\n",
      "[epoch 2252]: training loss: 1118.957275, consuming time:61.5554 s\n",
      "[epoch 2253]: training loss: 968.621155, consuming time:61.4143 s\n",
      "[epoch 2254]: training loss: 725.398193, consuming time:61.6403 s\n",
      "[epoch 2255]: training loss: 1174.298340, consuming time:61.4358 s\n",
      "[epoch 2256]: training loss: 1107.395508, consuming time:61.5414 s\n",
      "[epoch 2257]: training loss: 813.836426, consuming time:61.6989 s\n",
      "[epoch 2258]: training loss: 716.662476, consuming time:61.4004 s\n",
      "[epoch 2259]: training loss: 973.625977, consuming time:61.7551 s\n",
      "[epoch 2260]: training loss: 1014.703247, consuming time:61.6647 s\n",
      "[epoch 2261]: training loss: 921.939331, consuming time:61.3681 s\n",
      "[epoch 2262]: training loss: 896.574463, consuming time:61.6691 s\n",
      "[epoch 2263]: training loss: 1119.010498, consuming time:61.4979 s\n",
      "[epoch 2264]: training loss: 976.841797, consuming time:61.3221 s\n",
      "[epoch 2265]: training loss: 973.392212, consuming time:61.8293 s\n",
      "[epoch 2266]: training loss: 1066.476807, consuming time:61.5543 s\n",
      "[epoch 2267]: training loss: 869.258362, consuming time:61.6075 s\n",
      "[epoch 2268]: training loss: 1483.425659, consuming time:61.6413 s\n",
      "[epoch 2269]: training loss: 779.424866, consuming time:61.3804 s\n",
      "[epoch 2270]: training loss: 1171.305176, consuming time:61.8706 s\n",
      "[epoch 2271]: training loss: 993.841248, consuming time:61.9236 s\n",
      "[epoch 2272]: training loss: 1076.127441, consuming time:61.6356 s\n",
      "[epoch 2273]: training loss: 782.614868, consuming time:61.7210 s\n",
      "[epoch 2274]: training loss: 1052.325439, consuming time:61.5456 s\n",
      "[epoch 2275]: training loss: 1006.249695, consuming time:61.3223 s\n",
      "[epoch 2276]: training loss: 1073.766357, consuming time:61.3139 s\n",
      "[epoch 2277]: training loss: 717.888733, consuming time:61.3796 s\n",
      "[epoch 2278]: training loss: 1038.438965, consuming time:61.3317 s\n",
      "[epoch 2279]: training loss: 1004.882385, consuming time:61.6222 s\n",
      "[epoch 2280]: training loss: 1014.237305, consuming time:61.7204 s\n",
      "[epoch 2281]: training loss: 1119.000977, consuming time:62.0087 s\n",
      "[epoch 2282]: training loss: 942.780334, consuming time:61.9981 s\n",
      "[epoch 2283]: training loss: 945.082764, consuming time:61.6502 s\n",
      "[epoch 2284]: training loss: 989.843506, consuming time:61.6481 s\n",
      "[epoch 2285]: training loss: 1276.331543, consuming time:61.6759 s\n",
      "[epoch 2286]: training loss: 992.881836, consuming time:61.8158 s\n",
      "[epoch 2287]: training loss: 1027.459717, consuming time:61.2682 s\n",
      "[epoch 2288]: training loss: 854.040466, consuming time:62.0266 s\n",
      "[epoch 2289]: training loss: 1096.726318, consuming time:61.1178 s\n",
      "[epoch 2290]: training loss: 854.760742, consuming time:61.5294 s\n",
      "[epoch 2291]: training loss: 920.753845, consuming time:61.5277 s\n",
      "[epoch 2292]: training loss: 1094.774414, consuming time:61.4402 s\n",
      "[epoch 2293]: training loss: 1142.410889, consuming time:61.5796 s\n",
      "[epoch 2294]: training loss: 880.473328, consuming time:61.6569 s\n",
      "[epoch 2295]: training loss: 883.867371, consuming time:61.3163 s\n",
      "[epoch 2296]: training loss: 1082.439453, consuming time:61.8604 s\n",
      "[epoch 2297]: training loss: 1002.859436, consuming time:61.5000 s\n",
      "[epoch 2298]: training loss: 915.753235, consuming time:61.7332 s\n",
      "[epoch 2299]: training loss: 1264.272217, consuming time:61.7933 s\n",
      "[epoch 2300]: training loss: 997.222107, consuming time:62.0403 s\n",
      "[epoch 2301]: training loss: 945.152588, consuming time:61.4746 s\n",
      "[epoch 2302]: training loss: 1003.958801, consuming time:61.6614 s\n",
      "[epoch 2303]: training loss: 867.076050, consuming time:61.7085 s\n",
      "[epoch 2304]: training loss: 1069.270264, consuming time:61.9184 s\n",
      "[epoch 2305]: training loss: 950.348511, consuming time:61.6321 s\n",
      "[epoch 2306]: training loss: 827.326782, consuming time:61.8023 s\n",
      "[epoch 2307]: training loss: 855.035095, consuming time:61.8857 s\n",
      "[epoch 2308]: training loss: 834.534058, consuming time:61.4094 s\n",
      "[epoch 2309]: training loss: 738.870789, consuming time:61.8233 s\n",
      "[epoch 2310]: training loss: 905.590942, consuming time:61.5726 s\n",
      "[epoch 2311]: training loss: 1069.627441, consuming time:61.8401 s\n",
      "[epoch 2312]: training loss: 1303.098389, consuming time:61.6936 s\n",
      "[epoch 2313]: training loss: 890.635010, consuming time:62.0233 s\n",
      "[epoch 2314]: training loss: 1475.734497, consuming time:61.7371 s\n",
      "[epoch 2315]: training loss: 939.378052, consuming time:61.6924 s\n",
      "[epoch 2316]: training loss: 1065.148315, consuming time:61.6915 s\n",
      "[epoch 2317]: training loss: 1292.432739, consuming time:61.5449 s\n",
      "[epoch 2318]: training loss: 864.570190, consuming time:61.5999 s\n",
      "[epoch 2319]: training loss: 914.491577, consuming time:61.8195 s\n",
      "[epoch 2320]: training loss: 890.002930, consuming time:61.6446 s\n",
      "[epoch 2321]: training loss: 996.052490, consuming time:61.8181 s\n",
      "[epoch 2322]: training loss: 928.792725, consuming time:61.4723 s\n",
      "[epoch 2323]: training loss: 1157.922485, consuming time:61.6036 s\n",
      "[epoch 2324]: training loss: 925.593506, consuming time:62.1097 s\n",
      "[epoch 2325]: training loss: 785.505493, consuming time:61.6363 s\n",
      "[epoch 2326]: training loss: 726.143677, consuming time:61.7202 s\n",
      "[epoch 2327]: training loss: 792.854736, consuming time:61.8570 s\n",
      "[epoch 2328]: training loss: 1033.367554, consuming time:61.3196 s\n",
      "[epoch 2329]: training loss: 1106.339722, consuming time:61.6008 s\n",
      "[epoch 2330]: training loss: 1222.772217, consuming time:61.7436 s\n",
      "[epoch 2331]: training loss: 1186.691162, consuming time:61.9107 s\n",
      "[epoch 2332]: training loss: 1150.062256, consuming time:61.9365 s\n",
      "[epoch 2333]: training loss: 1057.542480, consuming time:61.4607 s\n",
      "[epoch 2334]: training loss: 1266.672607, consuming time:61.5291 s\n",
      "[epoch 2335]: training loss: 1159.347656, consuming time:61.8548 s\n",
      "[epoch 2336]: training loss: 908.200317, consuming time:61.7458 s\n",
      "[epoch 2337]: training loss: 933.796631, consuming time:61.7119 s\n",
      "[epoch 2338]: training loss: 848.828857, consuming time:61.6379 s\n",
      "[epoch 2339]: training loss: 1028.719360, consuming time:61.8044 s\n",
      "[epoch 2340]: training loss: 751.584961, consuming time:61.5111 s\n",
      "[epoch 2341]: training loss: 1157.727051, consuming time:61.9047 s\n",
      "[epoch 2342]: training loss: 869.019104, consuming time:61.4875 s\n",
      "[epoch 2343]: training loss: 828.790649, consuming time:61.7134 s\n",
      "[epoch 2344]: training loss: 950.598145, consuming time:61.6767 s\n",
      "[epoch 2345]: training loss: 1000.581299, consuming time:61.6627 s\n",
      "[epoch 2346]: training loss: 1019.323242, consuming time:61.3834 s\n",
      "[epoch 2347]: training loss: 832.555542, consuming time:61.7885 s\n",
      "[epoch 2348]: training loss: 1236.092773, consuming time:61.5972 s\n",
      "[epoch 2349]: training loss: 970.572571, consuming time:61.5205 s\n",
      "[epoch 2350]: training loss: 861.079956, consuming time:61.6410 s\n",
      "[epoch 2351]: training loss: 1290.383789, consuming time:61.5161 s\n",
      "[epoch 2352]: training loss: 991.346619, consuming time:61.4954 s\n",
      "[epoch 2353]: training loss: 1018.149414, consuming time:61.5823 s\n",
      "[epoch 2354]: training loss: 886.846375, consuming time:61.6612 s\n",
      "[epoch 2355]: training loss: 1148.779053, consuming time:61.5827 s\n",
      "[epoch 2356]: training loss: 1077.393921, consuming time:62.0940 s\n",
      "[epoch 2357]: training loss: 1186.275391, consuming time:61.3264 s\n",
      "[epoch 2358]: training loss: 910.547974, consuming time:61.8114 s\n",
      "[epoch 2359]: training loss: 770.428406, consuming time:61.6951 s\n",
      "[epoch 2360]: training loss: 872.630310, consuming time:61.3904 s\n",
      "[epoch 2361]: training loss: 933.670654, consuming time:61.5677 s\n",
      "[epoch 2362]: training loss: 781.564392, consuming time:61.4465 s\n",
      "[epoch 2363]: training loss: 875.437012, consuming time:61.5158 s\n",
      "[epoch 2364]: training loss: 947.318970, consuming time:61.3095 s\n",
      "[epoch 2365]: training loss: 1250.781616, consuming time:61.5755 s\n",
      "[epoch 2366]: training loss: 1081.347168, consuming time:61.6021 s\n",
      "[epoch 2367]: training loss: 979.460938, consuming time:61.3193 s\n",
      "[epoch 2368]: training loss: 904.983398, consuming time:61.7554 s\n",
      "[epoch 2369]: training loss: 855.544678, consuming time:61.4689 s\n",
      "[epoch 2370]: training loss: 741.282043, consuming time:61.2636 s\n",
      "[epoch 2371]: training loss: 1013.280762, consuming time:61.5767 s\n",
      "[epoch 2372]: training loss: 1052.489502, consuming time:61.2471 s\n",
      "[epoch 2373]: training loss: 794.438599, consuming time:61.7146 s\n",
      "[epoch 2374]: training loss: 851.965698, consuming time:61.7993 s\n",
      "[epoch 2375]: training loss: 1214.951172, consuming time:61.2587 s\n",
      "[epoch 2376]: training loss: 812.186768, consuming time:61.4204 s\n",
      "[epoch 2377]: training loss: 869.006409, consuming time:61.6813 s\n",
      "[epoch 2378]: training loss: 923.899048, consuming time:61.5487 s\n",
      "[epoch 2379]: training loss: 1072.664429, consuming time:61.8866 s\n",
      "[epoch 2380]: training loss: 960.599182, consuming time:61.5060 s\n",
      "[epoch 2381]: training loss: 947.516174, consuming time:61.6246 s\n",
      "[epoch 2382]: training loss: 1171.585571, consuming time:61.5597 s\n",
      "[epoch 2383]: training loss: 1131.245361, consuming time:61.7662 s\n",
      "[epoch 2384]: training loss: 888.867432, consuming time:61.8573 s\n",
      "[epoch 2385]: training loss: 1164.906006, consuming time:61.9102 s\n",
      "[epoch 2386]: training loss: 971.040344, consuming time:61.5093 s\n",
      "[epoch 2387]: training loss: 1378.685425, consuming time:61.4360 s\n",
      "[epoch 2388]: training loss: 738.939392, consuming time:61.4293 s\n",
      "[epoch 2389]: training loss: 979.288086, consuming time:61.5048 s\n",
      "[epoch 2390]: training loss: 988.158936, consuming time:61.5676 s\n",
      "[epoch 2391]: training loss: 931.998779, consuming time:61.7491 s\n",
      "[epoch 2392]: training loss: 875.814392, consuming time:61.5319 s\n",
      "[epoch 2393]: training loss: 1121.716919, consuming time:61.5852 s\n",
      "[epoch 2394]: training loss: 1012.231079, consuming time:62.2624 s\n",
      "[epoch 2395]: training loss: 1082.305542, consuming time:65.9914 s\n",
      "[epoch 2396]: training loss: 903.308350, consuming time:61.6271 s\n",
      "[epoch 2397]: training loss: 1030.161133, consuming time:61.4858 s\n",
      "[epoch 2398]: training loss: 1110.315186, consuming time:61.3987 s\n",
      "[epoch 2399]: training loss: 912.726562, consuming time:61.7416 s\n",
      "[epoch 2400]: training loss: 1162.064575, consuming time:61.2853 s\n",
      "[epoch 2401]: training loss: 1027.290405, consuming time:61.6390 s\n",
      "[epoch 2402]: training loss: 929.760498, consuming time:61.4109 s\n",
      "[epoch 2403]: training loss: 778.362244, consuming time:61.6478 s\n",
      "[epoch 2404]: training loss: 1121.376221, consuming time:61.2825 s\n",
      "[epoch 2405]: training loss: 820.018799, consuming time:61.6832 s\n",
      "[epoch 2406]: training loss: 1069.892212, consuming time:61.3319 s\n",
      "[epoch 2407]: training loss: 728.034363, consuming time:61.7214 s\n",
      "[epoch 2408]: training loss: 757.090149, consuming time:61.6346 s\n",
      "[epoch 2409]: training loss: 968.596802, consuming time:61.4335 s\n",
      "[epoch 2410]: training loss: 1011.409668, consuming time:61.2502 s\n",
      "[epoch 2411]: training loss: 1350.355469, consuming time:61.5641 s\n",
      "[epoch 2412]: training loss: 1333.401733, consuming time:61.8684 s\n",
      "[epoch 2413]: training loss: 992.000977, consuming time:61.6713 s\n",
      "[epoch 2414]: training loss: 1144.423096, consuming time:61.4748 s\n",
      "[epoch 2415]: training loss: 1065.865479, consuming time:61.5464 s\n",
      "[epoch 2416]: training loss: 1025.729248, consuming time:61.3496 s\n",
      "[epoch 2417]: training loss: 1192.978516, consuming time:61.4351 s\n",
      "[epoch 2418]: training loss: 958.564331, consuming time:61.4237 s\n",
      "[epoch 2419]: training loss: 1141.000610, consuming time:61.4337 s\n",
      "[epoch 2420]: training loss: 746.372314, consuming time:61.7634 s\n",
      "[epoch 2421]: training loss: 750.069641, consuming time:61.5089 s\n",
      "[epoch 2422]: training loss: 1057.786011, consuming time:61.9753 s\n",
      "[epoch 2423]: training loss: 814.849854, consuming time:61.5070 s\n",
      "[epoch 2424]: training loss: 746.316650, consuming time:61.4773 s\n",
      "[epoch 2425]: training loss: 1085.074463, consuming time:61.5967 s\n",
      "[epoch 2426]: training loss: 1076.406616, consuming time:61.6847 s\n",
      "[epoch 2427]: training loss: 895.228882, consuming time:61.3615 s\n",
      "[epoch 2428]: training loss: 1084.680664, consuming time:61.6812 s\n",
      "[epoch 2429]: training loss: 965.637817, consuming time:61.3710 s\n",
      "[epoch 2430]: training loss: 1187.448120, consuming time:61.7037 s\n",
      "[epoch 2431]: training loss: 1005.874268, consuming time:61.7109 s\n",
      "[epoch 2432]: training loss: 731.827087, consuming time:61.7104 s\n",
      "[epoch 2433]: training loss: 941.681213, consuming time:61.4346 s\n",
      "[epoch 2434]: training loss: 1253.062012, consuming time:61.5043 s\n",
      "[epoch 2435]: training loss: 879.384094, consuming time:62.1506 s\n",
      "[epoch 2436]: training loss: 1017.266602, consuming time:61.6559 s\n",
      "[epoch 2437]: training loss: 794.643433, consuming time:61.5539 s\n",
      "[epoch 2438]: training loss: 1300.916382, consuming time:61.4147 s\n",
      "[epoch 2439]: training loss: 969.169922, consuming time:61.3175 s\n",
      "[epoch 2440]: training loss: 618.203491, consuming time:61.6868 s\n",
      "[epoch 2441]: training loss: 973.119019, consuming time:61.6419 s\n",
      "[epoch 2442]: training loss: 1121.836670, consuming time:61.2610 s\n",
      "[epoch 2443]: training loss: 858.805664, consuming time:61.5985 s\n",
      "[epoch 2444]: training loss: 1026.037842, consuming time:61.5451 s\n",
      "[epoch 2445]: training loss: 1020.161865, consuming time:61.3930 s\n",
      "[epoch 2446]: training loss: 760.821289, consuming time:61.6552 s\n",
      "[epoch 2447]: training loss: 1281.290039, consuming time:61.6319 s\n",
      "[epoch 2448]: training loss: 1006.245056, consuming time:61.3686 s\n",
      "[epoch 2449]: training loss: 877.087891, consuming time:61.4494 s\n",
      "[epoch 2450]: training loss: 917.294006, consuming time:61.8762 s\n",
      "[epoch 2451]: training loss: 892.800903, consuming time:61.7668 s\n",
      "[epoch 2452]: training loss: 970.974548, consuming time:61.3839 s\n",
      "[epoch 2453]: training loss: 957.180115, consuming time:61.5267 s\n",
      "[epoch 2454]: training loss: 1147.531006, consuming time:61.9139 s\n",
      "[epoch 2455]: training loss: 1003.128845, consuming time:61.5140 s\n",
      "[epoch 2456]: training loss: 999.883667, consuming time:61.6615 s\n",
      "[epoch 2457]: training loss: 835.147766, consuming time:61.9000 s\n",
      "[epoch 2458]: training loss: 1575.477783, consuming time:61.6468 s\n",
      "[epoch 2459]: training loss: 936.566895, consuming time:61.8775 s\n",
      "[epoch 2460]: training loss: 971.435547, consuming time:61.4645 s\n",
      "[epoch 2461]: training loss: 794.858154, consuming time:61.2164 s\n",
      "[epoch 2462]: training loss: 1010.233032, consuming time:61.8542 s\n",
      "[epoch 2463]: training loss: 727.042480, consuming time:61.4813 s\n",
      "[epoch 2464]: training loss: 996.997620, consuming time:61.6901 s\n",
      "[epoch 2465]: training loss: 984.920044, consuming time:61.6760 s\n",
      "[epoch 2466]: training loss: 1029.807007, consuming time:61.6439 s\n",
      "[epoch 2467]: training loss: 964.273499, consuming time:61.5948 s\n",
      "[epoch 2468]: training loss: 821.295654, consuming time:61.3613 s\n",
      "[epoch 2469]: training loss: 1123.227783, consuming time:61.2480 s\n",
      "[epoch 2470]: training loss: 1054.615723, consuming time:61.6115 s\n",
      "[epoch 2471]: training loss: 1039.156006, consuming time:61.5510 s\n",
      "[epoch 2472]: training loss: 859.572266, consuming time:61.5854 s\n",
      "[epoch 2473]: training loss: 1018.134644, consuming time:61.6404 s\n",
      "[epoch 2474]: training loss: 1051.776367, consuming time:61.3930 s\n",
      "[epoch 2475]: training loss: 925.611816, consuming time:61.6378 s\n",
      "[epoch 2476]: training loss: 898.670776, consuming time:61.3407 s\n",
      "[epoch 2477]: training loss: 896.516418, consuming time:61.6433 s\n",
      "[epoch 2478]: training loss: 1268.797241, consuming time:61.6447 s\n",
      "[epoch 2479]: training loss: 830.546265, consuming time:61.5470 s\n",
      "[epoch 2480]: training loss: 1093.251099, consuming time:61.5883 s\n",
      "[epoch 2481]: training loss: 998.145569, consuming time:61.5305 s\n",
      "[epoch 2482]: training loss: 921.709778, consuming time:61.6959 s\n",
      "[epoch 2483]: training loss: 991.058044, consuming time:61.5561 s\n",
      "[epoch 2484]: training loss: 872.032593, consuming time:61.4855 s\n",
      "[epoch 2485]: training loss: 778.033264, consuming time:61.9437 s\n",
      "[epoch 2486]: training loss: 1171.099487, consuming time:61.3086 s\n",
      "[epoch 2487]: training loss: 964.526978, consuming time:61.6592 s\n",
      "[epoch 2488]: training loss: 971.459167, consuming time:61.9161 s\n",
      "[epoch 2489]: training loss: 1067.145752, consuming time:61.4738 s\n",
      "[epoch 2490]: training loss: 1382.927490, consuming time:62.1743 s\n",
      "[epoch 2491]: training loss: 993.082703, consuming time:62.0598 s\n",
      "[epoch 2492]: training loss: 1082.404053, consuming time:61.4319 s\n",
      "[epoch 2493]: training loss: 998.137573, consuming time:64.0857 s\n",
      "[epoch 2494]: training loss: 1272.821045, consuming time:64.0122 s\n",
      "[epoch 2495]: training loss: 964.028564, consuming time:61.3624 s\n",
      "[epoch 2496]: training loss: 929.233887, consuming time:61.6051 s\n",
      "[epoch 2497]: training loss: 1015.374451, consuming time:61.5402 s\n",
      "[epoch 2498]: training loss: 1323.224731, consuming time:60.9855 s\n",
      "[epoch 2499]: training loss: 904.761108, consuming time:61.5032 s\n",
      "[epoch 2500]: training loss: 1096.343994, consuming time:61.5770 s\n",
      "[epoch 2501]: training loss: 1029.467407, consuming time:61.5755 s\n",
      "[epoch 2502]: training loss: 1184.108643, consuming time:61.6709 s\n",
      "[epoch 2503]: training loss: 1007.624207, consuming time:61.4571 s\n",
      "[epoch 2504]: training loss: 1174.644043, consuming time:61.5228 s\n",
      "[epoch 2505]: training loss: 1001.311096, consuming time:61.6220 s\n",
      "[epoch 2506]: training loss: 852.698486, consuming time:61.3256 s\n",
      "[epoch 2507]: training loss: 982.462891, consuming time:61.6284 s\n",
      "[epoch 2508]: training loss: 1182.492432, consuming time:61.6398 s\n",
      "[epoch 2509]: training loss: 953.801636, consuming time:61.4172 s\n",
      "[epoch 2510]: training loss: 893.552002, consuming time:61.9155 s\n",
      "[epoch 2511]: training loss: 1045.637329, consuming time:61.6867 s\n",
      "[epoch 2512]: training loss: 639.032227, consuming time:61.6151 s\n",
      "[epoch 2513]: training loss: 1331.673462, consuming time:61.7479 s\n",
      "[epoch 2514]: training loss: 972.848206, consuming time:62.1070 s\n",
      "[epoch 2515]: training loss: 788.304810, consuming time:61.8302 s\n",
      "[epoch 2516]: training loss: 1413.430176, consuming time:61.8320 s\n",
      "[epoch 2517]: training loss: 1161.438232, consuming time:61.7738 s\n",
      "[epoch 2518]: training loss: 707.553711, consuming time:61.8819 s\n",
      "[epoch 2519]: training loss: 1147.362305, consuming time:61.7638 s\n",
      "[epoch 2520]: training loss: 981.491333, consuming time:61.7303 s\n",
      "[epoch 2521]: training loss: 953.858398, consuming time:61.7757 s\n",
      "[epoch 2522]: training loss: 727.488892, consuming time:62.0280 s\n",
      "[epoch 2523]: training loss: 988.266602, consuming time:62.1125 s\n",
      "[epoch 2524]: training loss: 795.675049, consuming time:61.8177 s\n",
      "[epoch 2525]: training loss: 1279.878540, consuming time:61.6985 s\n",
      "[epoch 2526]: training loss: 1005.267090, consuming time:61.4159 s\n",
      "[epoch 2527]: training loss: 983.436157, consuming time:61.5181 s\n",
      "[epoch 2528]: training loss: 898.871582, consuming time:61.4362 s\n",
      "[epoch 2529]: training loss: 1369.694092, consuming time:61.5579 s\n",
      "[epoch 2530]: training loss: 1028.041138, consuming time:61.4241 s\n",
      "[epoch 2531]: training loss: 922.288208, consuming time:61.8122 s\n",
      "[epoch 2532]: training loss: 1063.750977, consuming time:61.7096 s\n",
      "[epoch 2533]: training loss: 1090.700439, consuming time:62.0461 s\n",
      "[epoch 2534]: training loss: 1363.735840, consuming time:62.4213 s\n",
      "[epoch 2535]: training loss: 967.133606, consuming time:61.8723 s\n",
      "[epoch 2536]: training loss: 1065.948242, consuming time:61.9867 s\n",
      "[epoch 2537]: training loss: 906.660889, consuming time:61.8525 s\n",
      "[epoch 2538]: training loss: 1200.436157, consuming time:61.7882 s\n",
      "[epoch 2539]: training loss: 1072.422852, consuming time:61.7123 s\n",
      "[epoch 2540]: training loss: 1047.915649, consuming time:61.7414 s\n",
      "[epoch 2541]: training loss: 1176.886597, consuming time:61.7487 s\n",
      "[epoch 2542]: training loss: 859.506775, consuming time:62.0712 s\n",
      "[epoch 2543]: training loss: 1006.821411, consuming time:61.7341 s\n",
      "[epoch 2544]: training loss: 835.295166, consuming time:61.5667 s\n",
      "[epoch 2545]: training loss: 933.728271, consuming time:61.9146 s\n",
      "[epoch 2546]: training loss: 996.171204, consuming time:61.7884 s\n",
      "[epoch 2547]: training loss: 1231.424805, consuming time:61.5817 s\n",
      "[epoch 2548]: training loss: 1181.778076, consuming time:61.9292 s\n",
      "[epoch 2549]: training loss: 1122.763794, consuming time:61.6693 s\n",
      "[epoch 2550]: training loss: 1174.962402, consuming time:61.6489 s\n",
      "[epoch 2551]: training loss: 1002.847046, consuming time:61.9570 s\n",
      "[epoch 2552]: training loss: 1009.577576, consuming time:61.7166 s\n",
      "[epoch 2553]: training loss: 845.188782, consuming time:61.9636 s\n",
      "[epoch 2554]: training loss: 780.149414, consuming time:61.8028 s\n",
      "[epoch 2555]: training loss: 1049.425903, consuming time:62.0148 s\n",
      "[epoch 2556]: training loss: 979.838379, consuming time:61.6369 s\n",
      "[epoch 2557]: training loss: 870.612427, consuming time:61.8129 s\n",
      "[epoch 2558]: training loss: 1014.636230, consuming time:61.5456 s\n",
      "[epoch 2559]: training loss: 843.798889, consuming time:62.0077 s\n",
      "[epoch 2560]: training loss: 1114.282349, consuming time:61.8089 s\n",
      "[epoch 2561]: training loss: 1067.488525, consuming time:61.8896 s\n",
      "[epoch 2562]: training loss: 896.439148, consuming time:61.6455 s\n",
      "[epoch 2563]: training loss: 870.782593, consuming time:61.8153 s\n",
      "[epoch 2564]: training loss: 802.438599, consuming time:61.8859 s\n",
      "[epoch 2565]: training loss: 954.640747, consuming time:61.9742 s\n",
      "[epoch 2566]: training loss: 1197.234253, consuming time:61.8805 s\n",
      "[epoch 2567]: training loss: 931.258057, consuming time:61.7123 s\n",
      "[epoch 2568]: training loss: 807.461426, consuming time:61.7432 s\n",
      "[epoch 2569]: training loss: 1070.864746, consuming time:61.8454 s\n",
      "[epoch 2570]: training loss: 961.753662, consuming time:61.9682 s\n",
      "[epoch 2571]: training loss: 1366.274536, consuming time:61.9035 s\n",
      "[epoch 2572]: training loss: 1147.673340, consuming time:61.9058 s\n",
      "[epoch 2573]: training loss: 1042.932007, consuming time:61.6559 s\n",
      "[epoch 2574]: training loss: 1180.052490, consuming time:62.3729 s\n",
      "[epoch 2575]: training loss: 623.098999, consuming time:61.7326 s\n",
      "[epoch 2576]: training loss: 1116.724487, consuming time:61.5914 s\n",
      "[epoch 2577]: training loss: 1243.595581, consuming time:61.7950 s\n",
      "[epoch 2578]: training loss: 786.412231, consuming time:61.6558 s\n",
      "[epoch 2579]: training loss: 897.946899, consuming time:61.6987 s\n",
      "[epoch 2580]: training loss: 995.059570, consuming time:61.9253 s\n",
      "[epoch 2581]: training loss: 962.930298, consuming time:61.4123 s\n",
      "[epoch 2582]: training loss: 989.704529, consuming time:61.9485 s\n",
      "[epoch 2583]: training loss: 1217.645508, consuming time:61.7367 s\n",
      "[epoch 2584]: training loss: 965.723083, consuming time:61.6143 s\n",
      "[epoch 2585]: training loss: 1143.958984, consuming time:61.8290 s\n",
      "[epoch 2586]: training loss: 853.101379, consuming time:61.5574 s\n",
      "[epoch 2587]: training loss: 1133.921143, consuming time:61.6330 s\n",
      "[epoch 2588]: training loss: 1136.192627, consuming time:61.8110 s\n",
      "[epoch 2589]: training loss: 1157.021484, consuming time:61.9358 s\n",
      "[epoch 2590]: training loss: 976.183838, consuming time:61.8128 s\n",
      "[epoch 2591]: training loss: 1144.499634, consuming time:61.8680 s\n",
      "[epoch 2592]: training loss: 932.787964, consuming time:62.0065 s\n",
      "[epoch 2593]: training loss: 664.092285, consuming time:61.9243 s\n",
      "[epoch 2594]: training loss: 917.045471, consuming time:61.6565 s\n",
      "[epoch 2595]: training loss: 852.850281, consuming time:61.4533 s\n",
      "[epoch 2596]: training loss: 1052.514160, consuming time:62.0724 s\n",
      "[epoch 2597]: training loss: 870.571594, consuming time:61.7003 s\n",
      "[epoch 2598]: training loss: 593.199585, consuming time:61.5099 s\n",
      "[epoch 2599]: training loss: 926.996521, consuming time:61.5383 s\n",
      "[epoch 2600]: training loss: 815.280212, consuming time:62.0765 s\n",
      "[epoch 2601]: training loss: 665.564697, consuming time:62.0535 s\n",
      "[epoch 2602]: training loss: 740.429138, consuming time:61.5459 s\n",
      "[epoch 2603]: training loss: 1046.958130, consuming time:61.8265 s\n",
      "[epoch 2604]: training loss: 1102.575928, consuming time:61.6209 s\n",
      "[epoch 2605]: training loss: 1002.738403, consuming time:61.8840 s\n",
      "[epoch 2606]: training loss: 1230.277588, consuming time:61.8959 s\n",
      "[epoch 2607]: training loss: 1086.391602, consuming time:61.5607 s\n",
      "[epoch 2608]: training loss: 1033.773193, consuming time:61.7019 s\n",
      "[epoch 2609]: training loss: 960.209656, consuming time:61.4926 s\n",
      "[epoch 2610]: training loss: 728.601562, consuming time:61.7157 s\n",
      "[epoch 2611]: training loss: 799.239380, consuming time:61.8688 s\n",
      "[epoch 2612]: training loss: 971.161987, consuming time:61.4717 s\n",
      "[epoch 2613]: training loss: 1209.162842, consuming time:61.9022 s\n",
      "[epoch 2614]: training loss: 922.545471, consuming time:61.6132 s\n",
      "[epoch 2615]: training loss: 1075.683960, consuming time:61.5509 s\n",
      "[epoch 2616]: training loss: 1076.612793, consuming time:61.9432 s\n",
      "[epoch 2617]: training loss: 936.545044, consuming time:61.8387 s\n",
      "[epoch 2618]: training loss: 1000.865173, consuming time:61.5949 s\n",
      "[epoch 2619]: training loss: 900.810669, consuming time:61.8443 s\n",
      "[epoch 2620]: training loss: 1060.767090, consuming time:61.3713 s\n",
      "[epoch 2621]: training loss: 1113.767334, consuming time:61.7463 s\n",
      "[epoch 2622]: training loss: 1117.189453, consuming time:61.5445 s\n",
      "[epoch 2623]: training loss: 1078.770142, consuming time:61.8798 s\n",
      "[epoch 2624]: training loss: 1080.187744, consuming time:62.0652 s\n",
      "[epoch 2625]: training loss: 916.933228, consuming time:61.6566 s\n",
      "[epoch 2626]: training loss: 1152.892578, consuming time:61.5278 s\n",
      "[epoch 2627]: training loss: 1292.833008, consuming time:61.5998 s\n",
      "[epoch 2628]: training loss: 1101.701904, consuming time:61.5052 s\n",
      "[epoch 2629]: training loss: 865.215271, consuming time:61.6959 s\n",
      "[epoch 2630]: training loss: 976.698486, consuming time:61.8488 s\n",
      "[epoch 2631]: training loss: 1018.043213, consuming time:61.7759 s\n",
      "[epoch 2632]: training loss: 1063.109985, consuming time:61.3751 s\n",
      "[epoch 2633]: training loss: 1001.937378, consuming time:62.0843 s\n",
      "[epoch 2634]: training loss: 1004.042419, consuming time:61.8142 s\n",
      "[epoch 2635]: training loss: 992.711548, consuming time:61.8061 s\n",
      "[epoch 2636]: training loss: 686.863770, consuming time:61.5563 s\n",
      "[epoch 2637]: training loss: 1004.079285, consuming time:61.3443 s\n",
      "[epoch 2638]: training loss: 1188.972778, consuming time:61.4593 s\n",
      "[epoch 2639]: training loss: 1061.839355, consuming time:61.1295 s\n",
      "[epoch 2640]: training loss: 1060.896362, consuming time:61.2981 s\n",
      "[epoch 2641]: training loss: 812.047607, consuming time:61.2614 s\n",
      "[epoch 2642]: training loss: 1000.009766, consuming time:61.4757 s\n",
      "[epoch 2643]: training loss: 1177.190063, consuming time:61.2732 s\n",
      "[epoch 2644]: training loss: 1255.579102, consuming time:61.3994 s\n",
      "[epoch 2645]: training loss: 900.520874, consuming time:61.4204 s\n",
      "[epoch 2646]: training loss: 800.004944, consuming time:61.5379 s\n",
      "[epoch 2647]: training loss: 1034.740112, consuming time:61.4963 s\n",
      "[epoch 2648]: training loss: 888.047485, consuming time:61.4987 s\n",
      "[epoch 2649]: training loss: 1040.904785, consuming time:61.3850 s\n",
      "[epoch 2650]: training loss: 890.733887, consuming time:61.4384 s\n",
      "[epoch 2651]: training loss: 714.992493, consuming time:61.5754 s\n",
      "[epoch 2652]: training loss: 926.233643, consuming time:61.7791 s\n",
      "[epoch 2653]: training loss: 836.828003, consuming time:61.7065 s\n",
      "[epoch 2654]: training loss: 944.911316, consuming time:62.2114 s\n",
      "[epoch 2655]: training loss: 800.714661, consuming time:61.6438 s\n",
      "[epoch 2656]: training loss: 1301.881958, consuming time:61.4248 s\n",
      "[epoch 2657]: training loss: 999.380859, consuming time:61.5913 s\n",
      "[epoch 2658]: training loss: 1300.190918, consuming time:61.7962 s\n",
      "[epoch 2659]: training loss: 793.474243, consuming time:61.6498 s\n",
      "[epoch 2660]: training loss: 1033.763794, consuming time:61.4981 s\n",
      "[epoch 2661]: training loss: 781.775452, consuming time:61.0833 s\n",
      "[epoch 2662]: training loss: 1042.307861, consuming time:61.4226 s\n",
      "[epoch 2663]: training loss: 945.153992, consuming time:66.9043 s\n",
      "[epoch 2664]: training loss: 1006.745361, consuming time:68.4470 s\n",
      "[epoch 2665]: training loss: 855.077454, consuming time:66.5576 s\n",
      "[epoch 2666]: training loss: 803.516846, consuming time:61.6534 s\n",
      "[epoch 2667]: training loss: 1018.646118, consuming time:63.2128 s\n",
      "[epoch 2668]: training loss: 936.996155, consuming time:66.4850 s\n",
      "[epoch 2669]: training loss: 908.484924, consuming time:65.1066 s\n",
      "[epoch 2670]: training loss: 917.699524, consuming time:63.0079 s\n",
      "[epoch 2671]: training loss: 997.274414, consuming time:68.3653 s\n",
      "[epoch 2672]: training loss: 924.967407, consuming time:66.7253 s\n",
      "[epoch 2673]: training loss: 910.292419, consuming time:62.6211 s\n",
      "[epoch 2674]: training loss: 877.702881, consuming time:61.9514 s\n",
      "[epoch 2675]: training loss: 865.851013, consuming time:62.7874 s\n",
      "[epoch 2676]: training loss: 757.022522, consuming time:62.6534 s\n",
      "[epoch 2677]: training loss: 1044.424561, consuming time:69.0194 s\n",
      "[epoch 2678]: training loss: 960.468689, consuming time:67.0530 s\n",
      "[epoch 2679]: training loss: 1002.686096, consuming time:66.3435 s\n",
      "[epoch 2680]: training loss: 1195.971680, consuming time:67.8217 s\n",
      "[epoch 2681]: training loss: 1325.127930, consuming time:67.3703 s\n",
      "[epoch 2682]: training loss: 1267.029297, consuming time:69.3629 s\n",
      "[epoch 2683]: training loss: 1065.219116, consuming time:68.4651 s\n",
      "[epoch 2684]: training loss: 1230.523682, consuming time:68.9433 s\n",
      "[epoch 2685]: training loss: 1144.672729, consuming time:66.2755 s\n",
      "[epoch 2686]: training loss: 1096.135986, consuming time:66.9133 s\n",
      "[epoch 2687]: training loss: 994.505249, consuming time:66.0683 s\n",
      "[epoch 2688]: training loss: 715.672485, consuming time:70.0600 s\n",
      "[epoch 2689]: training loss: 968.339355, consuming time:61.7532 s\n",
      "[epoch 2690]: training loss: 799.930298, consuming time:61.2798 s\n",
      "[epoch 2691]: training loss: 1020.301147, consuming time:65.3151 s\n",
      "[epoch 2692]: training loss: 948.579407, consuming time:63.9230 s\n",
      "[epoch 2693]: training loss: 1096.695068, consuming time:62.6435 s\n",
      "[epoch 2694]: training loss: 838.555908, consuming time:66.1112 s\n",
      "[epoch 2695]: training loss: 738.429382, consuming time:65.1031 s\n",
      "[epoch 2696]: training loss: 1169.082153, consuming time:66.7741 s\n",
      "[epoch 2697]: training loss: 1217.323242, consuming time:65.1325 s\n",
      "[epoch 2698]: training loss: 1178.096558, consuming time:63.1128 s\n",
      "[epoch 2699]: training loss: 979.605591, consuming time:66.7043 s\n",
      "[epoch 2700]: training loss: 1104.554199, consuming time:66.6407 s\n",
      "[epoch 2701]: training loss: 1018.018372, consuming time:70.0672 s\n",
      "[epoch 2702]: training loss: 1143.098389, consuming time:70.0359 s\n",
      "[epoch 2703]: training loss: 1028.930542, consuming time:71.2514 s\n",
      "[epoch 2704]: training loss: 1015.542786, consuming time:69.3343 s\n",
      "[epoch 2705]: training loss: 874.792603, consuming time:61.6118 s\n",
      "[epoch 2706]: training loss: 1191.852051, consuming time:61.1188 s\n",
      "[epoch 2707]: training loss: 861.460938, consuming time:63.8987 s\n",
      "[epoch 2708]: training loss: 1163.994873, consuming time:62.5955 s\n",
      "[epoch 2709]: training loss: 928.517090, consuming time:61.4677 s\n",
      "[epoch 2710]: training loss: 985.607727, consuming time:62.6543 s\n",
      "[epoch 2711]: training loss: 908.285889, consuming time:62.0944 s\n",
      "[epoch 2712]: training loss: 1149.527344, consuming time:61.3916 s\n",
      "[epoch 2713]: training loss: 1079.266357, consuming time:60.7265 s\n",
      "[epoch 2714]: training loss: 1265.570435, consuming time:60.9199 s\n",
      "[epoch 2715]: training loss: 1001.532837, consuming time:61.7984 s\n",
      "[epoch 2716]: training loss: 1044.315308, consuming time:63.6986 s\n",
      "[epoch 2717]: training loss: 859.437561, consuming time:61.8976 s\n",
      "[epoch 2718]: training loss: 948.568420, consuming time:61.0429 s\n",
      "[epoch 2719]: training loss: 918.635010, consuming time:61.1421 s\n",
      "[epoch 2720]: training loss: 954.998962, consuming time:61.6717 s\n",
      "[epoch 2721]: training loss: 1173.875488, consuming time:62.0354 s\n",
      "[epoch 2722]: training loss: 1124.850342, consuming time:61.4752 s\n",
      "[epoch 2723]: training loss: 1035.047729, consuming time:61.1597 s\n",
      "[epoch 2724]: training loss: 1090.751465, consuming time:61.1366 s\n",
      "[epoch 2725]: training loss: 1309.250122, consuming time:60.8935 s\n",
      "[epoch 2726]: training loss: 1001.986267, consuming time:61.1499 s\n",
      "[epoch 2727]: training loss: 1123.836914, consuming time:61.7390 s\n",
      "[epoch 2728]: training loss: 680.917480, consuming time:62.2470 s\n",
      "[epoch 2729]: training loss: 980.387085, consuming time:61.1144 s\n",
      "[epoch 2730]: training loss: 973.570679, consuming time:61.1786 s\n",
      "[epoch 2731]: training loss: 849.813599, consuming time:61.4987 s\n",
      "[epoch 2732]: training loss: 1231.579956, consuming time:62.1868 s\n",
      "[epoch 2733]: training loss: 1044.485840, consuming time:61.1119 s\n",
      "[epoch 2734]: training loss: 985.519653, consuming time:61.5961 s\n",
      "[epoch 2735]: training loss: 756.568848, consuming time:61.6228 s\n",
      "[epoch 2736]: training loss: 1170.002930, consuming time:60.8651 s\n",
      "[epoch 2737]: training loss: 947.858215, consuming time:61.7655 s\n",
      "[epoch 2738]: training loss: 1000.934448, consuming time:61.1644 s\n",
      "[epoch 2739]: training loss: 608.891174, consuming time:61.2161 s\n",
      "[epoch 2740]: training loss: 850.166199, consuming time:60.9390 s\n",
      "[epoch 2741]: training loss: 1007.521973, consuming time:61.3108 s\n",
      "[epoch 2742]: training loss: 946.455505, consuming time:61.3459 s\n",
      "[epoch 2743]: training loss: 849.414917, consuming time:62.2735 s\n",
      "[epoch 2744]: training loss: 1012.866577, consuming time:61.6530 s\n",
      "[epoch 2745]: training loss: 1260.294434, consuming time:61.3911 s\n",
      "[epoch 2746]: training loss: 925.465759, consuming time:61.4051 s\n",
      "[epoch 2747]: training loss: 1091.803223, consuming time:61.0744 s\n",
      "[epoch 2748]: training loss: 1105.083008, consuming time:61.0524 s\n",
      "[epoch 2749]: training loss: 909.476196, consuming time:61.0011 s\n",
      "[epoch 2750]: training loss: 1116.770020, consuming time:61.7519 s\n",
      "[epoch 2751]: training loss: 1081.807007, consuming time:61.4107 s\n",
      "[epoch 2752]: training loss: 1064.107666, consuming time:65.3225 s\n",
      "[epoch 2753]: training loss: 1072.685181, consuming time:61.5053 s\n",
      "[epoch 2754]: training loss: 769.717957, consuming time:61.3748 s\n",
      "[epoch 2755]: training loss: 891.056396, consuming time:62.7049 s\n",
      "[epoch 2756]: training loss: 960.744385, consuming time:61.4913 s\n",
      "[epoch 2757]: training loss: 895.839539, consuming time:61.1949 s\n",
      "[epoch 2758]: training loss: 1152.944580, consuming time:60.9002 s\n",
      "[epoch 2759]: training loss: 847.150269, consuming time:61.2436 s\n",
      "[epoch 2760]: training loss: 1003.602539, consuming time:61.0755 s\n",
      "[epoch 2761]: training loss: 1030.475098, consuming time:61.3186 s\n",
      "[epoch 2762]: training loss: 899.877563, consuming time:68.1212 s\n",
      "[epoch 2763]: training loss: 821.460510, consuming time:62.2711 s\n",
      "[epoch 2764]: training loss: 988.069336, consuming time:61.2453 s\n",
      "[epoch 2765]: training loss: 885.472595, consuming time:61.2502 s\n",
      "[epoch 2766]: training loss: 1246.933960, consuming time:61.3248 s\n",
      "[epoch 2767]: training loss: 837.188721, consuming time:64.4234 s\n",
      "[epoch 2768]: training loss: 880.895935, consuming time:61.0752 s\n",
      "[epoch 2769]: training loss: 1048.465088, consuming time:61.7020 s\n",
      "[epoch 2770]: training loss: 889.230347, consuming time:63.0063 s\n",
      "[epoch 2771]: training loss: 763.403809, consuming time:62.2936 s\n",
      "[epoch 2772]: training loss: 1151.881104, consuming time:62.1408 s\n",
      "[epoch 2773]: training loss: 1080.732300, consuming time:61.2701 s\n",
      "[epoch 2774]: training loss: 1289.332764, consuming time:61.0043 s\n",
      "[epoch 2775]: training loss: 1300.367188, consuming time:60.8981 s\n",
      "[epoch 2776]: training loss: 843.371826, consuming time:60.9786 s\n",
      "[epoch 2777]: training loss: 1210.142090, consuming time:60.8844 s\n",
      "[epoch 2778]: training loss: 863.585327, consuming time:60.8380 s\n",
      "[epoch 2779]: training loss: 1274.003906, consuming time:61.2556 s\n",
      "[epoch 2780]: training loss: 1004.075317, consuming time:62.5503 s\n",
      "[epoch 2781]: training loss: 747.003601, consuming time:61.4625 s\n",
      "[epoch 2782]: training loss: 1179.952271, consuming time:62.0033 s\n",
      "[epoch 2783]: training loss: 785.799622, consuming time:61.7254 s\n",
      "[epoch 2784]: training loss: 769.207031, consuming time:61.3380 s\n",
      "[epoch 2785]: training loss: 1228.023193, consuming time:61.6460 s\n",
      "[epoch 2786]: training loss: 759.543579, consuming time:61.4082 s\n",
      "[epoch 2787]: training loss: 917.791870, consuming time:61.4123 s\n",
      "[epoch 2788]: training loss: 1278.032104, consuming time:61.1954 s\n",
      "[epoch 2789]: training loss: 798.301208, consuming time:61.8157 s\n",
      "[epoch 2790]: training loss: 1051.711914, consuming time:61.4147 s\n",
      "[epoch 2791]: training loss: 909.290466, consuming time:62.3341 s\n",
      "[epoch 2792]: training loss: 910.568237, consuming time:61.9410 s\n",
      "[epoch 2793]: training loss: 1069.277100, consuming time:61.7383 s\n",
      "[epoch 2794]: training loss: 1010.960571, consuming time:61.4190 s\n",
      "[epoch 2795]: training loss: 790.128784, consuming time:61.3907 s\n",
      "[epoch 2796]: training loss: 951.730774, consuming time:61.8163 s\n",
      "[epoch 2797]: training loss: 1181.490845, consuming time:61.8576 s\n",
      "[epoch 2798]: training loss: 1065.689087, consuming time:61.7005 s\n",
      "[epoch 2799]: training loss: 1202.097900, consuming time:61.6371 s\n",
      "[epoch 2800]: training loss: 919.894531, consuming time:61.7045 s\n",
      "[epoch 2801]: training loss: 1174.083008, consuming time:61.6051 s\n",
      "[epoch 2802]: training loss: 888.825134, consuming time:61.8257 s\n",
      "[epoch 2803]: training loss: 693.106079, consuming time:61.6407 s\n",
      "[epoch 2804]: training loss: 943.195435, consuming time:61.6349 s\n",
      "[epoch 2805]: training loss: 1101.002686, consuming time:61.8741 s\n",
      "[epoch 2806]: training loss: 944.786133, consuming time:61.4185 s\n",
      "[epoch 2807]: training loss: 941.211365, consuming time:61.5386 s\n",
      "[epoch 2808]: training loss: 1086.695557, consuming time:61.8271 s\n",
      "[epoch 2809]: training loss: 940.565063, consuming time:61.5202 s\n",
      "[epoch 2810]: training loss: 1058.731689, consuming time:61.5235 s\n",
      "[epoch 2811]: training loss: 858.630920, consuming time:61.7133 s\n",
      "[epoch 2812]: training loss: 943.193848, consuming time:61.5390 s\n",
      "[epoch 2813]: training loss: 1152.093262, consuming time:61.4417 s\n",
      "[epoch 2814]: training loss: 1102.463989, consuming time:61.8619 s\n",
      "[epoch 2815]: training loss: 994.596130, consuming time:61.7076 s\n",
      "[epoch 2816]: training loss: 1105.273193, consuming time:61.7386 s\n",
      "[epoch 2817]: training loss: 912.434875, consuming time:61.5977 s\n",
      "[epoch 2818]: training loss: 869.015015, consuming time:61.7006 s\n",
      "[epoch 2819]: training loss: 942.786621, consuming time:61.4925 s\n",
      "[epoch 2820]: training loss: 999.570374, consuming time:61.8153 s\n",
      "[epoch 2821]: training loss: 994.354126, consuming time:61.7157 s\n",
      "[epoch 2822]: training loss: 895.229858, consuming time:61.8711 s\n",
      "[epoch 2823]: training loss: 870.626404, consuming time:61.5496 s\n",
      "[epoch 2824]: training loss: 999.791382, consuming time:61.4014 s\n",
      "[epoch 2825]: training loss: 724.869568, consuming time:61.6898 s\n",
      "[epoch 2826]: training loss: 1131.547974, consuming time:61.4122 s\n",
      "[epoch 2827]: training loss: 798.299927, consuming time:61.5725 s\n",
      "[epoch 2828]: training loss: 732.603210, consuming time:61.7821 s\n",
      "[epoch 2829]: training loss: 966.369873, consuming time:61.4195 s\n",
      "[epoch 2830]: training loss: 843.256226, consuming time:61.5548 s\n",
      "[epoch 2831]: training loss: 1217.806763, consuming time:61.6354 s\n",
      "[epoch 2832]: training loss: 847.656189, consuming time:61.5390 s\n",
      "[epoch 2833]: training loss: 1100.099854, consuming time:61.8564 s\n",
      "[epoch 2834]: training loss: 1237.650879, consuming time:61.7878 s\n",
      "[epoch 2835]: training loss: 871.889404, consuming time:61.7167 s\n",
      "[epoch 2836]: training loss: 1175.813721, consuming time:61.6080 s\n",
      "[epoch 2837]: training loss: 1013.286255, consuming time:61.7363 s\n",
      "[epoch 2838]: training loss: 732.380493, consuming time:61.6067 s\n",
      "[epoch 2839]: training loss: 909.565918, consuming time:61.7340 s\n",
      "[epoch 2840]: training loss: 1551.687500, consuming time:61.5513 s\n",
      "[epoch 2841]: training loss: 1093.741699, consuming time:61.4912 s\n",
      "[epoch 2842]: training loss: 974.115234, consuming time:61.7520 s\n",
      "[epoch 2843]: training loss: 1377.287842, consuming time:61.5283 s\n",
      "[epoch 2844]: training loss: 886.159607, consuming time:61.7037 s\n",
      "[epoch 2845]: training loss: 1120.504883, consuming time:61.6161 s\n",
      "[epoch 2846]: training loss: 1102.063599, consuming time:61.6909 s\n",
      "[epoch 2847]: training loss: 855.774658, consuming time:61.6981 s\n",
      "[epoch 2848]: training loss: 1068.255249, consuming time:61.7974 s\n",
      "[epoch 2849]: training loss: 882.417542, consuming time:61.6320 s\n",
      "[epoch 2850]: training loss: 935.489990, consuming time:61.6656 s\n",
      "[epoch 2851]: training loss: 987.779907, consuming time:61.5083 s\n",
      "[epoch 2852]: training loss: 976.525879, consuming time:61.4605 s\n",
      "[epoch 2853]: training loss: 947.862122, consuming time:61.7403 s\n",
      "[epoch 2854]: training loss: 884.753418, consuming time:61.7694 s\n",
      "[epoch 2855]: training loss: 811.273682, consuming time:61.5660 s\n",
      "[epoch 2856]: training loss: 729.801880, consuming time:61.7705 s\n",
      "[epoch 2857]: training loss: 900.057861, consuming time:61.7331 s\n",
      "[epoch 2858]: training loss: 1112.801270, consuming time:61.4984 s\n",
      "[epoch 2859]: training loss: 1270.576050, consuming time:61.5973 s\n",
      "[epoch 2860]: training loss: 839.983521, consuming time:61.5701 s\n",
      "[epoch 2861]: training loss: 977.787781, consuming time:61.3707 s\n",
      "[epoch 2862]: training loss: 766.677856, consuming time:61.7619 s\n",
      "[epoch 2863]: training loss: 1116.834961, consuming time:61.7749 s\n",
      "[epoch 2864]: training loss: 936.922119, consuming time:61.4940 s\n",
      "[epoch 2865]: training loss: 907.878723, consuming time:61.5939 s\n",
      "[epoch 2866]: training loss: 1095.189209, consuming time:61.7655 s\n",
      "[epoch 2867]: training loss: 836.011353, consuming time:61.7811 s\n",
      "[epoch 2868]: training loss: 1054.448975, consuming time:61.5760 s\n",
      "[epoch 2869]: training loss: 1219.184692, consuming time:61.4519 s\n",
      "[epoch 2870]: training loss: 1037.087158, consuming time:61.7858 s\n",
      "[epoch 2871]: training loss: 1108.401733, consuming time:61.8576 s\n",
      "[epoch 2872]: training loss: 661.500427, consuming time:61.7104 s\n",
      "[epoch 2873]: training loss: 1256.996826, consuming time:61.7158 s\n",
      "[epoch 2874]: training loss: 681.405396, consuming time:61.8666 s\n",
      "[epoch 2875]: training loss: 811.938232, consuming time:61.7356 s\n",
      "[epoch 2876]: training loss: 1030.681396, consuming time:61.3866 s\n",
      "[epoch 2877]: training loss: 1030.244141, consuming time:61.5581 s\n",
      "[epoch 2878]: training loss: 1166.311279, consuming time:61.6785 s\n",
      "[epoch 2879]: training loss: 1135.495605, consuming time:61.6923 s\n",
      "[epoch 2880]: training loss: 752.028259, consuming time:61.5593 s\n",
      "[epoch 2881]: training loss: 1032.807373, consuming time:61.5908 s\n",
      "[epoch 2882]: training loss: 984.017761, consuming time:61.5748 s\n",
      "[epoch 2883]: training loss: 971.965942, consuming time:61.6703 s\n",
      "[epoch 2884]: training loss: 1038.556152, consuming time:61.5317 s\n",
      "[epoch 2885]: training loss: 1024.079590, consuming time:61.5923 s\n",
      "[epoch 2886]: training loss: 1233.182129, consuming time:61.5206 s\n",
      "[epoch 2887]: training loss: 1278.665527, consuming time:61.5619 s\n",
      "[epoch 2888]: training loss: 980.938354, consuming time:61.5395 s\n",
      "[epoch 2889]: training loss: 814.973145, consuming time:61.4975 s\n",
      "[epoch 2890]: training loss: 947.704041, consuming time:61.7688 s\n",
      "[epoch 2891]: training loss: 1018.809204, consuming time:61.6778 s\n",
      "[epoch 2892]: training loss: 1272.890381, consuming time:61.6035 s\n",
      "[epoch 2893]: training loss: 944.420044, consuming time:61.4107 s\n",
      "[epoch 2894]: training loss: 1186.042480, consuming time:61.8951 s\n",
      "[epoch 2895]: training loss: 1005.867676, consuming time:61.7067 s\n",
      "[epoch 2896]: training loss: 1049.421631, consuming time:61.7173 s\n",
      "[epoch 2897]: training loss: 708.846802, consuming time:61.7807 s\n",
      "[epoch 2898]: training loss: 907.086670, consuming time:61.5223 s\n",
      "[epoch 2899]: training loss: 873.291748, consuming time:61.4132 s\n",
      "[epoch 2900]: training loss: 898.569214, consuming time:61.7770 s\n",
      "[epoch 2901]: training loss: 1149.474487, consuming time:61.7525 s\n",
      "[epoch 2902]: training loss: 1065.007812, consuming time:61.5591 s\n",
      "[epoch 2903]: training loss: 938.600647, consuming time:61.5449 s\n",
      "[epoch 2904]: training loss: 991.038269, consuming time:61.7499 s\n",
      "[epoch 2905]: training loss: 861.644165, consuming time:61.6245 s\n",
      "[epoch 2906]: training loss: 901.477417, consuming time:61.6893 s\n",
      "[epoch 2907]: training loss: 1041.546631, consuming time:61.8493 s\n",
      "[epoch 2908]: training loss: 1025.216431, consuming time:61.7907 s\n",
      "[epoch 2909]: training loss: 1099.390625, consuming time:61.5076 s\n",
      "[epoch 2910]: training loss: 1033.469971, consuming time:61.8542 s\n",
      "[epoch 2911]: training loss: 852.856079, consuming time:61.5300 s\n",
      "[epoch 2912]: training loss: 1061.327393, consuming time:61.6965 s\n",
      "[epoch 2913]: training loss: 812.294067, consuming time:61.7619 s\n",
      "[epoch 2914]: training loss: 867.316406, consuming time:65.2941 s\n",
      "[epoch 2915]: training loss: 1040.692261, consuming time:61.7593 s\n",
      "[epoch 2916]: training loss: 917.307861, consuming time:61.5154 s\n",
      "[epoch 2917]: training loss: 1139.200195, consuming time:61.7541 s\n",
      "[epoch 2918]: training loss: 868.517517, consuming time:61.6352 s\n",
      "[epoch 2919]: training loss: 931.132996, consuming time:61.6255 s\n",
      "[epoch 2920]: training loss: 1008.426147, consuming time:61.6417 s\n",
      "[epoch 2921]: training loss: 884.624634, consuming time:61.7254 s\n",
      "[epoch 2922]: training loss: 1466.479736, consuming time:61.7784 s\n",
      "[epoch 2923]: training loss: 688.209900, consuming time:61.7594 s\n",
      "[epoch 2924]: training loss: 1012.998413, consuming time:61.7061 s\n",
      "[epoch 2925]: training loss: 1087.309082, consuming time:61.8098 s\n",
      "[epoch 2926]: training loss: 1074.357422, consuming time:61.6536 s\n",
      "[epoch 2927]: training loss: 1010.311890, consuming time:61.4790 s\n",
      "[epoch 2928]: training loss: 1087.757202, consuming time:61.5795 s\n",
      "[epoch 2929]: training loss: 1083.625977, consuming time:61.7978 s\n",
      "[epoch 2930]: training loss: 945.474121, consuming time:61.7505 s\n",
      "[epoch 2931]: training loss: 1006.896118, consuming time:61.7626 s\n",
      "[epoch 2932]: training loss: 1167.360840, consuming time:61.7896 s\n",
      "[epoch 2933]: training loss: 1002.111450, consuming time:61.6933 s\n",
      "[epoch 2934]: training loss: 1344.374146, consuming time:61.4650 s\n",
      "[epoch 2935]: training loss: 1226.168701, consuming time:61.7380 s\n",
      "[epoch 2936]: training loss: 1160.784912, consuming time:61.7513 s\n",
      "[epoch 2937]: training loss: 1164.805542, consuming time:61.6320 s\n",
      "[epoch 2938]: training loss: 975.222168, consuming time:61.5916 s\n",
      "[epoch 2939]: training loss: 1075.858032, consuming time:61.8065 s\n",
      "[epoch 2940]: training loss: 1132.691406, consuming time:61.5952 s\n",
      "[epoch 2941]: training loss: 840.963257, consuming time:61.5740 s\n",
      "[epoch 2942]: training loss: 1060.212891, consuming time:61.7728 s\n",
      "[epoch 2943]: training loss: 974.945923, consuming time:61.6738 s\n",
      "[epoch 2944]: training loss: 1161.658081, consuming time:61.7884 s\n",
      "[epoch 2945]: training loss: 985.667419, consuming time:61.7098 s\n",
      "[epoch 2946]: training loss: 745.527466, consuming time:61.7346 s\n",
      "[epoch 2947]: training loss: 1081.427612, consuming time:61.7193 s\n",
      "[epoch 2948]: training loss: 1017.718140, consuming time:61.8410 s\n",
      "[epoch 2949]: training loss: 976.240234, consuming time:61.8107 s\n",
      "[epoch 2950]: training loss: 1013.086792, consuming time:63.8620 s\n",
      "[epoch 2951]: training loss: 1195.649414, consuming time:64.0935 s\n",
      "[epoch 2952]: training loss: 590.286804, consuming time:64.7024 s\n",
      "[epoch 2953]: training loss: 875.128113, consuming time:63.5047 s\n",
      "[epoch 2954]: training loss: 721.437439, consuming time:65.1350 s\n",
      "[epoch 2955]: training loss: 1129.836426, consuming time:73.1721 s\n",
      "[epoch 2956]: training loss: 1066.713867, consuming time:75.8355 s\n",
      "[epoch 2957]: training loss: 1442.530762, consuming time:71.0033 s\n",
      "[epoch 2958]: training loss: 963.042969, consuming time:72.5283 s\n",
      "[epoch 2959]: training loss: 983.942505, consuming time:74.0052 s\n",
      "[epoch 2960]: training loss: 841.983276, consuming time:72.9274 s\n",
      "[epoch 2961]: training loss: 956.292725, consuming time:65.8783 s\n",
      "[epoch 2962]: training loss: 935.892517, consuming time:62.0008 s\n",
      "[epoch 2963]: training loss: 776.702271, consuming time:61.9194 s\n",
      "[epoch 2964]: training loss: 1079.184448, consuming time:62.0596 s\n",
      "[epoch 2965]: training loss: 812.480408, consuming time:74.1133 s\n",
      "[epoch 2966]: training loss: 876.915283, consuming time:72.3576 s\n",
      "[epoch 2967]: training loss: 948.377380, consuming time:71.2898 s\n",
      "[epoch 2968]: training loss: 1006.703796, consuming time:72.5070 s\n",
      "[epoch 2969]: training loss: 983.963013, consuming time:61.8594 s\n",
      "[epoch 2970]: training loss: 1060.014282, consuming time:61.7551 s\n",
      "[epoch 2971]: training loss: 1029.618286, consuming time:61.5481 s\n",
      "[epoch 2972]: training loss: 1096.487061, consuming time:61.9925 s\n",
      "[epoch 2973]: training loss: 666.346802, consuming time:61.7868 s\n",
      "[epoch 2974]: training loss: 1211.640503, consuming time:61.8752 s\n",
      "[epoch 2975]: training loss: 891.819702, consuming time:61.6735 s\n",
      "[epoch 2976]: training loss: 991.364624, consuming time:61.9011 s\n",
      "[epoch 2977]: training loss: 922.188416, consuming time:61.8407 s\n",
      "[epoch 2978]: training loss: 1022.552856, consuming time:73.0715 s\n",
      "[epoch 2979]: training loss: 852.755127, consuming time:63.1644 s\n",
      "[epoch 2980]: training loss: 723.051270, consuming time:66.4017 s\n",
      "[epoch 2981]: training loss: 1068.339233, consuming time:69.2884 s\n",
      "[epoch 2982]: training loss: 1088.865234, consuming time:62.1682 s\n",
      "[epoch 2983]: training loss: 1066.868652, consuming time:62.0671 s\n",
      "[epoch 2984]: training loss: 1001.908569, consuming time:61.5326 s\n",
      "[epoch 2985]: training loss: 1056.729492, consuming time:66.2140 s\n",
      "[epoch 2986]: training loss: 889.299927, consuming time:63.2349 s\n",
      "[epoch 2987]: training loss: 1099.664795, consuming time:61.9916 s\n",
      "[epoch 2988]: training loss: 991.303589, consuming time:61.8961 s\n",
      "[epoch 2989]: training loss: 1082.289795, consuming time:62.0523 s\n",
      "[epoch 2990]: training loss: 712.980713, consuming time:62.0800 s\n",
      "[epoch 2991]: training loss: 693.874023, consuming time:61.8171 s\n",
      "[epoch 2992]: training loss: 947.282532, consuming time:62.0166 s\n",
      "[epoch 2993]: training loss: 1083.543945, consuming time:62.4757 s\n",
      "[epoch 2994]: training loss: 1505.378174, consuming time:62.1418 s\n",
      "[epoch 2995]: training loss: 894.067261, consuming time:61.9127 s\n",
      "[epoch 2996]: training loss: 823.683594, consuming time:62.0247 s\n",
      "[epoch 2997]: training loss: 967.634521, consuming time:62.0008 s\n",
      "[epoch 2998]: training loss: 793.500732, consuming time:61.9036 s\n",
      "[epoch 2999]: training loss: 1511.347168, consuming time:62.0977 s\n",
      "[epoch 3000]: training loss: 654.406067, consuming time:65.2742 s\n",
      "[epoch 3001]: training loss: 943.589844, consuming time:68.1122 s\n",
      "[epoch 3002]: training loss: 934.384155, consuming time:65.7324 s\n",
      "[epoch 3003]: training loss: 1127.680420, consuming time:63.1598 s\n",
      "[epoch 3004]: training loss: 973.894409, consuming time:62.7180 s\n",
      "[epoch 3005]: training loss: 724.745728, consuming time:62.7951 s\n",
      "[epoch 3006]: training loss: 885.519531, consuming time:63.4130 s\n",
      "[epoch 3007]: training loss: 1118.801270, consuming time:62.1964 s\n",
      "[epoch 3008]: training loss: 951.570312, consuming time:62.2863 s\n",
      "[epoch 3009]: training loss: 951.812012, consuming time:62.0601 s\n",
      "[epoch 3010]: training loss: 1024.413574, consuming time:61.8704 s\n",
      "[epoch 3011]: training loss: 938.886169, consuming time:61.9936 s\n",
      "[epoch 3012]: training loss: 997.006165, consuming time:62.0777 s\n",
      "[epoch 3013]: training loss: 928.983398, consuming time:62.1812 s\n",
      "[epoch 3014]: training loss: 1206.695190, consuming time:62.1423 s\n",
      "[epoch 3015]: training loss: 996.540588, consuming time:61.9884 s\n",
      "[epoch 3016]: training loss: 857.886169, consuming time:62.0028 s\n",
      "[epoch 3017]: training loss: 1091.778931, consuming time:61.8504 s\n",
      "[epoch 3018]: training loss: 796.180420, consuming time:61.8551 s\n",
      "[epoch 3019]: training loss: 1155.624023, consuming time:62.0900 s\n",
      "[epoch 3020]: training loss: 924.484558, consuming time:61.8904 s\n",
      "[epoch 3021]: training loss: 982.800171, consuming time:62.0902 s\n",
      "[epoch 3022]: training loss: 920.315430, consuming time:61.8822 s\n",
      "[epoch 3023]: training loss: 1114.906738, consuming time:63.3597 s\n",
      "[epoch 3024]: training loss: 721.259155, consuming time:64.6873 s\n",
      "[epoch 3025]: training loss: 994.554688, consuming time:61.9393 s\n",
      "[epoch 3026]: training loss: 1049.695435, consuming time:62.1681 s\n",
      "[epoch 3027]: training loss: 890.005920, consuming time:62.8549 s\n",
      "[epoch 3028]: training loss: 1066.321899, consuming time:70.7121 s\n",
      "[epoch 3029]: training loss: 955.039917, consuming time:70.1744 s\n",
      "[epoch 3030]: training loss: 1074.090576, consuming time:67.0310 s\n",
      "[epoch 3031]: training loss: 1636.676025, consuming time:68.5402 s\n",
      "[epoch 3032]: training loss: 1142.862061, consuming time:62.1256 s\n",
      "[epoch 3033]: training loss: 884.767456, consuming time:62.4272 s\n",
      "[epoch 3034]: training loss: 969.299866, consuming time:62.1198 s\n",
      "[epoch 3035]: training loss: 963.888428, consuming time:62.0833 s\n",
      "[epoch 3036]: training loss: 965.535889, consuming time:61.7596 s\n",
      "[epoch 3037]: training loss: 912.790588, consuming time:61.9652 s\n",
      "[epoch 3038]: training loss: 997.416382, consuming time:61.9108 s\n",
      "[epoch 3039]: training loss: 968.965271, consuming time:62.0496 s\n",
      "[epoch 3040]: training loss: 752.386963, consuming time:62.1367 s\n",
      "[epoch 3041]: training loss: 849.112793, consuming time:65.5886 s\n",
      "[epoch 3042]: training loss: 882.111816, consuming time:67.1161 s\n",
      "[epoch 3043]: training loss: 752.532288, consuming time:69.3721 s\n",
      "[epoch 3044]: training loss: 942.176941, consuming time:68.5738 s\n",
      "[epoch 3045]: training loss: 1035.040527, consuming time:63.9246 s\n",
      "[epoch 3046]: training loss: 874.850891, consuming time:62.1417 s\n",
      "[epoch 3047]: training loss: 766.852905, consuming time:68.6703 s\n",
      "[epoch 3048]: training loss: 1290.370117, consuming time:62.6864 s\n",
      "[epoch 3049]: training loss: 966.987793, consuming time:62.2166 s\n",
      "[epoch 3050]: training loss: 1199.351807, consuming time:61.6933 s\n",
      "[epoch 3051]: training loss: 890.251221, consuming time:62.2429 s\n",
      "[epoch 3052]: training loss: 1177.942749, consuming time:61.9712 s\n",
      "[epoch 3053]: training loss: 1109.281494, consuming time:61.6910 s\n",
      "[epoch 3054]: training loss: 1159.803833, consuming time:62.0053 s\n",
      "[epoch 3055]: training loss: 1197.382080, consuming time:61.8376 s\n",
      "[epoch 3056]: training loss: 1100.312134, consuming time:61.7470 s\n",
      "[epoch 3057]: training loss: 1011.317749, consuming time:61.6953 s\n",
      "[epoch 3058]: training loss: 783.374451, consuming time:63.5220 s\n",
      "[epoch 3059]: training loss: 976.517944, consuming time:73.6326 s\n",
      "[epoch 3060]: training loss: 1079.349731, consuming time:76.0645 s\n",
      "[epoch 3061]: training loss: 599.594055, consuming time:72.9117 s\n",
      "[epoch 3062]: training loss: 1088.100220, consuming time:75.8317 s\n",
      "[epoch 3063]: training loss: 1027.919434, consuming time:73.4300 s\n",
      "[epoch 3064]: training loss: 1018.702576, consuming time:67.4301 s\n",
      "[epoch 3065]: training loss: 754.967041, consuming time:69.6428 s\n",
      "[epoch 3066]: training loss: 1149.201050, consuming time:72.0572 s\n",
      "[epoch 3067]: training loss: 934.670532, consuming time:72.1111 s\n",
      "[epoch 3068]: training loss: 1012.456787, consuming time:69.7032 s\n",
      "[epoch 3069]: training loss: 1025.624756, consuming time:61.8597 s\n",
      "[epoch 3070]: training loss: 985.390747, consuming time:62.0305 s\n",
      "[epoch 3071]: training loss: 1062.735352, consuming time:61.9008 s\n",
      "[epoch 3072]: training loss: 1144.853638, consuming time:62.5479 s\n",
      "[epoch 3073]: training loss: 647.148315, consuming time:62.0435 s\n",
      "[epoch 3074]: training loss: 1012.546753, consuming time:62.0661 s\n",
      "[epoch 3075]: training loss: 834.948730, consuming time:62.1481 s\n",
      "[epoch 3076]: training loss: 881.466980, consuming time:62.0707 s\n",
      "[epoch 3077]: training loss: 951.555786, consuming time:61.8767 s\n",
      "[epoch 3078]: training loss: 921.993591, consuming time:61.9796 s\n",
      "[epoch 3079]: training loss: 926.349609, consuming time:67.7019 s\n",
      "[epoch 3080]: training loss: 986.018494, consuming time:73.6616 s\n",
      "[epoch 3081]: training loss: 633.998657, consuming time:63.3026 s\n",
      "[epoch 3082]: training loss: 920.567993, consuming time:65.3100 s\n",
      "[epoch 3083]: training loss: 755.995239, consuming time:67.0696 s\n",
      "[epoch 3084]: training loss: 1052.629517, consuming time:64.9776 s\n",
      "[epoch 3085]: training loss: 1095.287109, consuming time:62.5746 s\n",
      "[epoch 3086]: training loss: 929.479370, consuming time:71.7705 s\n",
      "[epoch 3087]: training loss: 1175.626221, consuming time:69.9206 s\n",
      "[epoch 3088]: training loss: 1322.742188, consuming time:72.8328 s\n",
      "[epoch 3089]: training loss: 1115.191284, consuming time:64.1068 s\n",
      "[epoch 3090]: training loss: 1136.410645, consuming time:61.5489 s\n",
      "[epoch 3091]: training loss: 969.847351, consuming time:61.3443 s\n",
      "[epoch 3092]: training loss: 940.047607, consuming time:61.5962 s\n",
      "[epoch 3093]: training loss: 891.997070, consuming time:63.3249 s\n",
      "[epoch 3094]: training loss: 1151.274902, consuming time:61.5058 s\n",
      "[epoch 3095]: training loss: 879.352173, consuming time:63.3016 s\n",
      "[epoch 3096]: training loss: 965.814697, consuming time:66.4363 s\n",
      "[epoch 3097]: training loss: 1044.559937, consuming time:62.4594 s\n",
      "[epoch 3098]: training loss: 682.211426, consuming time:61.4752 s\n",
      "[epoch 3099]: training loss: 1012.601562, consuming time:61.5095 s\n",
      "[epoch 3100]: training loss: 1086.451660, consuming time:61.3171 s\n",
      "[epoch 3101]: training loss: 985.640381, consuming time:62.5263 s\n",
      "[epoch 3102]: training loss: 1269.227905, consuming time:64.3042 s\n",
      "[epoch 3103]: training loss: 1150.092285, consuming time:62.5728 s\n",
      "[epoch 3104]: training loss: 1172.382568, consuming time:61.7784 s\n",
      "[epoch 3105]: training loss: 1004.918213, consuming time:61.8264 s\n",
      "[epoch 3106]: training loss: 1024.470825, consuming time:64.7491 s\n",
      "[epoch 3107]: training loss: 854.653687, consuming time:66.2134 s\n",
      "[epoch 3108]: training loss: 814.586548, consuming time:62.9790 s\n",
      "[epoch 3109]: training loss: 853.875854, consuming time:61.5899 s\n",
      "[epoch 3110]: training loss: 1060.243286, consuming time:61.8543 s\n",
      "[epoch 3111]: training loss: 1165.738281, consuming time:61.6859 s\n",
      "[epoch 3112]: training loss: 978.990234, consuming time:61.8193 s\n",
      "[epoch 3113]: training loss: 963.948303, consuming time:61.5564 s\n",
      "[epoch 3114]: training loss: 900.044189, consuming time:61.4844 s\n",
      "[epoch 3115]: training loss: 807.842651, consuming time:61.5321 s\n",
      "[epoch 3116]: training loss: 840.094360, consuming time:61.2813 s\n",
      "[epoch 3117]: training loss: 1054.777710, consuming time:61.3935 s\n",
      "[epoch 3118]: training loss: 1348.312256, consuming time:61.6076 s\n",
      "[epoch 3119]: training loss: 888.184875, consuming time:61.5159 s\n",
      "[epoch 3120]: training loss: 791.706482, consuming time:61.5830 s\n",
      "[epoch 3121]: training loss: 1194.470825, consuming time:61.4279 s\n",
      "[epoch 3122]: training loss: 960.604736, consuming time:61.6732 s\n",
      "[epoch 3123]: training loss: 1185.007080, consuming time:61.6223 s\n",
      "[epoch 3124]: training loss: 835.495056, consuming time:61.6223 s\n",
      "[epoch 3125]: training loss: 958.460266, consuming time:61.4917 s\n",
      "[epoch 3126]: training loss: 725.844727, consuming time:61.5800 s\n",
      "[epoch 3127]: training loss: 843.014160, consuming time:61.5228 s\n",
      "[epoch 3128]: training loss: 958.564941, consuming time:61.3376 s\n",
      "[epoch 3129]: training loss: 963.106812, consuming time:61.5162 s\n",
      "[epoch 3130]: training loss: 988.661865, consuming time:61.5646 s\n",
      "[epoch 3131]: training loss: 900.994141, consuming time:61.4438 s\n",
      "[epoch 3132]: training loss: 883.489746, consuming time:61.4231 s\n",
      "[epoch 3133]: training loss: 1524.730225, consuming time:61.2952 s\n",
      "[epoch 3134]: training loss: 1234.293701, consuming time:61.4920 s\n",
      "[epoch 3135]: training loss: 867.829956, consuming time:61.6842 s\n",
      "[epoch 3136]: training loss: 1226.902832, consuming time:61.2550 s\n",
      "[epoch 3137]: training loss: 934.096252, consuming time:61.3330 s\n",
      "[epoch 3138]: training loss: 1036.514038, consuming time:61.6177 s\n",
      "[epoch 3139]: training loss: 1040.901855, consuming time:61.3357 s\n",
      "[epoch 3140]: training loss: 1180.178955, consuming time:61.5643 s\n",
      "[epoch 3141]: training loss: 1214.350830, consuming time:61.6165 s\n",
      "[epoch 3142]: training loss: 764.488831, consuming time:61.3105 s\n",
      "[epoch 3143]: training loss: 791.125793, consuming time:61.4929 s\n",
      "[epoch 3144]: training loss: 816.193115, consuming time:61.5948 s\n",
      "[epoch 3145]: training loss: 764.573914, consuming time:61.4025 s\n",
      "[epoch 3146]: training loss: 912.410950, consuming time:61.5999 s\n",
      "[epoch 3147]: training loss: 813.714966, consuming time:61.4655 s\n",
      "[epoch 3148]: training loss: 1166.749756, consuming time:61.5478 s\n",
      "[epoch 3149]: training loss: 1347.548340, consuming time:61.4552 s\n",
      "[epoch 3150]: training loss: 1074.504639, consuming time:61.6031 s\n",
      "[epoch 3151]: training loss: 1054.729736, consuming time:61.6278 s\n",
      "[epoch 3152]: training loss: 1148.502808, consuming time:61.5111 s\n",
      "[epoch 3153]: training loss: 877.712891, consuming time:61.4468 s\n",
      "[epoch 3154]: training loss: 1285.669189, consuming time:61.3511 s\n",
      "[epoch 3155]: training loss: 1576.992188, consuming time:61.4924 s\n",
      "[epoch 3156]: training loss: 997.384033, consuming time:61.5345 s\n",
      "[epoch 3157]: training loss: 1037.249146, consuming time:61.3079 s\n",
      "[epoch 3158]: training loss: 1153.003296, consuming time:61.6221 s\n",
      "[epoch 3159]: training loss: 847.803650, consuming time:61.3724 s\n",
      "[epoch 3160]: training loss: 1058.738525, consuming time:61.6432 s\n",
      "[epoch 3161]: training loss: 899.751831, consuming time:78.0361 s\n",
      "[epoch 3162]: training loss: 1160.356079, consuming time:85.7652 s\n",
      "[epoch 3163]: training loss: 1166.987549, consuming time:78.6456 s\n",
      "[epoch 3164]: training loss: 973.054199, consuming time:123.8721 s\n",
      "[epoch 3165]: training loss: 868.192505, consuming time:124.0694 s\n",
      "[epoch 3166]: training loss: 737.372253, consuming time:129.5563 s\n",
      "[epoch 3167]: training loss: 966.209290, consuming time:126.7924 s\n",
      "[epoch 3168]: training loss: 952.530029, consuming time:129.6619 s\n",
      "[epoch 3169]: training loss: 1140.560181, consuming time:128.0750 s\n",
      "[epoch 3170]: training loss: 1121.114502, consuming time:99.7940 s\n",
      "[epoch 3171]: training loss: 1024.031372, consuming time:99.6957 s\n",
      "[epoch 3172]: training loss: 1117.365845, consuming time:133.3902 s\n",
      "[epoch 3173]: training loss: 929.226807, consuming time:140.8567 s\n",
      "[epoch 3174]: training loss: 712.964294, consuming time:137.2445 s\n",
      "[epoch 3175]: training loss: 1149.342285, consuming time:138.0234 s\n",
      "[epoch 3176]: training loss: 844.613464, consuming time:136.9737 s\n",
      "[epoch 3177]: training loss: 1290.240234, consuming time:146.2172 s\n",
      "[epoch 3178]: training loss: 632.001953, consuming time:139.5158 s\n",
      "[epoch 3179]: training loss: 1289.645630, consuming time:114.6532 s\n",
      "[epoch 3180]: training loss: 1027.569946, consuming time:84.1459 s\n",
      "[epoch 3181]: training loss: 1151.812744, consuming time:79.9695 s\n",
      "[epoch 3182]: training loss: 770.498718, consuming time:109.0022 s\n",
      "[epoch 3183]: training loss: 1142.224121, consuming time:125.0722 s\n",
      "[epoch 3184]: training loss: 993.611694, consuming time:124.7515 s\n",
      "[epoch 3185]: training loss: 981.935974, consuming time:137.7550 s\n",
      "[epoch 3186]: training loss: 1183.675049, consuming time:127.2156 s\n",
      "[epoch 3187]: training loss: 1113.444214, consuming time:131.8108 s\n",
      "[epoch 3188]: training loss: 810.498840, consuming time:126.2483 s\n",
      "[epoch 3189]: training loss: 875.228271, consuming time:127.7655 s\n",
      "[epoch 3190]: training loss: 961.616333, consuming time:118.8129 s\n",
      "[epoch 3191]: training loss: 948.594849, consuming time:83.6825 s\n",
      "[epoch 3192]: training loss: 769.051025, consuming time:77.2395 s\n",
      "[epoch 3193]: training loss: 735.932861, consuming time:124.0827 s\n",
      "[epoch 3194]: training loss: 802.395142, consuming time:127.0894 s\n",
      "[epoch 3195]: training loss: 894.532349, consuming time:135.2790 s\n",
      "[epoch 3196]: training loss: 824.386841, consuming time:132.8904 s\n",
      "[epoch 3197]: training loss: 935.385864, consuming time:139.6866 s\n",
      "[epoch 3198]: training loss: 902.597473, consuming time:136.0734 s\n",
      "[epoch 3199]: training loss: 850.740234, consuming time:133.8422 s\n",
      "[epoch 3200]: training loss: 709.953674, consuming time:125.4316 s\n",
      "[epoch 3201]: training loss: 822.774780, consuming time:100.8716 s\n",
      "[epoch 3202]: training loss: 821.407959, consuming time:104.1379 s\n",
      "[epoch 3203]: training loss: 1135.574951, consuming time:124.9710 s\n",
      "[epoch 3204]: training loss: 1314.907959, consuming time:129.0208 s\n",
      "[epoch 3205]: training loss: 828.670898, consuming time:127.7755 s\n",
      "[epoch 3206]: training loss: 943.192871, consuming time:125.5245 s\n",
      "[epoch 3207]: training loss: 952.746582, consuming time:133.5576 s\n",
      "[epoch 3208]: training loss: 1056.330566, consuming time:130.1856 s\n",
      "[epoch 3209]: training loss: 929.924683, consuming time:129.9850 s\n",
      "[epoch 3210]: training loss: 1033.335205, consuming time:127.7429 s\n",
      "[epoch 3211]: training loss: 846.365479, consuming time:126.4032 s\n",
      "[epoch 3212]: training loss: 1205.347412, consuming time:103.7377 s\n",
      "[epoch 3213]: training loss: 1304.878418, consuming time:81.0946 s\n",
      "[epoch 3214]: training loss: 868.416687, consuming time:75.0013 s\n",
      "[epoch 3215]: training loss: 951.007202, consuming time:110.6061 s\n",
      "[epoch 3216]: training loss: 939.618164, consuming time:133.4272 s\n",
      "[epoch 3217]: training loss: 979.878235, consuming time:129.5628 s\n",
      "[epoch 3218]: training loss: 726.398376, consuming time:127.1846 s\n",
      "[epoch 3219]: training loss: 931.498962, consuming time:136.2080 s\n",
      "[epoch 3220]: training loss: 1042.563965, consuming time:131.0190 s\n",
      "[epoch 3221]: training loss: 1071.687744, consuming time:125.9287 s\n",
      "[epoch 3222]: training loss: 893.322388, consuming time:128.8674 s\n",
      "[epoch 3223]: training loss: 1005.051880, consuming time:135.7511 s\n",
      "[epoch 3224]: training loss: 883.635376, consuming time:125.2772 s\n",
      "[epoch 3225]: training loss: 906.571777, consuming time:134.1505 s\n",
      "[epoch 3226]: training loss: 980.771484, consuming time:83.9906 s\n",
      "[epoch 3227]: training loss: 1177.837158, consuming time:79.9480 s\n",
      "[epoch 3228]: training loss: 895.355835, consuming time:74.7808 s\n",
      "[epoch 3229]: training loss: 948.508545, consuming time:117.2635 s\n",
      "[epoch 3230]: training loss: 1336.235596, consuming time:134.4596 s\n",
      "[epoch 3231]: training loss: 697.817627, consuming time:135.3437 s\n",
      "[epoch 3232]: training loss: 1133.034424, consuming time:129.4676 s\n",
      "[epoch 3233]: training loss: 960.110657, consuming time:131.3317 s\n",
      "[epoch 3234]: training loss: 900.361938, consuming time:135.1554 s\n",
      "[epoch 3235]: training loss: 1041.572754, consuming time:132.5881 s\n",
      "[epoch 3236]: training loss: 1036.157349, consuming time:131.1407 s\n",
      "[epoch 3237]: training loss: 903.317871, consuming time:131.3133 s\n",
      "[epoch 3238]: training loss: 1046.661255, consuming time:82.3809 s\n",
      "[epoch 3239]: training loss: 1162.004761, consuming time:63.7077 s\n",
      "[epoch 3240]: training loss: 1099.994385, consuming time:64.1259 s\n",
      "[epoch 3241]: training loss: 819.467407, consuming time:61.7980 s\n",
      "[epoch 3242]: training loss: 845.348022, consuming time:61.1029 s\n",
      "[epoch 3243]: training loss: 1006.211304, consuming time:61.8318 s\n",
      "[epoch 3244]: training loss: 901.362244, consuming time:62.1009 s\n",
      "[epoch 3245]: training loss: 1025.623047, consuming time:61.6180 s\n",
      "[epoch 3246]: training loss: 952.025146, consuming time:61.3771 s\n",
      "[epoch 3247]: training loss: 1008.059937, consuming time:61.1854 s\n",
      "[epoch 3248]: training loss: 942.468384, consuming time:62.3863 s\n",
      "[epoch 3249]: training loss: 1157.137451, consuming time:62.3950 s\n",
      "[epoch 3250]: training loss: 870.106812, consuming time:64.6569 s\n",
      "[epoch 3251]: training loss: 1280.396362, consuming time:63.1664 s\n",
      "[epoch 3252]: training loss: 931.532227, consuming time:63.0857 s\n",
      "[epoch 3253]: training loss: 936.907898, consuming time:63.4789 s\n",
      "[epoch 3254]: training loss: 1014.702515, consuming time:63.1492 s\n",
      "[epoch 3255]: training loss: 1121.051514, consuming time:62.9612 s\n",
      "[epoch 3256]: training loss: 833.579346, consuming time:63.8630 s\n",
      "[epoch 3257]: training loss: 974.342896, consuming time:63.7745 s\n",
      "[epoch 3258]: training loss: 946.053467, consuming time:63.1325 s\n",
      "[epoch 3259]: training loss: 1072.622314, consuming time:63.6071 s\n",
      "[epoch 3260]: training loss: 847.856873, consuming time:63.8944 s\n",
      "[epoch 3261]: training loss: 883.077393, consuming time:64.7403 s\n",
      "[epoch 3262]: training loss: 1156.568115, consuming time:61.8482 s\n",
      "[epoch 3263]: training loss: 1295.062500, consuming time:62.0718 s\n",
      "[epoch 3264]: training loss: 970.453674, consuming time:61.9194 s\n",
      "[epoch 3265]: training loss: 858.771118, consuming time:62.0449 s\n",
      "[epoch 3266]: training loss: 834.541931, consuming time:61.3332 s\n",
      "[epoch 3267]: training loss: 1004.975098, consuming time:61.9908 s\n",
      "[epoch 3268]: training loss: 1224.499268, consuming time:61.1222 s\n",
      "[epoch 3269]: training loss: 976.861450, consuming time:60.8246 s\n",
      "[epoch 3270]: training loss: 1158.134521, consuming time:61.1891 s\n",
      "[epoch 3271]: training loss: 1009.378113, consuming time:61.9160 s\n",
      "[epoch 3272]: training loss: 728.332275, consuming time:61.4311 s\n",
      "[epoch 3273]: training loss: 1237.957764, consuming time:61.6822 s\n",
      "[epoch 3274]: training loss: 1015.056458, consuming time:61.8436 s\n",
      "[epoch 3275]: training loss: 1291.617432, consuming time:62.7789 s\n",
      "[epoch 3276]: training loss: 1036.541992, consuming time:66.8622 s\n",
      "[epoch 3277]: training loss: 1045.312134, consuming time:70.6944 s\n",
      "[epoch 3278]: training loss: 959.415283, consuming time:61.5207 s\n",
      "[epoch 3279]: training loss: 949.358643, consuming time:60.9698 s\n",
      "[epoch 3280]: training loss: 991.676514, consuming time:60.6800 s\n",
      "[epoch 3281]: training loss: 810.101074, consuming time:61.3560 s\n",
      "[epoch 3282]: training loss: 955.110779, consuming time:62.0186 s\n",
      "[epoch 3283]: training loss: 884.105103, consuming time:62.0006 s\n",
      "[epoch 3284]: training loss: 1295.375610, consuming time:61.0219 s\n",
      "[epoch 3285]: training loss: 1719.381836, consuming time:61.2060 s\n",
      "[epoch 3286]: training loss: 1107.085205, consuming time:61.9662 s\n",
      "[epoch 3287]: training loss: 839.981567, consuming time:61.4427 s\n",
      "[epoch 3288]: training loss: 858.376831, consuming time:61.9835 s\n",
      "[epoch 3289]: training loss: 1119.768188, consuming time:61.0393 s\n",
      "[epoch 3290]: training loss: 971.478699, consuming time:60.4742 s\n",
      "[epoch 3291]: training loss: 1122.150391, consuming time:60.5120 s\n",
      "[epoch 3292]: training loss: 1065.732788, consuming time:60.6666 s\n",
      "[epoch 3293]: training loss: 1134.032959, consuming time:60.4282 s\n",
      "[epoch 3294]: training loss: 1040.598877, consuming time:61.9742 s\n",
      "[epoch 3295]: training loss: 776.083557, consuming time:60.5068 s\n",
      "[epoch 3296]: training loss: 880.768494, consuming time:60.6833 s\n",
      "[epoch 3297]: training loss: 1141.298340, consuming time:60.5870 s\n",
      "[epoch 3298]: training loss: 1035.673462, consuming time:60.6083 s\n",
      "[epoch 3299]: training loss: 1017.761841, consuming time:61.5159 s\n",
      "[epoch 3300]: training loss: 865.563477, consuming time:61.8362 s\n",
      "[epoch 3301]: training loss: 915.896118, consuming time:62.4271 s\n",
      "[epoch 3302]: training loss: 1013.157349, consuming time:62.2808 s\n",
      "[epoch 3303]: training loss: 1089.366211, consuming time:62.2234 s\n",
      "[epoch 3304]: training loss: 1066.318359, consuming time:61.5639 s\n",
      "[epoch 3305]: training loss: 1048.224243, consuming time:63.9518 s\n",
      "[epoch 3306]: training loss: 879.938477, consuming time:60.7350 s\n",
      "[epoch 3307]: training loss: 619.848511, consuming time:60.9879 s\n",
      "[epoch 3308]: training loss: 795.261230, consuming time:60.7022 s\n",
      "[epoch 3309]: training loss: 1070.836548, consuming time:60.8229 s\n",
      "[epoch 3310]: training loss: 872.622070, consuming time:61.4070 s\n",
      "[epoch 3311]: training loss: 821.401855, consuming time:60.7082 s\n",
      "[epoch 3312]: training loss: 1044.596680, consuming time:60.2878 s\n",
      "[epoch 3313]: training loss: 1039.882935, consuming time:60.8249 s\n",
      "[epoch 3314]: training loss: 1031.733032, consuming time:60.7954 s\n",
      "[epoch 3315]: training loss: 1002.304565, consuming time:60.6649 s\n",
      "[epoch 3316]: training loss: 1128.316895, consuming time:60.6137 s\n",
      "[epoch 3317]: training loss: 961.625732, consuming time:60.4507 s\n",
      "[epoch 3318]: training loss: 1273.819824, consuming time:60.3421 s\n",
      "[epoch 3319]: training loss: 1104.734253, consuming time:60.9944 s\n",
      "[epoch 3320]: training loss: 850.482910, consuming time:60.7143 s\n",
      "[epoch 3321]: training loss: 833.818970, consuming time:61.0819 s\n",
      "[epoch 3322]: training loss: 1052.668823, consuming time:61.0723 s\n",
      "[epoch 3323]: training loss: 1087.091675, consuming time:60.5910 s\n",
      "[epoch 3324]: training loss: 1098.841187, consuming time:60.5253 s\n",
      "[epoch 3325]: training loss: 978.603516, consuming time:61.1059 s\n",
      "[epoch 3326]: training loss: 881.888367, consuming time:60.8681 s\n",
      "[epoch 3327]: training loss: 988.044128, consuming time:60.8271 s\n",
      "[epoch 3328]: training loss: 1084.009277, consuming time:60.5757 s\n",
      "[epoch 3329]: training loss: 674.366272, consuming time:60.9646 s\n",
      "[epoch 3330]: training loss: 1040.729736, consuming time:60.6945 s\n",
      "[epoch 3331]: training loss: 942.259277, consuming time:60.7568 s\n",
      "[epoch 3332]: training loss: 1397.047607, consuming time:60.5184 s\n",
      "[epoch 3333]: training loss: 1067.265503, consuming time:61.8029 s\n",
      "[epoch 3334]: training loss: 1152.507324, consuming time:60.8147 s\n",
      "[epoch 3335]: training loss: 844.840820, consuming time:60.2073 s\n",
      "[epoch 3336]: training loss: 737.961121, consuming time:60.2219 s\n",
      "[epoch 3337]: training loss: 752.868958, consuming time:61.0619 s\n",
      "[epoch 3338]: training loss: 721.606750, consuming time:61.3849 s\n",
      "[epoch 3339]: training loss: 938.864685, consuming time:61.3832 s\n",
      "[epoch 3340]: training loss: 880.531372, consuming time:63.0640 s\n",
      "[epoch 3341]: training loss: 905.542603, consuming time:62.5225 s\n",
      "[epoch 3342]: training loss: 846.991333, consuming time:63.3385 s\n",
      "[epoch 3343]: training loss: 956.380981, consuming time:62.8486 s\n",
      "[epoch 3344]: training loss: 995.144287, consuming time:62.0882 s\n",
      "[epoch 3345]: training loss: 1081.696045, consuming time:62.8979 s\n",
      "[epoch 3346]: training loss: 877.415527, consuming time:61.3307 s\n",
      "[epoch 3347]: training loss: 1246.834106, consuming time:60.9929 s\n",
      "[epoch 3348]: training loss: 1139.332764, consuming time:62.6349 s\n",
      "[epoch 3349]: training loss: 982.239258, consuming time:61.4523 s\n",
      "[epoch 3350]: training loss: 939.310669, consuming time:61.1648 s\n",
      "[epoch 3351]: training loss: 783.210022, consuming time:61.9559 s\n",
      "[epoch 3352]: training loss: 907.711365, consuming time:63.6729 s\n",
      "[epoch 3353]: training loss: 808.736816, consuming time:62.6811 s\n",
      "[epoch 3354]: training loss: 1106.854492, consuming time:61.5385 s\n",
      "[epoch 3355]: training loss: 1178.190430, consuming time:61.8010 s\n",
      "[epoch 3356]: training loss: 1077.025513, consuming time:61.0772 s\n",
      "[epoch 3357]: training loss: 1072.410034, consuming time:60.9423 s\n",
      "[epoch 3358]: training loss: 1219.561035, consuming time:62.5230 s\n",
      "[epoch 3359]: training loss: 1069.677734, consuming time:61.0789 s\n",
      "[epoch 3360]: training loss: 1165.544678, consuming time:60.6940 s\n",
      "[epoch 3361]: training loss: 900.444275, consuming time:61.3319 s\n",
      "[epoch 3362]: training loss: 852.710815, consuming time:60.9600 s\n",
      "[epoch 3363]: training loss: 813.297119, consuming time:61.5323 s\n",
      "[epoch 3364]: training loss: 1016.995422, consuming time:61.1364 s\n",
      "[epoch 3365]: training loss: 1062.573242, consuming time:60.6025 s\n",
      "[epoch 3366]: training loss: 1207.398560, consuming time:60.7522 s\n",
      "[epoch 3367]: training loss: 1085.958984, consuming time:64.4346 s\n",
      "[epoch 3368]: training loss: 864.288452, consuming time:61.0282 s\n",
      "[epoch 3369]: training loss: 794.977844, consuming time:61.0380 s\n",
      "[epoch 3370]: training loss: 1181.438599, consuming time:62.1298 s\n",
      "[epoch 3371]: training loss: 973.765686, consuming time:65.0108 s\n",
      "[epoch 3372]: training loss: 1128.800293, consuming time:62.2933 s\n",
      "[epoch 3373]: training loss: 1022.687439, consuming time:62.2336 s\n",
      "[epoch 3374]: training loss: 1174.392212, consuming time:63.1681 s\n",
      "[epoch 3375]: training loss: 1185.429199, consuming time:62.2997 s\n",
      "[epoch 3376]: training loss: 938.155273, consuming time:62.6701 s\n",
      "[epoch 3377]: training loss: 1292.329590, consuming time:61.7881 s\n",
      "[epoch 3378]: training loss: 738.085205, consuming time:61.6067 s\n",
      "[epoch 3379]: training loss: 1103.960205, consuming time:61.6932 s\n",
      "[epoch 3380]: training loss: 1113.926514, consuming time:63.3077 s\n",
      "[epoch 3381]: training loss: 966.737915, consuming time:66.9480 s\n",
      "[epoch 3382]: training loss: 1112.980957, consuming time:61.6772 s\n",
      "[epoch 3383]: training loss: 912.666565, consuming time:61.1234 s\n",
      "[epoch 3384]: training loss: 855.318542, consuming time:60.0930 s\n",
      "[epoch 3385]: training loss: 1068.039062, consuming time:60.3821 s\n",
      "[epoch 3386]: training loss: 1085.644287, consuming time:60.2170 s\n",
      "[epoch 3387]: training loss: 909.916870, consuming time:60.2299 s\n",
      "[epoch 3388]: training loss: 925.899231, consuming time:60.2375 s\n",
      "[epoch 3389]: training loss: 859.760132, consuming time:60.6947 s\n",
      "[epoch 3390]: training loss: 1215.077148, consuming time:59.8203 s\n",
      "[epoch 3391]: training loss: 1003.398621, consuming time:60.5509 s\n",
      "[epoch 3392]: training loss: 748.758911, consuming time:59.9960 s\n",
      "[epoch 3393]: training loss: 955.236267, consuming time:60.3450 s\n",
      "[epoch 3394]: training loss: 1040.498657, consuming time:60.3079 s\n",
      "[epoch 3395]: training loss: 1387.483643, consuming time:60.6569 s\n",
      "[epoch 3396]: training loss: 920.871155, consuming time:59.5704 s\n",
      "[epoch 3397]: training loss: 948.570923, consuming time:60.1784 s\n",
      "[epoch 3398]: training loss: 1048.705322, consuming time:60.4060 s\n",
      "[epoch 3399]: training loss: 854.139893, consuming time:60.4193 s\n",
      "[epoch 3400]: training loss: 663.214172, consuming time:60.2530 s\n",
      "[epoch 3401]: training loss: 919.293518, consuming time:60.4807 s\n",
      "[epoch 3402]: training loss: 1183.930298, consuming time:60.0573 s\n",
      "[epoch 3403]: training loss: 1033.539185, consuming time:60.1945 s\n",
      "[epoch 3404]: training loss: 923.475220, consuming time:60.4492 s\n",
      "[epoch 3405]: training loss: 958.933899, consuming time:60.1165 s\n",
      "[epoch 3406]: training loss: 1084.160645, consuming time:60.5418 s\n",
      "[epoch 3407]: training loss: 732.416504, consuming time:60.3234 s\n",
      "[epoch 3408]: training loss: 893.389954, consuming time:60.0235 s\n",
      "[epoch 3409]: training loss: 818.552734, consuming time:60.2463 s\n",
      "[epoch 3410]: training loss: 1009.426636, consuming time:60.1916 s\n",
      "[epoch 3411]: training loss: 963.542358, consuming time:60.3169 s\n",
      "[epoch 3412]: training loss: 985.261597, consuming time:60.0029 s\n",
      "[epoch 3413]: training loss: 947.927856, consuming time:60.0083 s\n",
      "[epoch 3414]: training loss: 1028.523926, consuming time:60.4047 s\n",
      "[epoch 3415]: training loss: 916.804749, consuming time:60.1659 s\n",
      "[epoch 3416]: training loss: 876.821045, consuming time:60.5545 s\n",
      "[epoch 3417]: training loss: 801.009583, consuming time:60.2792 s\n",
      "[epoch 3418]: training loss: 1285.140625, consuming time:60.3405 s\n",
      "[epoch 3419]: training loss: 942.140686, consuming time:60.4044 s\n",
      "[epoch 3420]: training loss: 743.701843, consuming time:60.0490 s\n",
      "[epoch 3421]: training loss: 1004.248352, consuming time:60.3748 s\n",
      "[epoch 3422]: training loss: 791.751465, consuming time:60.4300 s\n",
      "[epoch 3423]: training loss: 1084.870361, consuming time:60.2032 s\n",
      "[epoch 3424]: training loss: 888.708252, consuming time:60.3759 s\n",
      "[epoch 3425]: training loss: 998.598022, consuming time:60.1714 s\n",
      "[epoch 3426]: training loss: 1044.042969, consuming time:59.7055 s\n",
      "[epoch 3427]: training loss: 901.125549, consuming time:60.2574 s\n",
      "[epoch 3428]: training loss: 1164.611328, consuming time:60.0913 s\n",
      "[epoch 3429]: training loss: 1415.874146, consuming time:60.3158 s\n",
      "[epoch 3430]: training loss: 807.020020, consuming time:60.5500 s\n",
      "[epoch 3431]: training loss: 1074.085205, consuming time:60.0794 s\n",
      "[epoch 3432]: training loss: 746.783386, consuming time:60.1255 s\n",
      "[epoch 3433]: training loss: 996.975281, consuming time:60.4373 s\n",
      "[epoch 3434]: training loss: 985.591553, consuming time:60.5306 s\n",
      "[epoch 3435]: training loss: 932.355225, consuming time:60.7050 s\n",
      "[epoch 3436]: training loss: 1402.516235, consuming time:60.4335 s\n",
      "[epoch 3437]: training loss: 1326.734741, consuming time:60.3448 s\n",
      "[epoch 3438]: training loss: 1001.840759, consuming time:60.0574 s\n",
      "[epoch 3439]: training loss: 960.653931, consuming time:60.3500 s\n",
      "[epoch 3440]: training loss: 1052.026611, consuming time:60.6579 s\n",
      "[epoch 3441]: training loss: 1028.559570, consuming time:60.8173 s\n",
      "[epoch 3442]: training loss: 869.024292, consuming time:60.1611 s\n",
      "[epoch 3443]: training loss: 1287.565674, consuming time:60.2411 s\n",
      "[epoch 3444]: training loss: 927.329102, consuming time:60.0889 s\n",
      "[epoch 3445]: training loss: 915.321655, consuming time:60.3996 s\n",
      "[epoch 3446]: training loss: 1132.478027, consuming time:60.3973 s\n",
      "[epoch 3447]: training loss: 987.706055, consuming time:60.5861 s\n",
      "[epoch 3448]: training loss: 1104.698608, consuming time:60.2235 s\n",
      "[epoch 3449]: training loss: 1188.616333, consuming time:60.0724 s\n",
      "[epoch 3450]: training loss: 1027.595459, consuming time:60.0832 s\n",
      "[epoch 3451]: training loss: 924.769226, consuming time:60.4010 s\n",
      "[epoch 3452]: training loss: 904.141724, consuming time:60.4175 s\n",
      "[epoch 3453]: training loss: 875.336792, consuming time:60.5480 s\n",
      "[epoch 3454]: training loss: 1117.987549, consuming time:60.4579 s\n",
      "[epoch 3455]: training loss: 1308.351074, consuming time:59.9614 s\n",
      "[epoch 3456]: training loss: 926.533936, consuming time:60.1390 s\n",
      "[epoch 3457]: training loss: 755.204468, consuming time:60.4000 s\n",
      "[epoch 3458]: training loss: 1012.403564, consuming time:60.1247 s\n",
      "[epoch 3459]: training loss: 993.858765, consuming time:60.4843 s\n",
      "[epoch 3460]: training loss: 1485.380249, consuming time:60.1397 s\n",
      "[epoch 3461]: training loss: 1090.699219, consuming time:59.8662 s\n",
      "[epoch 3462]: training loss: 1141.130493, consuming time:60.0668 s\n",
      "[epoch 3463]: training loss: 886.685913, consuming time:60.3401 s\n",
      "[epoch 3464]: training loss: 1234.044922, consuming time:60.1386 s\n",
      "[epoch 3465]: training loss: 969.661499, consuming time:60.6244 s\n",
      "[epoch 3466]: training loss: 1173.805420, consuming time:60.3362 s\n",
      "[epoch 3467]: training loss: 1034.098267, consuming time:60.0239 s\n",
      "[epoch 3468]: training loss: 1356.315308, consuming time:60.1192 s\n",
      "[epoch 3469]: training loss: 832.764160, consuming time:60.3447 s\n",
      "[epoch 3470]: training loss: 800.664551, consuming time:60.5484 s\n",
      "[epoch 3471]: training loss: 1049.575928, consuming time:60.5581 s\n",
      "[epoch 3472]: training loss: 928.373230, consuming time:60.4199 s\n",
      "[epoch 3473]: training loss: 820.474854, consuming time:59.9942 s\n",
      "[epoch 3474]: training loss: 829.036255, consuming time:59.9629 s\n",
      "[epoch 3475]: training loss: 808.856323, consuming time:60.3776 s\n",
      "[epoch 3476]: training loss: 900.723267, consuming time:60.6378 s\n",
      "[epoch 3477]: training loss: 897.664368, consuming time:60.3771 s\n",
      "[epoch 3478]: training loss: 1298.224609, consuming time:60.3702 s\n",
      "[epoch 3479]: training loss: 1006.571289, consuming time:59.8935 s\n",
      "[epoch 3480]: training loss: 1073.483032, consuming time:60.3179 s\n",
      "[epoch 3481]: training loss: 832.150574, consuming time:60.1309 s\n",
      "[epoch 3482]: training loss: 1216.594971, consuming time:60.2231 s\n",
      "[epoch 3483]: training loss: 788.173584, consuming time:60.2713 s\n",
      "[epoch 3484]: training loss: 973.467346, consuming time:60.0722 s\n",
      "[epoch 3485]: training loss: 1042.552734, consuming time:59.9790 s\n",
      "[epoch 3486]: training loss: 1008.155029, consuming time:60.1260 s\n",
      "[epoch 3487]: training loss: 1144.511841, consuming time:60.3906 s\n",
      "[epoch 3488]: training loss: 764.810059, consuming time:60.3958 s\n",
      "[epoch 3489]: training loss: 760.791626, consuming time:60.2205 s\n",
      "[epoch 3490]: training loss: 1023.529907, consuming time:60.1635 s\n",
      "[epoch 3491]: training loss: 1093.328247, consuming time:60.0040 s\n",
      "[epoch 3492]: training loss: 803.679565, consuming time:60.0750 s\n",
      "[epoch 3493]: training loss: 1072.130371, consuming time:60.5684 s\n",
      "[epoch 3494]: training loss: 764.035034, consuming time:60.5751 s\n",
      "[epoch 3495]: training loss: 1018.504395, consuming time:60.3701 s\n",
      "[epoch 3496]: training loss: 1185.248535, consuming time:60.2619 s\n",
      "[epoch 3497]: training loss: 745.640625, consuming time:59.9553 s\n",
      "[epoch 3498]: training loss: 869.515503, consuming time:60.6100 s\n",
      "[epoch 3499]: training loss: 1026.128418, consuming time:60.4159 s\n",
      "[epoch 3500]: training loss: 1069.872925, consuming time:60.4782 s\n",
      "[epoch 3501]: training loss: 1281.167725, consuming time:60.4278 s\n",
      "[epoch 3502]: training loss: 1083.874023, consuming time:60.4721 s\n",
      "[epoch 3503]: training loss: 1064.298584, consuming time:59.7926 s\n",
      "[epoch 3504]: training loss: 840.227173, consuming time:60.3507 s\n",
      "[epoch 3505]: training loss: 950.672852, consuming time:60.4578 s\n",
      "[epoch 3506]: training loss: 876.061890, consuming time:60.3667 s\n",
      "[epoch 3507]: training loss: 904.248169, consuming time:60.2411 s\n",
      "[epoch 3508]: training loss: 577.453918, consuming time:60.0357 s\n",
      "[epoch 3509]: training loss: 932.061584, consuming time:60.2086 s\n",
      "[epoch 3510]: training loss: 996.434204, consuming time:60.1713 s\n",
      "[epoch 3511]: training loss: 726.406860, consuming time:60.4121 s\n",
      "[epoch 3512]: training loss: 872.572632, consuming time:60.3581 s\n",
      "[epoch 3513]: training loss: 1148.280273, consuming time:60.2082 s\n",
      "[epoch 3514]: training loss: 1026.561768, consuming time:60.3896 s\n",
      "[epoch 3515]: training loss: 961.285828, consuming time:59.8562 s\n",
      "[epoch 3516]: training loss: 1041.857910, consuming time:60.4969 s\n",
      "[epoch 3517]: training loss: 983.772522, consuming time:60.4365 s\n",
      "[epoch 3518]: training loss: 839.517212, consuming time:60.1588 s\n",
      "[epoch 3519]: training loss: 1152.656250, consuming time:60.6358 s\n",
      "[epoch 3520]: training loss: 863.303345, consuming time:60.4777 s\n",
      "[epoch 3521]: training loss: 1071.498535, consuming time:59.7792 s\n",
      "[epoch 3522]: training loss: 900.723877, consuming time:60.1658 s\n",
      "[epoch 3523]: training loss: 953.254578, consuming time:60.2043 s\n",
      "[epoch 3524]: training loss: 977.108093, consuming time:60.1596 s\n",
      "[epoch 3525]: training loss: 1355.682617, consuming time:60.3070 s\n",
      "[epoch 3526]: training loss: 855.927185, consuming time:60.4306 s\n",
      "[epoch 3527]: training loss: 1101.242920, consuming time:59.9889 s\n",
      "[epoch 3528]: training loss: 869.375854, consuming time:59.9454 s\n",
      "[epoch 3529]: training loss: 1221.466797, consuming time:60.5935 s\n",
      "[epoch 3530]: training loss: 878.488770, consuming time:60.1446 s\n",
      "[epoch 3531]: training loss: 815.868408, consuming time:60.3588 s\n",
      "[epoch 3532]: training loss: 985.113525, consuming time:60.1125 s\n",
      "[epoch 3533]: training loss: 1279.711548, consuming time:60.0620 s\n",
      "[epoch 3534]: training loss: 1257.283569, consuming time:60.1675 s\n",
      "[epoch 3535]: training loss: 763.436890, consuming time:60.6142 s\n",
      "[epoch 3536]: training loss: 1080.588135, consuming time:60.2381 s\n",
      "[epoch 3537]: training loss: 1080.443604, consuming time:60.3776 s\n",
      "[epoch 3538]: training loss: 881.959534, consuming time:60.1704 s\n",
      "[epoch 3539]: training loss: 851.360901, consuming time:59.7778 s\n",
      "[epoch 3540]: training loss: 716.149475, consuming time:60.2478 s\n",
      "[epoch 3541]: training loss: 917.122620, consuming time:60.6217 s\n",
      "[epoch 3542]: training loss: 1041.366455, consuming time:60.4839 s\n",
      "[epoch 3543]: training loss: 1165.669189, consuming time:60.3673 s\n",
      "[epoch 3544]: training loss: 972.037415, consuming time:60.3208 s\n",
      "[epoch 3545]: training loss: 1056.557861, consuming time:59.8489 s\n",
      "[epoch 3546]: training loss: 1006.715759, consuming time:60.2593 s\n",
      "[epoch 3547]: training loss: 811.734741, consuming time:60.3088 s\n",
      "[epoch 3548]: training loss: 1016.245239, consuming time:60.2587 s\n",
      "[epoch 3549]: training loss: 956.549316, consuming time:60.2655 s\n",
      "[epoch 3550]: training loss: 916.328247, consuming time:60.2746 s\n",
      "[epoch 3551]: training loss: 930.904175, consuming time:59.7306 s\n",
      "[epoch 3552]: training loss: 1080.368652, consuming time:60.4236 s\n",
      "[epoch 3553]: training loss: 942.613281, consuming time:60.3294 s\n",
      "[epoch 3554]: training loss: 923.140381, consuming time:60.2767 s\n",
      "[epoch 3555]: training loss: 1098.295532, consuming time:60.5833 s\n",
      "[epoch 3556]: training loss: 838.885315, consuming time:60.4288 s\n",
      "[epoch 3557]: training loss: 963.697876, consuming time:60.0187 s\n",
      "[epoch 3558]: training loss: 1170.765015, consuming time:60.4873 s\n",
      "[epoch 3559]: training loss: 991.530396, consuming time:60.5391 s\n",
      "[epoch 3560]: training loss: 851.551147, consuming time:60.2561 s\n",
      "[epoch 3561]: training loss: 1054.589355, consuming time:60.6957 s\n",
      "[epoch 3562]: training loss: 757.971558, consuming time:60.3745 s\n",
      "[epoch 3563]: training loss: 1245.461182, consuming time:60.2149 s\n",
      "[epoch 3564]: training loss: 702.960144, consuming time:60.4425 s\n",
      "[epoch 3565]: training loss: 848.658752, consuming time:60.3275 s\n",
      "[epoch 3566]: training loss: 1027.693237, consuming time:60.3506 s\n",
      "[epoch 3567]: training loss: 980.843750, consuming time:60.3311 s\n",
      "[epoch 3568]: training loss: 794.543152, consuming time:63.9233 s\n",
      "[epoch 3569]: training loss: 1215.952393, consuming time:60.1507 s\n",
      "[epoch 3570]: training loss: 848.587219, consuming time:60.3860 s\n",
      "[epoch 3571]: training loss: 1026.834229, consuming time:60.3216 s\n",
      "[epoch 3572]: training loss: 608.600586, consuming time:60.2475 s\n",
      "[epoch 3573]: training loss: 963.663818, consuming time:60.5506 s\n",
      "[epoch 3574]: training loss: 940.152588, consuming time:60.4084 s\n",
      "[epoch 3575]: training loss: 786.450195, consuming time:60.1563 s\n",
      "[epoch 3576]: training loss: 1055.306274, consuming time:60.3006 s\n",
      "[epoch 3577]: training loss: 881.860229, consuming time:60.6071 s\n",
      "[epoch 3578]: training loss: 981.758240, consuming time:60.2724 s\n",
      "[epoch 3579]: training loss: 583.247314, consuming time:60.4336 s\n",
      "[epoch 3580]: training loss: 939.626831, consuming time:59.9816 s\n",
      "[epoch 3581]: training loss: 1043.544189, consuming time:60.0310 s\n",
      "[epoch 3582]: training loss: 944.913086, consuming time:60.2459 s\n",
      "[epoch 3583]: training loss: 912.905396, consuming time:60.4397 s\n",
      "[epoch 3584]: training loss: 743.293518, consuming time:60.2036 s\n",
      "[epoch 3585]: training loss: 987.861267, consuming time:60.2560 s\n",
      "[epoch 3586]: training loss: 941.253784, consuming time:59.9941 s\n",
      "[epoch 3587]: training loss: 1179.224854, consuming time:59.9883 s\n",
      "[epoch 3588]: training loss: 1036.173828, consuming time:60.1364 s\n",
      "[epoch 3589]: training loss: 649.668823, consuming time:60.2352 s\n",
      "[epoch 3590]: training loss: 601.547729, consuming time:60.3326 s\n",
      "[epoch 3591]: training loss: 862.945068, consuming time:60.0868 s\n",
      "[epoch 3592]: training loss: 1141.401367, consuming time:60.0678 s\n",
      "[epoch 3593]: training loss: 966.918030, consuming time:59.9467 s\n",
      "[epoch 3594]: training loss: 1201.728638, consuming time:60.1203 s\n",
      "[epoch 3595]: training loss: 937.504028, consuming time:60.3814 s\n",
      "[epoch 3596]: training loss: 885.008972, consuming time:60.0877 s\n",
      "[epoch 3597]: training loss: 941.933472, consuming time:60.1998 s\n",
      "[epoch 3598]: training loss: 989.218018, consuming time:60.3225 s\n",
      "[epoch 3599]: training loss: 1020.995483, consuming time:59.8490 s\n",
      "[epoch 3600]: training loss: 1208.337891, consuming time:60.5215 s\n",
      "[epoch 3601]: training loss: 1031.096436, consuming time:60.3104 s\n",
      "[epoch 3602]: training loss: 1201.535645, consuming time:60.5469 s\n",
      "[epoch 3603]: training loss: 964.293762, consuming time:60.2282 s\n",
      "[epoch 3604]: training loss: 1054.534302, consuming time:60.1555 s\n",
      "[epoch 3605]: training loss: 981.333923, consuming time:59.6820 s\n",
      "[epoch 3606]: training loss: 996.486206, consuming time:60.3152 s\n",
      "[epoch 3607]: training loss: 971.156738, consuming time:60.1740 s\n",
      "[epoch 3608]: training loss: 1093.736572, consuming time:60.2141 s\n",
      "[epoch 3609]: training loss: 1159.054199, consuming time:60.2138 s\n",
      "[epoch 3610]: training loss: 762.905396, consuming time:60.3677 s\n",
      "[epoch 3611]: training loss: 1140.164795, consuming time:59.6057 s\n",
      "[epoch 3612]: training loss: 860.815613, consuming time:60.3797 s\n",
      "[epoch 3613]: training loss: 925.636963, consuming time:60.3641 s\n",
      "[epoch 3614]: training loss: 689.381836, consuming time:60.6437 s\n",
      "[epoch 3615]: training loss: 1014.207764, consuming time:60.3762 s\n",
      "[epoch 3616]: training loss: 989.271606, consuming time:60.1926 s\n",
      "[epoch 3617]: training loss: 1033.101929, consuming time:59.7823 s\n",
      "[epoch 3618]: training loss: 814.056885, consuming time:60.1487 s\n",
      "[epoch 3619]: training loss: 819.474548, consuming time:60.2454 s\n",
      "[epoch 3620]: training loss: 850.563354, consuming time:60.6118 s\n",
      "[epoch 3621]: training loss: 1032.363037, consuming time:60.0982 s\n",
      "[epoch 3622]: training loss: 1014.723145, consuming time:60.2656 s\n",
      "[epoch 3623]: training loss: 942.833252, consuming time:59.7334 s\n",
      "[epoch 3624]: training loss: 964.403564, consuming time:60.2043 s\n",
      "[epoch 3625]: training loss: 830.866211, consuming time:60.5847 s\n",
      "[epoch 3626]: training loss: 1053.515137, consuming time:60.4132 s\n",
      "[epoch 3627]: training loss: 1446.938354, consuming time:60.6699 s\n",
      "[epoch 3628]: training loss: 917.339966, consuming time:60.4134 s\n",
      "[epoch 3629]: training loss: 961.110657, consuming time:60.0601 s\n",
      "[epoch 3630]: training loss: 931.788818, consuming time:60.5490 s\n",
      "[epoch 3631]: training loss: 1059.200317, consuming time:60.3993 s\n",
      "[epoch 3632]: training loss: 823.117676, consuming time:60.4445 s\n",
      "[epoch 3633]: training loss: 993.108154, consuming time:60.1550 s\n",
      "[epoch 3634]: training loss: 1326.352539, consuming time:60.2727 s\n",
      "[epoch 3635]: training loss: 907.228943, consuming time:60.0427 s\n",
      "[epoch 3636]: training loss: 1059.586914, consuming time:60.0377 s\n",
      "[epoch 3637]: training loss: 1011.537720, consuming time:60.2984 s\n",
      "[epoch 3638]: training loss: 966.691711, consuming time:60.3572 s\n",
      "[epoch 3639]: training loss: 740.576599, consuming time:59.9196 s\n",
      "[epoch 3640]: training loss: 847.765991, consuming time:60.3911 s\n",
      "[epoch 3641]: training loss: 979.931091, consuming time:59.8700 s\n",
      "[epoch 3642]: training loss: 968.704590, consuming time:60.1334 s\n",
      "[epoch 3643]: training loss: 711.522827, consuming time:60.3504 s\n",
      "[epoch 3644]: training loss: 719.265503, consuming time:60.3286 s\n",
      "[epoch 3645]: training loss: 1177.974854, consuming time:60.2321 s\n",
      "[epoch 3646]: training loss: 945.419067, consuming time:60.0315 s\n",
      "[epoch 3647]: training loss: 968.999146, consuming time:59.9838 s\n",
      "[epoch 3648]: training loss: 1041.961914, consuming time:60.2899 s\n",
      "[epoch 3649]: training loss: 1180.258789, consuming time:60.0563 s\n",
      "[epoch 3650]: training loss: 761.335815, consuming time:60.1924 s\n",
      "[epoch 3651]: training loss: 835.687500, consuming time:60.2928 s\n",
      "[epoch 3652]: training loss: 851.078552, consuming time:60.0971 s\n",
      "[epoch 3653]: training loss: 913.196655, consuming time:59.9573 s\n",
      "[epoch 3654]: training loss: 918.757263, consuming time:60.4181 s\n",
      "[epoch 3655]: training loss: 975.082886, consuming time:60.4246 s\n",
      "[epoch 3656]: training loss: 903.558105, consuming time:60.5285 s\n",
      "[epoch 3657]: training loss: 1040.920166, consuming time:60.6462 s\n",
      "[epoch 3658]: training loss: 1057.959229, consuming time:60.0111 s\n",
      "[epoch 3659]: training loss: 1187.488281, consuming time:60.0492 s\n",
      "[epoch 3660]: training loss: 920.156250, consuming time:60.3752 s\n",
      "[epoch 3661]: training loss: 932.826294, consuming time:60.3922 s\n",
      "[epoch 3662]: training loss: 907.881104, consuming time:60.6526 s\n",
      "[epoch 3663]: training loss: 646.018616, consuming time:60.2223 s\n",
      "[epoch 3664]: training loss: 1321.408813, consuming time:59.9640 s\n",
      "[epoch 3665]: training loss: 1372.448486, consuming time:60.0065 s\n",
      "[epoch 3666]: training loss: 896.369019, consuming time:60.6119 s\n",
      "[epoch 3667]: training loss: 1210.180176, consuming time:60.3655 s\n",
      "[epoch 3668]: training loss: 1041.129272, consuming time:60.3536 s\n",
      "[epoch 3669]: training loss: 1127.816650, consuming time:60.1084 s\n",
      "[epoch 3670]: training loss: 1118.858154, consuming time:60.1708 s\n",
      "[epoch 3671]: training loss: 977.790039, consuming time:60.2297 s\n",
      "[epoch 3672]: training loss: 1019.890808, consuming time:60.5163 s\n",
      "[epoch 3673]: training loss: 843.713013, consuming time:60.5046 s\n",
      "[epoch 3674]: training loss: 1023.382568, consuming time:60.0547 s\n",
      "[epoch 3675]: training loss: 1014.078430, consuming time:60.3591 s\n",
      "[epoch 3676]: training loss: 1227.316528, consuming time:59.9142 s\n",
      "[epoch 3677]: training loss: 960.586182, consuming time:59.9609 s\n",
      "[epoch 3678]: training loss: 1108.462402, consuming time:60.1885 s\n",
      "[epoch 3679]: training loss: 674.098755, consuming time:60.5931 s\n",
      "[epoch 3680]: training loss: 901.103516, consuming time:60.3841 s\n",
      "[epoch 3681]: training loss: 797.823120, consuming time:60.2689 s\n",
      "[epoch 3682]: training loss: 815.034546, consuming time:59.9922 s\n",
      "[epoch 3683]: training loss: 876.663696, consuming time:59.9921 s\n",
      "[epoch 3684]: training loss: 1155.800293, consuming time:60.4126 s\n",
      "[epoch 3685]: training loss: 1057.661377, consuming time:60.1464 s\n",
      "[epoch 3686]: training loss: 834.080811, consuming time:60.5271 s\n",
      "[epoch 3687]: training loss: 966.568848, consuming time:60.1555 s\n",
      "[epoch 3688]: training loss: 1154.909180, consuming time:60.0559 s\n",
      "[epoch 3689]: training loss: 1164.378540, consuming time:60.2134 s\n",
      "[epoch 3690]: training loss: 1139.959473, consuming time:60.1894 s\n",
      "[epoch 3691]: training loss: 908.648438, consuming time:60.3638 s\n",
      "[epoch 3692]: training loss: 1054.293457, consuming time:60.6307 s\n",
      "[epoch 3693]: training loss: 930.661499, consuming time:60.2642 s\n",
      "[epoch 3694]: training loss: 977.027466, consuming time:60.1170 s\n",
      "[epoch 3695]: training loss: 859.537476, consuming time:60.4428 s\n",
      "[epoch 3696]: training loss: 1134.724243, consuming time:60.2654 s\n",
      "[epoch 3697]: training loss: 806.613159, consuming time:60.3333 s\n",
      "[epoch 3698]: training loss: 976.642761, consuming time:60.5184 s\n",
      "[epoch 3699]: training loss: 878.148071, consuming time:60.0090 s\n",
      "[epoch 3700]: training loss: 949.595520, consuming time:59.9300 s\n",
      "[epoch 3701]: training loss: 778.080811, consuming time:60.5046 s\n",
      "[epoch 3702]: training loss: 798.435547, consuming time:60.1757 s\n",
      "[epoch 3703]: training loss: 1077.370117, consuming time:60.4147 s\n",
      "[epoch 3704]: training loss: 829.658325, consuming time:60.5412 s\n",
      "[epoch 3705]: training loss: 1109.617432, consuming time:60.1253 s\n",
      "[epoch 3706]: training loss: 771.886841, consuming time:60.1085 s\n",
      "[epoch 3707]: training loss: 1017.655029, consuming time:60.1457 s\n",
      "[epoch 3708]: training loss: 1036.602661, consuming time:60.6447 s\n",
      "[epoch 3709]: training loss: 1093.302368, consuming time:60.4064 s\n",
      "[epoch 3710]: training loss: 997.081848, consuming time:60.2740 s\n",
      "[epoch 3711]: training loss: 1280.289795, consuming time:60.4525 s\n",
      "[epoch 3712]: training loss: 1109.793823, consuming time:59.9880 s\n",
      "[epoch 3713]: training loss: 956.372192, consuming time:59.8816 s\n",
      "[epoch 3714]: training loss: 1235.099365, consuming time:60.4732 s\n",
      "[epoch 3715]: training loss: 936.900208, consuming time:60.3647 s\n",
      "[epoch 3716]: training loss: 973.093933, consuming time:60.5301 s\n",
      "[epoch 3717]: training loss: 1464.167969, consuming time:60.1079 s\n",
      "[epoch 3718]: training loss: 936.209595, consuming time:59.8381 s\n",
      "[epoch 3719]: training loss: 863.433594, consuming time:60.2339 s\n",
      "[epoch 3720]: training loss: 821.592163, consuming time:60.2550 s\n",
      "[epoch 3721]: training loss: 880.582153, consuming time:60.3325 s\n",
      "[epoch 3722]: training loss: 960.305054, consuming time:60.2795 s\n",
      "[epoch 3723]: training loss: 838.574829, consuming time:60.0666 s\n",
      "[epoch 3724]: training loss: 1427.190796, consuming time:59.8843 s\n",
      "[epoch 3725]: training loss: 1044.041016, consuming time:60.2746 s\n",
      "[epoch 3726]: training loss: 971.519653, consuming time:60.0962 s\n",
      "[epoch 3727]: training loss: 835.153564, consuming time:60.2428 s\n",
      "[epoch 3728]: training loss: 847.310791, consuming time:60.1855 s\n",
      "[epoch 3729]: training loss: 662.129028, consuming time:60.3710 s\n",
      "[epoch 3730]: training loss: 1083.549316, consuming time:59.9150 s\n",
      "[epoch 3731]: training loss: 730.207642, consuming time:60.0088 s\n",
      "[epoch 3732]: training loss: 1371.665283, consuming time:60.4487 s\n",
      "[epoch 3733]: training loss: 814.453125, consuming time:60.5166 s\n",
      "[epoch 3734]: training loss: 1216.326660, consuming time:60.1850 s\n",
      "[epoch 3735]: training loss: 1007.250122, consuming time:60.6430 s\n",
      "[epoch 3736]: training loss: 885.269470, consuming time:59.9405 s\n",
      "[epoch 3737]: training loss: 792.992310, consuming time:59.9826 s\n",
      "[epoch 3738]: training loss: 1318.886353, consuming time:60.4309 s\n",
      "[epoch 3739]: training loss: 1274.856445, consuming time:60.5630 s\n",
      "[epoch 3740]: training loss: 763.165039, consuming time:60.3016 s\n",
      "[epoch 3741]: training loss: 881.558289, consuming time:60.1805 s\n",
      "[epoch 3742]: training loss: 966.293945, consuming time:59.8603 s\n",
      "[epoch 3743]: training loss: 884.295654, consuming time:60.3456 s\n",
      "[epoch 3744]: training loss: 1153.937744, consuming time:60.5013 s\n",
      "[epoch 3745]: training loss: 1112.444702, consuming time:60.3134 s\n",
      "[epoch 3746]: training loss: 1448.821045, consuming time:60.4251 s\n",
      "[epoch 3747]: training loss: 1009.475708, consuming time:60.4827 s\n",
      "[epoch 3748]: training loss: 807.135986, consuming time:59.6157 s\n",
      "[epoch 3749]: training loss: 724.476562, consuming time:60.1866 s\n",
      "[epoch 3750]: training loss: 1070.623535, consuming time:60.6029 s\n",
      "[epoch 3751]: training loss: 1273.049316, consuming time:60.2509 s\n",
      "[epoch 3752]: training loss: 990.566406, consuming time:60.4599 s\n",
      "[epoch 3753]: training loss: 951.781128, consuming time:60.2832 s\n",
      "[epoch 3754]: training loss: 939.334961, consuming time:60.0067 s\n",
      "[epoch 3755]: training loss: 939.978699, consuming time:60.3282 s\n",
      "[epoch 3756]: training loss: 1044.117432, consuming time:60.5250 s\n",
      "[epoch 3757]: training loss: 785.235107, consuming time:60.4976 s\n",
      "[epoch 3758]: training loss: 813.077026, consuming time:60.6174 s\n",
      "[epoch 3759]: training loss: 634.695801, consuming time:59.8635 s\n",
      "[epoch 3760]: training loss: 1137.525635, consuming time:59.6790 s\n",
      "[epoch 3761]: training loss: 917.239197, consuming time:60.5436 s\n",
      "[epoch 3762]: training loss: 937.865295, consuming time:60.5347 s\n",
      "[epoch 3763]: training loss: 989.806641, consuming time:60.2229 s\n",
      "[epoch 3764]: training loss: 1123.214844, consuming time:60.1658 s\n",
      "[epoch 3765]: training loss: 1054.610107, consuming time:60.3662 s\n",
      "[epoch 3766]: training loss: 1022.617493, consuming time:59.9115 s\n",
      "[epoch 3767]: training loss: 867.181396, consuming time:60.3430 s\n",
      "[epoch 3768]: training loss: 689.724121, consuming time:60.3792 s\n",
      "[epoch 3769]: training loss: 866.567139, consuming time:60.2103 s\n",
      "[epoch 3770]: training loss: 978.896851, consuming time:60.3542 s\n",
      "[epoch 3771]: training loss: 780.851440, consuming time:60.1965 s\n",
      "[epoch 3772]: training loss: 1183.105225, consuming time:59.9488 s\n",
      "[epoch 3773]: training loss: 1018.773193, consuming time:60.5447 s\n",
      "[epoch 3774]: training loss: 818.788025, consuming time:60.3292 s\n",
      "[epoch 3775]: training loss: 1248.869385, consuming time:60.2864 s\n",
      "[epoch 3776]: training loss: 1021.881470, consuming time:60.1696 s\n",
      "[epoch 3777]: training loss: 798.686157, consuming time:60.4725 s\n",
      "[epoch 3778]: training loss: 913.575073, consuming time:59.8759 s\n",
      "[epoch 3779]: training loss: 1256.027710, consuming time:60.2286 s\n",
      "[epoch 3780]: training loss: 985.313049, consuming time:60.4810 s\n",
      "[epoch 3781]: training loss: 1092.190918, consuming time:60.2973 s\n",
      "[epoch 3782]: training loss: 989.856079, consuming time:60.2966 s\n",
      "[epoch 3783]: training loss: 948.821106, consuming time:60.2960 s\n",
      "[epoch 3784]: training loss: 936.527100, consuming time:59.8637 s\n",
      "[epoch 3785]: training loss: 889.750122, consuming time:60.3491 s\n",
      "[epoch 3786]: training loss: 912.913208, consuming time:60.2138 s\n",
      "[epoch 3787]: training loss: 829.973267, consuming time:60.4192 s\n",
      "[epoch 3788]: training loss: 1224.696289, consuming time:60.5406 s\n",
      "[epoch 3789]: training loss: 1094.739380, consuming time:60.3919 s\n",
      "[epoch 3790]: training loss: 754.230591, consuming time:59.4998 s\n",
      "[epoch 3791]: training loss: 792.073242, consuming time:60.3436 s\n",
      "[epoch 3792]: training loss: 1185.420288, consuming time:60.3893 s\n",
      "[epoch 3793]: training loss: 1091.670532, consuming time:60.3367 s\n",
      "[epoch 3794]: training loss: 902.469727, consuming time:60.5293 s\n",
      "[epoch 3795]: training loss: 1213.564941, consuming time:60.5239 s\n",
      "[epoch 3796]: training loss: 1118.901367, consuming time:59.8581 s\n",
      "[epoch 3797]: training loss: 862.373230, consuming time:60.4435 s\n",
      "[epoch 3798]: training loss: 980.270142, consuming time:60.7139 s\n",
      "[epoch 3799]: training loss: 841.897949, consuming time:60.7025 s\n",
      "[epoch 3800]: training loss: 1080.838745, consuming time:60.5038 s\n",
      "[epoch 3801]: training loss: 767.411926, consuming time:60.1316 s\n",
      "[epoch 3802]: training loss: 1145.883545, consuming time:59.8698 s\n",
      "[epoch 3803]: training loss: 1003.472412, consuming time:60.3815 s\n",
      "[epoch 3804]: training loss: 930.292847, consuming time:60.5863 s\n",
      "[epoch 3805]: training loss: 1113.175903, consuming time:60.4235 s\n",
      "[epoch 3806]: training loss: 985.756287, consuming time:60.8073 s\n",
      "[epoch 3807]: training loss: 1272.112305, consuming time:60.2393 s\n",
      "[epoch 3808]: training loss: 1114.252197, consuming time:60.0063 s\n",
      "[epoch 3809]: training loss: 1228.696289, consuming time:60.1720 s\n",
      "[epoch 3810]: training loss: 1023.462036, consuming time:60.3250 s\n",
      "[epoch 3811]: training loss: 870.517212, consuming time:60.2231 s\n",
      "[epoch 3812]: training loss: 966.471680, consuming time:60.2412 s\n",
      "[epoch 3813]: training loss: 1145.424438, consuming time:60.1715 s\n",
      "[epoch 3814]: training loss: 1186.254395, consuming time:60.1959 s\n",
      "[epoch 3815]: training loss: 1276.870117, consuming time:60.1428 s\n",
      "[epoch 3816]: training loss: 944.389038, consuming time:60.3525 s\n",
      "[epoch 3817]: training loss: 769.841248, consuming time:60.0859 s\n",
      "[epoch 3818]: training loss: 1004.754761, consuming time:60.1827 s\n",
      "[epoch 3819]: training loss: 712.903931, consuming time:60.4492 s\n",
      "[epoch 3820]: training loss: 1010.409668, consuming time:59.9932 s\n",
      "[epoch 3821]: training loss: 697.288330, consuming time:60.2536 s\n",
      "[epoch 3822]: training loss: 824.112671, consuming time:60.4746 s\n",
      "[epoch 3823]: training loss: 1221.265259, consuming time:60.5413 s\n",
      "[epoch 3824]: training loss: 830.000732, consuming time:60.2532 s\n",
      "[epoch 3825]: training loss: 814.998901, consuming time:60.3234 s\n",
      "[epoch 3826]: training loss: 1192.184937, consuming time:60.0981 s\n",
      "[epoch 3827]: training loss: 864.260925, consuming time:60.4374 s\n",
      "[epoch 3828]: training loss: 753.766907, consuming time:60.1139 s\n",
      "[epoch 3829]: training loss: 935.120728, consuming time:60.5005 s\n",
      "[epoch 3830]: training loss: 1043.940918, consuming time:60.3905 s\n",
      "[epoch 3831]: training loss: 973.748413, consuming time:60.2041 s\n",
      "[epoch 3832]: training loss: 945.447205, consuming time:59.8856 s\n",
      "[epoch 3833]: training loss: 920.063354, consuming time:60.2379 s\n",
      "[epoch 3834]: training loss: 932.508972, consuming time:60.2337 s\n",
      "[epoch 3835]: training loss: 767.065674, consuming time:60.5304 s\n",
      "[epoch 3836]: training loss: 1154.702393, consuming time:60.4233 s\n",
      "[epoch 3837]: training loss: 1075.083252, consuming time:60.0819 s\n",
      "[epoch 3838]: training loss: 898.676758, consuming time:60.0010 s\n",
      "[epoch 3839]: training loss: 669.898071, consuming time:60.1276 s\n",
      "[epoch 3840]: training loss: 1200.452148, consuming time:60.5457 s\n",
      "[epoch 3841]: training loss: 931.879272, consuming time:60.5185 s\n",
      "[epoch 3842]: training loss: 1193.669556, consuming time:60.3733 s\n",
      "[epoch 3843]: training loss: 909.073853, consuming time:60.1380 s\n",
      "[epoch 3844]: training loss: 876.311279, consuming time:59.5831 s\n",
      "[epoch 3845]: training loss: 953.628418, consuming time:60.5995 s\n",
      "[epoch 3846]: training loss: 1106.427979, consuming time:60.3924 s\n",
      "[epoch 3847]: training loss: 1180.977539, consuming time:60.4582 s\n",
      "[epoch 3848]: training loss: 910.568726, consuming time:60.3034 s\n",
      "[epoch 3849]: training loss: 1070.539062, consuming time:60.3338 s\n",
      "[epoch 3850]: training loss: 956.713623, consuming time:59.9020 s\n",
      "[epoch 3851]: training loss: 770.261353, consuming time:60.3735 s\n",
      "[epoch 3852]: training loss: 992.876038, consuming time:60.3213 s\n",
      "[epoch 3853]: training loss: 1270.681396, consuming time:60.3465 s\n",
      "[epoch 3854]: training loss: 1169.164062, consuming time:60.3117 s\n",
      "[epoch 3855]: training loss: 823.019287, consuming time:59.8920 s\n",
      "[epoch 3856]: training loss: 911.499634, consuming time:60.1976 s\n",
      "[epoch 3857]: training loss: 916.013184, consuming time:60.5530 s\n",
      "[epoch 3858]: training loss: 1157.157227, consuming time:60.2427 s\n",
      "[epoch 3859]: training loss: 1255.939697, consuming time:60.5466 s\n",
      "[epoch 3860]: training loss: 828.713745, consuming time:60.1875 s\n",
      "[epoch 3861]: training loss: 1018.781799, consuming time:64.5399 s\n",
      "[epoch 3862]: training loss: 1116.449097, consuming time:60.0521 s\n",
      "[epoch 3863]: training loss: 1068.404785, consuming time:60.2646 s\n",
      "[epoch 3864]: training loss: 940.478760, consuming time:60.6376 s\n",
      "[epoch 3865]: training loss: 1107.419434, consuming time:60.5675 s\n",
      "[epoch 3866]: training loss: 1034.428101, consuming time:60.2879 s\n",
      "[epoch 3867]: training loss: 1024.220459, consuming time:60.3765 s\n",
      "[epoch 3868]: training loss: 981.926758, consuming time:60.0624 s\n",
      "[epoch 3869]: training loss: 871.305603, consuming time:59.8741 s\n",
      "[epoch 3870]: training loss: 1014.902954, consuming time:60.4336 s\n",
      "[epoch 3871]: training loss: 1069.131592, consuming time:60.3514 s\n",
      "[epoch 3872]: training loss: 777.869019, consuming time:60.3063 s\n",
      "[epoch 3873]: training loss: 1026.472534, consuming time:60.1554 s\n",
      "[epoch 3874]: training loss: 1065.322266, consuming time:60.1329 s\n",
      "[epoch 3875]: training loss: 953.001343, consuming time:60.1554 s\n",
      "[epoch 3876]: training loss: 807.198669, consuming time:60.2323 s\n",
      "[epoch 3877]: training loss: 875.396484, consuming time:60.3334 s\n",
      "[epoch 3878]: training loss: 931.427002, consuming time:60.3978 s\n",
      "[epoch 3879]: training loss: 738.975586, consuming time:60.0004 s\n",
      "[epoch 3880]: training loss: 1018.243286, consuming time:60.0925 s\n",
      "[epoch 3881]: training loss: 709.108154, consuming time:60.2055 s\n",
      "[epoch 3882]: training loss: 833.079346, consuming time:60.1960 s\n",
      "[epoch 3883]: training loss: 930.701294, consuming time:60.4932 s\n",
      "[epoch 3884]: training loss: 963.086060, consuming time:60.3121 s\n",
      "[epoch 3885]: training loss: 981.601807, consuming time:60.1155 s\n",
      "[epoch 3886]: training loss: 891.512390, consuming time:59.9235 s\n",
      "[epoch 3887]: training loss: 946.513245, consuming time:60.0888 s\n",
      "[epoch 3888]: training loss: 906.386719, consuming time:60.2923 s\n",
      "[epoch 3889]: training loss: 818.599243, consuming time:60.2325 s\n",
      "[epoch 3890]: training loss: 859.915405, consuming time:60.4102 s\n",
      "[epoch 3891]: training loss: 960.993774, consuming time:60.2036 s\n",
      "[epoch 3892]: training loss: 1020.860779, consuming time:60.0911 s\n",
      "[epoch 3893]: training loss: 805.934082, consuming time:60.2855 s\n",
      "[epoch 3894]: training loss: 725.750610, consuming time:60.5077 s\n",
      "[epoch 3895]: training loss: 891.462463, consuming time:60.1570 s\n",
      "[epoch 3896]: training loss: 1025.344482, consuming time:60.4085 s\n",
      "[epoch 3897]: training loss: 1000.694214, consuming time:59.7072 s\n",
      "[epoch 3898]: training loss: 950.550964, consuming time:60.0916 s\n",
      "[epoch 3899]: training loss: 857.364624, consuming time:60.5106 s\n",
      "[epoch 3900]: training loss: 764.363159, consuming time:60.4136 s\n",
      "[epoch 3901]: training loss: 661.106934, consuming time:60.1114 s\n",
      "[epoch 3902]: training loss: 816.788086, consuming time:60.0953 s\n",
      "[epoch 3903]: training loss: 826.047485, consuming time:60.0280 s\n",
      "[epoch 3904]: training loss: 939.903198, consuming time:59.7369 s\n",
      "[epoch 3905]: training loss: 895.425537, consuming time:59.9881 s\n",
      "[epoch 3906]: training loss: 1067.338501, consuming time:60.1908 s\n",
      "[epoch 3907]: training loss: 1287.669922, consuming time:60.3752 s\n",
      "[epoch 3908]: training loss: 961.798584, consuming time:60.3117 s\n",
      "[epoch 3909]: training loss: 756.340698, consuming time:60.2452 s\n",
      "[epoch 3910]: training loss: 922.054077, consuming time:60.0447 s\n",
      "[epoch 3911]: training loss: 1276.890503, consuming time:60.3612 s\n",
      "[epoch 3912]: training loss: 1023.764587, consuming time:60.2539 s\n",
      "[epoch 3913]: training loss: 1127.706299, consuming time:60.3033 s\n",
      "[epoch 3914]: training loss: 808.798950, consuming time:59.9595 s\n",
      "[epoch 3915]: training loss: 903.241089, consuming time:60.1205 s\n",
      "[epoch 3916]: training loss: 797.706299, consuming time:60.1535 s\n",
      "[epoch 3917]: training loss: 881.112854, consuming time:59.9396 s\n",
      "[epoch 3918]: training loss: 930.463013, consuming time:60.4962 s\n",
      "[epoch 3919]: training loss: 1085.287598, consuming time:60.5006 s\n",
      "[epoch 3920]: training loss: 802.575439, consuming time:60.4437 s\n",
      "[epoch 3921]: training loss: 924.604309, consuming time:60.3698 s\n",
      "[epoch 3922]: training loss: 1112.332031, consuming time:59.6796 s\n",
      "[epoch 3923]: training loss: 842.964844, consuming time:60.0321 s\n",
      "[epoch 3924]: training loss: 1096.529419, consuming time:60.3607 s\n",
      "[epoch 3925]: training loss: 1077.145752, consuming time:60.5491 s\n",
      "[epoch 3926]: training loss: 1330.258545, consuming time:60.1492 s\n",
      "[epoch 3927]: training loss: 866.550842, consuming time:60.3291 s\n",
      "[epoch 3928]: training loss: 751.498718, consuming time:60.0948 s\n",
      "[epoch 3929]: training loss: 825.274963, consuming time:60.4104 s\n",
      "[epoch 3930]: training loss: 886.810547, consuming time:60.1406 s\n",
      "[epoch 3931]: training loss: 1120.410645, consuming time:60.5467 s\n",
      "[epoch 3932]: training loss: 988.190186, consuming time:60.1236 s\n",
      "[epoch 3933]: training loss: 968.523682, consuming time:59.8208 s\n",
      "[epoch 3934]: training loss: 1195.035034, consuming time:59.9831 s\n",
      "[epoch 3935]: training loss: 910.439453, consuming time:60.2162 s\n",
      "[epoch 3936]: training loss: 922.246582, consuming time:60.0703 s\n",
      "[epoch 3937]: training loss: 676.234741, consuming time:60.1853 s\n",
      "[epoch 3938]: training loss: 742.646545, consuming time:60.0724 s\n",
      "[epoch 3939]: training loss: 1223.775391, consuming time:59.7577 s\n",
      "[epoch 3940]: training loss: 983.068115, consuming time:59.8565 s\n",
      "[epoch 3941]: training loss: 713.872314, consuming time:59.9845 s\n",
      "[epoch 3942]: training loss: 1133.553955, consuming time:60.3243 s\n",
      "[epoch 3943]: training loss: 888.180786, consuming time:60.1086 s\n",
      "[epoch 3944]: training loss: 1268.192627, consuming time:59.9859 s\n",
      "[epoch 3945]: training loss: 872.981934, consuming time:60.1199 s\n",
      "[epoch 3946]: training loss: 1132.092773, consuming time:59.9529 s\n",
      "[epoch 3947]: training loss: 1036.638916, consuming time:60.1683 s\n",
      "[epoch 3948]: training loss: 974.463989, consuming time:60.0277 s\n",
      "[epoch 3949]: training loss: 862.767456, consuming time:60.1678 s\n",
      "[epoch 3950]: training loss: 1036.609863, consuming time:60.1179 s\n",
      "[epoch 3951]: training loss: 1130.115356, consuming time:59.8664 s\n",
      "[epoch 3952]: training loss: 648.194702, consuming time:59.8082 s\n",
      "[epoch 3953]: training loss: 692.645630, consuming time:60.1791 s\n",
      "[epoch 3954]: training loss: 812.153564, consuming time:60.2602 s\n",
      "[epoch 3955]: training loss: 1158.166016, consuming time:60.1595 s\n",
      "[epoch 3956]: training loss: 1128.061646, consuming time:59.9143 s\n",
      "[epoch 3957]: training loss: 798.219238, consuming time:59.6764 s\n",
      "[epoch 3958]: training loss: 1171.412598, consuming time:60.1649 s\n",
      "[epoch 3959]: training loss: 1015.916931, consuming time:60.0095 s\n",
      "[epoch 3960]: training loss: 1054.285522, consuming time:60.0630 s\n",
      "[epoch 3961]: training loss: 1117.210083, consuming time:60.4178 s\n",
      "[epoch 3962]: training loss: 744.824707, consuming time:60.4151 s\n",
      "[epoch 3963]: training loss: 1036.408325, consuming time:59.8073 s\n",
      "[epoch 3964]: training loss: 961.012695, consuming time:59.9164 s\n",
      "[epoch 3965]: training loss: 1005.580811, consuming time:59.7251 s\n",
      "[epoch 3966]: training loss: 952.069519, consuming time:60.4580 s\n",
      "[epoch 3967]: training loss: 1081.310303, consuming time:60.1424 s\n",
      "[epoch 3968]: training loss: 981.704102, consuming time:59.9218 s\n",
      "[epoch 3969]: training loss: 721.868225, consuming time:59.6132 s\n",
      "[epoch 3970]: training loss: 1336.664307, consuming time:60.0659 s\n",
      "[epoch 3971]: training loss: 1151.975220, consuming time:60.1389 s\n",
      "[epoch 3972]: training loss: 955.356934, consuming time:60.3374 s\n",
      "[epoch 3973]: training loss: 958.751526, consuming time:60.3180 s\n",
      "[epoch 3974]: training loss: 931.256592, consuming time:60.5467 s\n",
      "[epoch 3975]: training loss: 1162.359375, consuming time:59.8388 s\n",
      "[epoch 3976]: training loss: 891.051208, consuming time:59.9347 s\n",
      "[epoch 3977]: training loss: 971.235291, consuming time:60.4485 s\n",
      "[epoch 3978]: training loss: 859.480103, consuming time:60.3906 s\n",
      "[epoch 3979]: training loss: 964.754028, consuming time:60.0808 s\n",
      "[epoch 3980]: training loss: 1142.003418, consuming time:59.7202 s\n",
      "[epoch 3981]: training loss: 793.259277, consuming time:59.8940 s\n",
      "[epoch 3982]: training loss: 892.567078, consuming time:59.7907 s\n",
      "[epoch 3983]: training loss: 988.782959, consuming time:60.0367 s\n",
      "[epoch 3984]: training loss: 893.225098, consuming time:60.2062 s\n",
      "[epoch 3985]: training loss: 772.045837, consuming time:60.2771 s\n",
      "[epoch 3986]: training loss: 858.634766, consuming time:59.8105 s\n",
      "[epoch 3987]: training loss: 1012.490234, consuming time:59.9661 s\n",
      "[epoch 3988]: training loss: 941.346741, consuming time:59.8807 s\n",
      "[epoch 3989]: training loss: 954.284851, consuming time:60.2776 s\n",
      "[epoch 3990]: training loss: 939.339478, consuming time:60.3617 s\n",
      "[epoch 3991]: training loss: 889.588257, consuming time:60.2832 s\n",
      "[epoch 3992]: training loss: 1050.806885, consuming time:60.0409 s\n",
      "[epoch 3993]: training loss: 849.790894, consuming time:59.5333 s\n",
      "[epoch 3994]: training loss: 1256.276611, consuming time:60.0968 s\n",
      "[epoch 3995]: training loss: 904.347778, consuming time:63.7612 s\n",
      "[epoch 3996]: training loss: 1036.424805, consuming time:65.7986 s\n",
      "[epoch 3997]: training loss: 1155.823730, consuming time:61.8288 s\n",
      "[epoch 3998]: training loss: 706.039673, consuming time:61.6639 s\n",
      "[epoch 3999]: training loss: 1086.113770, consuming time:60.7862 s\n",
      "[epoch 4000]: training loss: 1025.892944, consuming time:61.1783 s\n",
      "[epoch 4001]: training loss: 786.477295, consuming time:61.6762 s\n",
      "[epoch 4002]: training loss: 858.412048, consuming time:61.8320 s\n",
      "[epoch 4003]: training loss: 898.009033, consuming time:60.4893 s\n",
      "[epoch 4004]: training loss: 781.177490, consuming time:60.1044 s\n",
      "[epoch 4005]: training loss: 851.376343, consuming time:59.6463 s\n",
      "[epoch 4006]: training loss: 750.898438, consuming time:60.0622 s\n",
      "[epoch 4007]: training loss: 1027.100098, consuming time:60.3455 s\n",
      "[epoch 4008]: training loss: 1005.889526, consuming time:60.2810 s\n",
      "[epoch 4009]: training loss: 1174.377686, consuming time:60.4136 s\n",
      "[epoch 4010]: training loss: 1148.321289, consuming time:60.6430 s\n",
      "[epoch 4011]: training loss: 828.712769, consuming time:59.6689 s\n",
      "[epoch 4012]: training loss: 990.284302, consuming time:60.2080 s\n",
      "[epoch 4013]: training loss: 1021.255310, consuming time:60.5993 s\n",
      "[epoch 4014]: training loss: 1168.547607, consuming time:60.4051 s\n",
      "[epoch 4015]: training loss: 895.856201, consuming time:60.3503 s\n",
      "[epoch 4016]: training loss: 1052.819214, consuming time:59.9579 s\n",
      "[epoch 4017]: training loss: 919.806030, consuming time:59.6121 s\n",
      "[epoch 4018]: training loss: 817.006958, consuming time:60.4218 s\n",
      "[epoch 4019]: training loss: 1160.465820, consuming time:60.1229 s\n",
      "[epoch 4020]: training loss: 841.359070, consuming time:60.4233 s\n",
      "[epoch 4021]: training loss: 988.055664, consuming time:60.1723 s\n",
      "[epoch 4022]: training loss: 1182.740967, consuming time:61.4567 s\n",
      "[epoch 4023]: training loss: 1264.893799, consuming time:61.2533 s\n",
      "[epoch 4024]: training loss: 1017.359375, consuming time:61.0690 s\n",
      "[epoch 4025]: training loss: 919.341797, consuming time:61.3660 s\n",
      "[epoch 4026]: training loss: 760.667786, consuming time:64.3957 s\n",
      "[epoch 4027]: training loss: 1335.098022, consuming time:60.8311 s\n",
      "[epoch 4028]: training loss: 860.751587, consuming time:61.3511 s\n",
      "[epoch 4029]: training loss: 954.318848, consuming time:61.4034 s\n",
      "[epoch 4030]: training loss: 1040.005859, consuming time:61.8821 s\n",
      "[epoch 4031]: training loss: 758.518311, consuming time:60.5706 s\n",
      "[epoch 4032]: training loss: 1173.952637, consuming time:62.5757 s\n",
      "[epoch 4033]: training loss: 851.116455, consuming time:61.8902 s\n",
      "[epoch 4034]: training loss: 901.058777, consuming time:61.6930 s\n",
      "[epoch 4035]: training loss: 1149.955811, consuming time:61.8766 s\n",
      "[epoch 4036]: training loss: 690.246948, consuming time:61.8839 s\n",
      "[epoch 4037]: training loss: 971.440918, consuming time:61.6002 s\n",
      "[epoch 4038]: training loss: 1112.480713, consuming time:60.3641 s\n",
      "[epoch 4039]: training loss: 650.292297, consuming time:59.9241 s\n",
      "[epoch 4040]: training loss: 932.255920, consuming time:60.7178 s\n",
      "[epoch 4041]: training loss: 829.573364, consuming time:60.2424 s\n",
      "[epoch 4042]: training loss: 827.637451, consuming time:60.6799 s\n",
      "[epoch 4043]: training loss: 988.143188, consuming time:60.5889 s\n",
      "[epoch 4044]: training loss: 797.733521, consuming time:60.6729 s\n",
      "[epoch 4045]: training loss: 1129.122314, consuming time:60.5201 s\n",
      "[epoch 4046]: training loss: 743.814636, consuming time:60.4231 s\n",
      "[epoch 4047]: training loss: 909.215027, consuming time:60.8347 s\n",
      "[epoch 4048]: training loss: 980.644043, consuming time:60.6659 s\n",
      "[epoch 4049]: training loss: 983.530396, consuming time:60.9150 s\n",
      "[epoch 4050]: training loss: 975.056152, consuming time:61.4222 s\n",
      "[epoch 4051]: training loss: 1203.083740, consuming time:61.4669 s\n",
      "[epoch 4052]: training loss: 792.954651, consuming time:61.0811 s\n",
      "[epoch 4053]: training loss: 1065.329834, consuming time:61.5037 s\n",
      "[epoch 4054]: training loss: 967.436157, consuming time:61.5703 s\n",
      "[epoch 4055]: training loss: 982.925049, consuming time:65.1458 s\n",
      "[epoch 4056]: training loss: 942.002930, consuming time:65.8402 s\n",
      "[epoch 4057]: training loss: 977.285034, consuming time:65.0622 s\n",
      "[epoch 4058]: training loss: 999.735596, consuming time:60.9924 s\n",
      "[epoch 4059]: training loss: 903.422913, consuming time:61.7183 s\n",
      "[epoch 4060]: training loss: 854.422607, consuming time:62.1270 s\n",
      "[epoch 4061]: training loss: 1119.500000, consuming time:61.9498 s\n",
      "[epoch 4062]: training loss: 872.679321, consuming time:60.9703 s\n",
      "[epoch 4063]: training loss: 642.521057, consuming time:60.8952 s\n",
      "[epoch 4064]: training loss: 1151.322021, consuming time:61.4667 s\n",
      "[epoch 4065]: training loss: 1113.881348, consuming time:61.5522 s\n",
      "[epoch 4066]: training loss: 816.424683, consuming time:61.3133 s\n",
      "[epoch 4067]: training loss: 863.665283, consuming time:61.4822 s\n",
      "[epoch 4068]: training loss: 1057.354248, consuming time:61.0797 s\n",
      "[epoch 4069]: training loss: 983.770081, consuming time:62.3438 s\n",
      "[epoch 4070]: training loss: 1033.458252, consuming time:61.4898 s\n",
      "[epoch 4071]: training loss: 933.329651, consuming time:60.5292 s\n",
      "[epoch 4072]: training loss: 985.863525, consuming time:60.1012 s\n",
      "[epoch 4073]: training loss: 1020.217102, consuming time:59.9819 s\n",
      "[epoch 4074]: training loss: 988.984375, consuming time:60.4384 s\n",
      "[epoch 4075]: training loss: 952.803711, consuming time:60.7620 s\n",
      "[epoch 4076]: training loss: 830.086060, consuming time:60.7922 s\n",
      "[epoch 4077]: training loss: 1033.160889, consuming time:60.9786 s\n",
      "[epoch 4078]: training loss: 1105.320557, consuming time:61.3815 s\n",
      "[epoch 4079]: training loss: 721.330627, consuming time:61.5033 s\n",
      "[epoch 4080]: training loss: 974.622192, consuming time:61.2188 s\n",
      "[epoch 4081]: training loss: 1018.927246, consuming time:61.0859 s\n",
      "[epoch 4082]: training loss: 952.619507, consuming time:61.4444 s\n",
      "[epoch 4083]: training loss: 1010.102661, consuming time:61.1712 s\n",
      "[epoch 4084]: training loss: 754.042725, consuming time:61.1694 s\n",
      "[epoch 4085]: training loss: 1063.123657, consuming time:61.2491 s\n",
      "[epoch 4086]: training loss: 1295.662476, consuming time:61.2291 s\n",
      "[epoch 4087]: training loss: 891.352417, consuming time:61.1130 s\n",
      "[epoch 4088]: training loss: 1026.167969, consuming time:61.1580 s\n",
      "[epoch 4089]: training loss: 1055.266479, consuming time:61.2097 s\n",
      "[epoch 4090]: training loss: 999.699097, consuming time:61.2983 s\n",
      "[epoch 4091]: training loss: 784.932495, consuming time:62.6558 s\n",
      "[epoch 4092]: training loss: 989.557495, consuming time:61.1040 s\n",
      "[epoch 4093]: training loss: 764.739624, consuming time:60.4755 s\n",
      "[epoch 4094]: training loss: 1003.383545, consuming time:60.6190 s\n",
      "[epoch 4095]: training loss: 1068.482178, consuming time:62.5633 s\n",
      "[epoch 4096]: training loss: 1059.086060, consuming time:65.6000 s\n",
      "[epoch 4097]: training loss: 999.770020, consuming time:61.6977 s\n",
      "[epoch 4098]: training loss: 871.580200, consuming time:61.4863 s\n",
      "[epoch 4099]: training loss: 891.579834, consuming time:61.1550 s\n",
      "[epoch 4100]: training loss: 780.489990, consuming time:61.4171 s\n",
      "[epoch 4101]: training loss: 832.612305, consuming time:61.7584 s\n",
      "[epoch 4102]: training loss: 1047.391113, consuming time:61.8014 s\n",
      "[epoch 4103]: training loss: 747.027832, consuming time:61.1665 s\n",
      "[epoch 4104]: training loss: 1090.040405, consuming time:60.7464 s\n",
      "[epoch 4105]: training loss: 1000.409668, consuming time:60.3717 s\n",
      "[epoch 4106]: training loss: 915.187744, consuming time:60.8281 s\n",
      "[epoch 4107]: training loss: 914.067261, consuming time:61.2327 s\n",
      "[epoch 4108]: training loss: 976.961609, consuming time:61.2143 s\n",
      "[epoch 4109]: training loss: 1006.192383, consuming time:60.9897 s\n",
      "[epoch 4110]: training loss: 1117.866699, consuming time:60.9581 s\n",
      "[epoch 4111]: training loss: 1154.552979, consuming time:60.7035 s\n",
      "[epoch 4112]: training loss: 1118.045410, consuming time:68.6627 s\n",
      "[epoch 4113]: training loss: 984.905151, consuming time:68.8573 s\n",
      "[epoch 4114]: training loss: 730.884644, consuming time:67.9515 s\n",
      "[epoch 4115]: training loss: 819.695679, consuming time:61.8389 s\n",
      "[epoch 4116]: training loss: 953.250549, consuming time:61.0029 s\n",
      "[epoch 4117]: training loss: 1012.676636, consuming time:60.7329 s\n",
      "[epoch 4118]: training loss: 896.795532, consuming time:66.4508 s\n",
      "[epoch 4119]: training loss: 1145.953613, consuming time:62.9340 s\n",
      "[epoch 4120]: training loss: 944.986084, consuming time:62.0242 s\n",
      "[epoch 4121]: training loss: 794.567810, consuming time:63.2970 s\n",
      "[epoch 4122]: training loss: 856.484924, consuming time:65.6483 s\n",
      "[epoch 4123]: training loss: 1085.324951, consuming time:61.9386 s\n",
      "[epoch 4124]: training loss: 745.864929, consuming time:61.6544 s\n",
      "[epoch 4125]: training loss: 1129.498901, consuming time:62.1387 s\n",
      "[epoch 4126]: training loss: 843.271484, consuming time:63.3012 s\n",
      "[epoch 4127]: training loss: 909.258301, consuming time:62.5472 s\n",
      "[epoch 4128]: training loss: 916.441406, consuming time:61.1938 s\n",
      "[epoch 4129]: training loss: 872.108154, consuming time:60.8940 s\n",
      "[epoch 4130]: training loss: 871.129517, consuming time:61.3541 s\n",
      "[epoch 4131]: training loss: 1095.311401, consuming time:61.3014 s\n",
      "[epoch 4132]: training loss: 864.155518, consuming time:60.8502 s\n",
      "[epoch 4133]: training loss: 790.564453, consuming time:59.9877 s\n",
      "[epoch 4134]: training loss: 1146.749878, consuming time:60.2682 s\n",
      "[epoch 4135]: training loss: 1106.020020, consuming time:60.5707 s\n",
      "[epoch 4136]: training loss: 910.556213, consuming time:59.8805 s\n",
      "[epoch 4137]: training loss: 1015.651062, consuming time:60.6628 s\n",
      "[epoch 4138]: training loss: 699.798584, consuming time:60.2918 s\n",
      "[epoch 4139]: training loss: 959.201538, consuming time:60.3070 s\n",
      "[epoch 4140]: training loss: 881.938843, consuming time:59.9550 s\n",
      "[epoch 4141]: training loss: 1049.593994, consuming time:60.5307 s\n",
      "[epoch 4142]: training loss: 1002.171265, consuming time:60.6295 s\n",
      "[epoch 4143]: training loss: 1099.842285, consuming time:60.4563 s\n",
      "[epoch 4144]: training loss: 775.688049, consuming time:60.4972 s\n",
      "[epoch 4145]: training loss: 981.089600, consuming time:60.5014 s\n",
      "[epoch 4146]: training loss: 1154.844482, consuming time:60.3285 s\n",
      "[epoch 4147]: training loss: 819.041443, consuming time:60.3076 s\n",
      "[epoch 4148]: training loss: 1252.501221, consuming time:60.7539 s\n",
      "[epoch 4149]: training loss: 1057.063965, consuming time:60.4332 s\n",
      "[epoch 4150]: training loss: 1122.673340, consuming time:60.6157 s\n",
      "[epoch 4151]: training loss: 703.543945, consuming time:60.4704 s\n",
      "[epoch 4152]: training loss: 939.238464, consuming time:60.2383 s\n",
      "[epoch 4153]: training loss: 1033.222656, consuming time:60.3176 s\n",
      "[epoch 4154]: training loss: 873.403137, consuming time:60.5610 s\n",
      "[epoch 4155]: training loss: 1028.865723, consuming time:60.5386 s\n",
      "[epoch 4156]: training loss: 1149.468994, consuming time:60.6348 s\n",
      "[epoch 4157]: training loss: 1084.430054, consuming time:60.2192 s\n",
      "[epoch 4158]: training loss: 997.266113, consuming time:60.0540 s\n",
      "[epoch 4159]: training loss: 1168.455566, consuming time:60.5461 s\n",
      "[epoch 4160]: training loss: 820.995483, consuming time:60.6162 s\n",
      "[epoch 4161]: training loss: 957.704346, consuming time:60.6603 s\n",
      "[epoch 4162]: training loss: 893.937317, consuming time:60.4591 s\n",
      "[epoch 4163]: training loss: 1004.594604, consuming time:60.2227 s\n",
      "[epoch 4164]: training loss: 886.980469, consuming time:60.1641 s\n",
      "[epoch 4165]: training loss: 971.310547, consuming time:60.1001 s\n",
      "[epoch 4166]: training loss: 1114.044922, consuming time:60.2562 s\n",
      "[epoch 4167]: training loss: 915.801086, consuming time:60.5837 s\n",
      "[epoch 4168]: training loss: 845.443481, consuming time:60.1114 s\n",
      "[epoch 4169]: training loss: 1151.869385, consuming time:60.3432 s\n",
      "[epoch 4170]: training loss: 1170.216309, consuming time:59.8620 s\n",
      "[epoch 4171]: training loss: 1142.859619, consuming time:60.2833 s\n",
      "[epoch 4172]: training loss: 703.452637, consuming time:60.5062 s\n",
      "[epoch 4173]: training loss: 942.146362, consuming time:60.1524 s\n",
      "[epoch 4174]: training loss: 962.970398, consuming time:60.6992 s\n",
      "[epoch 4175]: training loss: 998.487183, consuming time:59.8547 s\n",
      "[epoch 4176]: training loss: 736.573975, consuming time:59.9186 s\n",
      "[epoch 4177]: training loss: 844.466492, consuming time:60.5027 s\n",
      "[epoch 4178]: training loss: 853.499451, consuming time:60.3878 s\n",
      "[epoch 4179]: training loss: 1211.218872, consuming time:60.4041 s\n",
      "[epoch 4180]: training loss: 987.328491, consuming time:60.4096 s\n",
      "[epoch 4181]: training loss: 917.842285, consuming time:60.1976 s\n",
      "[epoch 4182]: training loss: 723.126465, consuming time:60.3000 s\n",
      "[epoch 4183]: training loss: 921.125244, consuming time:60.2537 s\n",
      "[epoch 4184]: training loss: 934.131775, consuming time:60.4983 s\n",
      "[epoch 4185]: training loss: 1043.625977, consuming time:60.7029 s\n",
      "[epoch 4186]: training loss: 1140.486206, consuming time:60.3850 s\n",
      "[epoch 4187]: training loss: 718.337463, consuming time:60.2298 s\n",
      "[epoch 4188]: training loss: 946.590210, consuming time:60.4048 s\n",
      "[epoch 4189]: training loss: 1309.264404, consuming time:60.3476 s\n",
      "[epoch 4190]: training loss: 832.958252, consuming time:60.7526 s\n",
      "[epoch 4191]: training loss: 629.850159, consuming time:60.5949 s\n",
      "[epoch 4192]: training loss: 933.241028, consuming time:60.5764 s\n",
      "[epoch 4193]: training loss: 1239.979248, consuming time:60.2278 s\n",
      "[epoch 4194]: training loss: 913.666992, consuming time:60.4242 s\n",
      "[epoch 4195]: training loss: 1196.843018, consuming time:60.2905 s\n",
      "[epoch 4196]: training loss: 974.467529, consuming time:60.7457 s\n",
      "[epoch 4197]: training loss: 1188.862549, consuming time:60.6328 s\n",
      "[epoch 4198]: training loss: 738.906128, consuming time:60.3426 s\n",
      "[epoch 4199]: training loss: 863.699585, consuming time:60.1742 s\n",
      "[epoch 4200]: training loss: 743.351807, consuming time:60.4122 s\n",
      "[epoch 4201]: training loss: 700.692993, consuming time:60.6646 s\n",
      "[epoch 4202]: training loss: 1110.083496, consuming time:60.5182 s\n",
      "[epoch 4203]: training loss: 881.416687, consuming time:60.7736 s\n",
      "[epoch 4204]: training loss: 970.365662, consuming time:60.5943 s\n",
      "[epoch 4205]: training loss: 907.137146, consuming time:60.3627 s\n",
      "[epoch 4206]: training loss: 946.024658, consuming time:60.2512 s\n",
      "[epoch 4207]: training loss: 834.684937, consuming time:60.9287 s\n",
      "[epoch 4208]: training loss: 1077.700317, consuming time:60.7957 s\n",
      "[epoch 4209]: training loss: 935.411987, consuming time:60.4015 s\n",
      "[epoch 4210]: training loss: 1206.561523, consuming time:60.3957 s\n",
      "[epoch 4211]: training loss: 1117.632324, consuming time:59.9950 s\n",
      "[epoch 4212]: training loss: 603.730896, consuming time:60.5637 s\n",
      "[epoch 4213]: training loss: 930.391113, consuming time:60.7887 s\n",
      "[epoch 4214]: training loss: 967.805420, consuming time:60.4545 s\n",
      "[epoch 4215]: training loss: 1023.292725, consuming time:60.4867 s\n",
      "[epoch 4216]: training loss: 638.397400, consuming time:60.6204 s\n",
      "[epoch 4217]: training loss: 1031.294189, consuming time:59.9036 s\n",
      "[epoch 4218]: training loss: 1078.417236, consuming time:60.5060 s\n",
      "[epoch 4219]: training loss: 1031.713989, consuming time:60.4309 s\n",
      "[epoch 4220]: training loss: 842.791016, consuming time:60.7041 s\n",
      "[epoch 4221]: training loss: 1090.921143, consuming time:60.4206 s\n",
      "[epoch 4222]: training loss: 723.500122, consuming time:60.5485 s\n",
      "[epoch 4223]: training loss: 915.905029, consuming time:59.9654 s\n",
      "[epoch 4224]: training loss: 825.776672, consuming time:60.3947 s\n",
      "[epoch 4225]: training loss: 887.991455, consuming time:60.6398 s\n",
      "[epoch 4226]: training loss: 1071.504028, consuming time:60.3424 s\n",
      "[epoch 4227]: training loss: 890.296814, consuming time:60.5523 s\n",
      "[epoch 4228]: training loss: 854.819519, consuming time:60.4821 s\n",
      "[epoch 4229]: training loss: 1080.765137, consuming time:59.9074 s\n",
      "[epoch 4230]: training loss: 1114.143066, consuming time:60.3618 s\n",
      "[epoch 4231]: training loss: 815.401428, consuming time:60.4314 s\n",
      "[epoch 4232]: training loss: 826.249695, consuming time:60.3751 s\n",
      "[epoch 4233]: training loss: 838.515808, consuming time:60.5762 s\n",
      "[epoch 4234]: training loss: 957.898743, consuming time:60.6357 s\n",
      "[epoch 4235]: training loss: 1068.211914, consuming time:59.9031 s\n",
      "[epoch 4236]: training loss: 1003.855225, consuming time:60.1475 s\n",
      "[epoch 4237]: training loss: 894.386353, consuming time:60.3523 s\n",
      "[epoch 4238]: training loss: 980.188660, consuming time:60.7993 s\n",
      "[epoch 4239]: training loss: 960.583374, consuming time:60.5235 s\n",
      "[epoch 4240]: training loss: 685.907288, consuming time:60.5925 s\n",
      "[epoch 4241]: training loss: 1312.333252, consuming time:59.7645 s\n",
      "[epoch 4242]: training loss: 1023.001221, consuming time:60.6788 s\n",
      "[epoch 4243]: training loss: 852.110352, consuming time:60.3844 s\n",
      "[epoch 4244]: training loss: 810.182007, consuming time:60.7318 s\n",
      "[epoch 4245]: training loss: 1321.962769, consuming time:60.5700 s\n",
      "[epoch 4246]: training loss: 929.823669, consuming time:60.4177 s\n",
      "[epoch 4247]: training loss: 1330.650146, consuming time:59.9538 s\n",
      "[epoch 4248]: training loss: 1157.314453, consuming time:60.6022 s\n",
      "[epoch 4249]: training loss: 907.563660, consuming time:60.4600 s\n",
      "[epoch 4250]: training loss: 1058.317627, consuming time:60.6768 s\n",
      "[epoch 4251]: training loss: 1098.938477, consuming time:60.6860 s\n",
      "[epoch 4252]: training loss: 809.312683, consuming time:60.2349 s\n",
      "[epoch 4253]: training loss: 1145.599365, consuming time:59.7199 s\n",
      "[epoch 4254]: training loss: 871.513672, consuming time:60.5602 s\n",
      "[epoch 4255]: training loss: 1073.907104, consuming time:60.6015 s\n",
      "[epoch 4256]: training loss: 840.033813, consuming time:60.5911 s\n",
      "[epoch 4257]: training loss: 892.606750, consuming time:60.6317 s\n",
      "[epoch 4258]: training loss: 903.542236, consuming time:60.3058 s\n",
      "[epoch 4259]: training loss: 908.220398, consuming time:59.9541 s\n",
      "[epoch 4260]: training loss: 970.405029, consuming time:60.3603 s\n",
      "[epoch 4261]: training loss: 856.780823, consuming time:60.7227 s\n",
      "[epoch 4262]: training loss: 1023.789612, consuming time:60.7468 s\n",
      "[epoch 4263]: training loss: 1011.413696, consuming time:60.5612 s\n",
      "[epoch 4264]: training loss: 1203.468384, consuming time:60.1214 s\n",
      "[epoch 4265]: training loss: 1031.304932, consuming time:59.9574 s\n",
      "[epoch 4266]: training loss: 873.502686, consuming time:60.2087 s\n",
      "[epoch 4267]: training loss: 909.564087, consuming time:60.4997 s\n",
      "[epoch 4268]: training loss: 1007.914673, consuming time:60.5085 s\n",
      "[epoch 4269]: training loss: 1072.150757, consuming time:60.2889 s\n",
      "[epoch 4270]: training loss: 1003.639648, consuming time:60.0939 s\n",
      "[epoch 4271]: training loss: 882.466309, consuming time:60.1297 s\n",
      "[epoch 4272]: training loss: 1098.829590, consuming time:60.4074 s\n",
      "[epoch 4273]: training loss: 999.779968, consuming time:60.6054 s\n",
      "[epoch 4274]: training loss: 899.627319, consuming time:60.6937 s\n",
      "[epoch 4275]: training loss: 1065.452148, consuming time:60.3311 s\n",
      "[epoch 4276]: training loss: 983.816833, consuming time:60.0892 s\n",
      "[epoch 4277]: training loss: 1046.090820, consuming time:60.1447 s\n",
      "[epoch 4278]: training loss: 1051.078613, consuming time:60.1214 s\n",
      "[epoch 4279]: training loss: 924.226868, consuming time:60.5945 s\n",
      "[epoch 4280]: training loss: 918.571533, consuming time:60.4170 s\n",
      "[epoch 4281]: training loss: 1141.638672, consuming time:60.5739 s\n",
      "[epoch 4282]: training loss: 1080.243408, consuming time:59.9925 s\n",
      "[epoch 4283]: training loss: 1038.296143, consuming time:60.3572 s\n",
      "[epoch 4284]: training loss: 933.728394, consuming time:60.4088 s\n",
      "[epoch 4285]: training loss: 1114.610107, consuming time:60.3806 s\n",
      "[epoch 4286]: training loss: 924.293335, consuming time:60.5950 s\n",
      "[epoch 4287]: training loss: 889.381409, consuming time:60.4905 s\n",
      "[epoch 4288]: training loss: 813.996155, consuming time:60.1849 s\n",
      "[epoch 4289]: training loss: 867.972046, consuming time:60.0723 s\n",
      "[epoch 4290]: training loss: 943.641663, consuming time:60.7346 s\n",
      "[epoch 4291]: training loss: 906.461060, consuming time:60.4613 s\n",
      "[epoch 4292]: training loss: 1040.308838, consuming time:60.2774 s\n",
      "[epoch 4293]: training loss: 1043.208862, consuming time:60.4945 s\n",
      "[epoch 4294]: training loss: 987.828003, consuming time:60.2092 s\n",
      "[epoch 4295]: training loss: 919.606384, consuming time:60.0136 s\n",
      "[epoch 4296]: training loss: 970.579590, consuming time:60.6637 s\n",
      "[epoch 4297]: training loss: 933.883057, consuming time:60.5297 s\n",
      "[epoch 4298]: training loss: 833.556213, consuming time:60.3639 s\n",
      "[epoch 4299]: training loss: 995.940796, consuming time:60.5423 s\n",
      "[epoch 4300]: training loss: 939.464600, consuming time:60.1957 s\n",
      "[epoch 4301]: training loss: 886.206055, consuming time:60.4131 s\n",
      "[epoch 4302]: training loss: 776.879395, consuming time:60.8057 s\n",
      "[epoch 4303]: training loss: 1026.954834, consuming time:60.4557 s\n",
      "[epoch 4304]: training loss: 873.076660, consuming time:60.7488 s\n",
      "[epoch 4305]: training loss: 958.725952, consuming time:60.3445 s\n",
      "[epoch 4306]: training loss: 988.848267, consuming time:60.3432 s\n",
      "[epoch 4307]: training loss: 805.132935, consuming time:60.4217 s\n",
      "[epoch 4308]: training loss: 1251.636475, consuming time:60.5663 s\n",
      "[epoch 4309]: training loss: 884.280640, consuming time:61.3713 s\n",
      "[epoch 4310]: training loss: 1222.485840, consuming time:60.9577 s\n",
      "[epoch 4311]: training loss: 915.586914, consuming time:60.8640 s\n",
      "[epoch 4312]: training loss: 953.694946, consuming time:65.1350 s\n",
      "[epoch 4313]: training loss: 979.019226, consuming time:61.3814 s\n",
      "[epoch 4314]: training loss: 663.063904, consuming time:61.5045 s\n",
      "[epoch 4315]: training loss: 833.381409, consuming time:63.1720 s\n",
      "[epoch 4316]: training loss: 876.535950, consuming time:62.0205 s\n",
      "[epoch 4317]: training loss: 698.048950, consuming time:68.0353 s\n",
      "[epoch 4318]: training loss: 1154.747070, consuming time:62.8241 s\n",
      "[epoch 4319]: training loss: 1303.771240, consuming time:62.5211 s\n",
      "[epoch 4320]: training loss: 986.355591, consuming time:61.3531 s\n",
      "[epoch 4321]: training loss: 1008.332825, consuming time:61.1595 s\n",
      "[epoch 4322]: training loss: 737.552002, consuming time:60.9583 s\n",
      "[epoch 4323]: training loss: 961.887878, consuming time:61.1094 s\n",
      "[epoch 4324]: training loss: 1218.139160, consuming time:60.8695 s\n",
      "[epoch 4325]: training loss: 916.342712, consuming time:62.6174 s\n",
      "[epoch 4326]: training loss: 710.021484, consuming time:61.3307 s\n",
      "[epoch 4327]: training loss: 1106.940308, consuming time:60.7620 s\n",
      "[epoch 4328]: training loss: 790.069214, consuming time:61.1383 s\n",
      "[epoch 4329]: training loss: 802.415039, consuming time:60.9056 s\n",
      "[epoch 4330]: training loss: 987.024414, consuming time:62.0138 s\n",
      "[epoch 4331]: training loss: 1159.002441, consuming time:65.3087 s\n",
      "[epoch 4332]: training loss: 833.606934, consuming time:61.2127 s\n",
      "[epoch 4333]: training loss: 1106.245483, consuming time:60.9083 s\n",
      "[epoch 4334]: training loss: 1079.683350, consuming time:62.0757 s\n",
      "[epoch 4335]: training loss: 982.592773, consuming time:63.6783 s\n",
      "[epoch 4336]: training loss: 994.932373, consuming time:61.2644 s\n",
      "[epoch 4337]: training loss: 887.979858, consuming time:60.9204 s\n",
      "[epoch 4338]: training loss: 798.471436, consuming time:60.9566 s\n",
      "[epoch 4339]: training loss: 897.145752, consuming time:60.5453 s\n",
      "[epoch 4340]: training loss: 1001.064331, consuming time:60.2888 s\n",
      "[epoch 4341]: training loss: 716.134216, consuming time:60.4396 s\n",
      "[epoch 4342]: training loss: 828.170288, consuming time:60.5346 s\n",
      "[epoch 4343]: training loss: 839.426514, consuming time:60.5818 s\n",
      "[epoch 4344]: training loss: 1064.098511, consuming time:60.5668 s\n",
      "[epoch 4345]: training loss: 1027.691162, consuming time:60.5250 s\n",
      "[epoch 4346]: training loss: 742.812073, consuming time:61.3720 s\n",
      "[epoch 4347]: training loss: 846.379883, consuming time:61.7397 s\n",
      "[epoch 4348]: training loss: 842.289917, consuming time:62.9089 s\n",
      "[epoch 4349]: training loss: 807.607849, consuming time:62.2147 s\n",
      "[epoch 4350]: training loss: 807.336182, consuming time:61.2711 s\n",
      "[epoch 4351]: training loss: 844.622498, consuming time:61.1092 s\n",
      "[epoch 4352]: training loss: 1016.450928, consuming time:60.9825 s\n",
      "[epoch 4353]: training loss: 747.415527, consuming time:61.2446 s\n",
      "[epoch 4354]: training loss: 1086.945068, consuming time:61.1865 s\n",
      "[epoch 4355]: training loss: 1097.567749, consuming time:61.1886 s\n",
      "[epoch 4356]: training loss: 989.417358, consuming time:61.2458 s\n",
      "[epoch 4357]: training loss: 1182.830933, consuming time:61.2235 s\n",
      "[epoch 4358]: training loss: 1063.773438, consuming time:61.3971 s\n",
      "[epoch 4359]: training loss: 875.489014, consuming time:61.2472 s\n",
      "[epoch 4360]: training loss: 934.800049, consuming time:61.3960 s\n",
      "[epoch 4361]: training loss: 1196.375244, consuming time:61.7999 s\n",
      "[epoch 4362]: training loss: 804.341675, consuming time:61.5069 s\n",
      "[epoch 4363]: training loss: 792.799622, consuming time:61.4142 s\n",
      "[epoch 4364]: training loss: 871.516602, consuming time:61.2207 s\n",
      "[epoch 4365]: training loss: 1118.682129, consuming time:61.2631 s\n",
      "[epoch 4366]: training loss: 918.220581, consuming time:61.3977 s\n",
      "[epoch 4367]: training loss: 1107.267090, consuming time:60.7052 s\n",
      "[epoch 4368]: training loss: 986.825073, consuming time:61.4384 s\n",
      "[epoch 4369]: training loss: 1007.608398, consuming time:61.2382 s\n",
      "[epoch 4370]: training loss: 856.069580, consuming time:61.1859 s\n",
      "[epoch 4371]: training loss: 868.613281, consuming time:61.1118 s\n",
      "[epoch 4372]: training loss: 923.183960, consuming time:61.2712 s\n",
      "[epoch 4373]: training loss: 749.996948, consuming time:61.5940 s\n",
      "[epoch 4374]: training loss: 789.667419, consuming time:61.2948 s\n",
      "[epoch 4375]: training loss: 1010.929321, consuming time:65.0927 s\n",
      "[epoch 4376]: training loss: 948.595337, consuming time:61.1687 s\n",
      "[epoch 4377]: training loss: 957.978271, consuming time:60.7885 s\n",
      "[epoch 4378]: training loss: 1067.176025, consuming time:61.3513 s\n",
      "[epoch 4379]: training loss: 771.842041, consuming time:61.7328 s\n",
      "[epoch 4380]: training loss: 892.240112, consuming time:61.6232 s\n",
      "[epoch 4381]: training loss: 786.349365, consuming time:61.0411 s\n",
      "[epoch 4382]: training loss: 801.723755, consuming time:61.4864 s\n",
      "[epoch 4383]: training loss: 909.438110, consuming time:60.8394 s\n",
      "[epoch 4384]: training loss: 740.435791, consuming time:61.4982 s\n",
      "[epoch 4385]: training loss: 1050.323242, consuming time:61.4425 s\n",
      "[epoch 4386]: training loss: 694.083862, consuming time:61.7569 s\n",
      "[epoch 4387]: training loss: 1077.705200, consuming time:62.0044 s\n",
      "[epoch 4388]: training loss: 1112.325928, consuming time:60.9030 s\n",
      "[epoch 4389]: training loss: 929.371826, consuming time:60.9833 s\n",
      "[epoch 4390]: training loss: 822.001282, consuming time:61.4868 s\n",
      "[epoch 4391]: training loss: 764.411682, consuming time:61.4775 s\n",
      "[epoch 4392]: training loss: 1047.085449, consuming time:61.1954 s\n",
      "[epoch 4393]: training loss: 1005.939941, consuming time:63.0522 s\n",
      "[epoch 4394]: training loss: 849.325195, consuming time:62.1908 s\n",
      "[epoch 4395]: training loss: 1390.976074, consuming time:63.6648 s\n",
      "[epoch 4396]: training loss: 974.039673, consuming time:62.3148 s\n",
      "[epoch 4397]: training loss: 1068.095459, consuming time:64.6785 s\n",
      "[epoch 4398]: training loss: 851.552551, consuming time:61.5678 s\n",
      "[epoch 4399]: training loss: 1012.086792, consuming time:61.1500 s\n",
      "[epoch 4400]: training loss: 1260.875610, consuming time:61.1807 s\n",
      "[epoch 4401]: training loss: 793.252441, consuming time:63.4570 s\n",
      "[epoch 4402]: training loss: 727.931152, consuming time:63.0437 s\n",
      "[epoch 4403]: training loss: 806.756897, consuming time:63.8247 s\n",
      "[epoch 4404]: training loss: 909.221802, consuming time:62.5517 s\n",
      "[epoch 4405]: training loss: 879.749023, consuming time:63.2837 s\n",
      "[epoch 4406]: training loss: 657.611877, consuming time:64.1112 s\n",
      "[epoch 4407]: training loss: 890.080505, consuming time:65.5480 s\n",
      "[epoch 4408]: training loss: 912.691223, consuming time:65.2065 s\n",
      "[epoch 4409]: training loss: 1083.076416, consuming time:63.5773 s\n",
      "[epoch 4410]: training loss: 877.954956, consuming time:65.7125 s\n",
      "[epoch 4411]: training loss: 1148.106445, consuming time:64.7431 s\n",
      "[epoch 4412]: training loss: 1141.853760, consuming time:63.0754 s\n",
      "[epoch 4413]: training loss: 963.048401, consuming time:66.2416 s\n",
      "[epoch 4414]: training loss: 1030.189087, consuming time:65.3833 s\n",
      "[epoch 4415]: training loss: 813.081787, consuming time:66.0371 s\n",
      "[epoch 4416]: training loss: 990.603271, consuming time:66.1451 s\n",
      "[epoch 4417]: training loss: 893.574585, consuming time:64.9246 s\n",
      "[epoch 4418]: training loss: 846.382568, consuming time:62.0892 s\n",
      "[epoch 4419]: training loss: 1441.975098, consuming time:62.4036 s\n",
      "[epoch 4420]: training loss: 915.968140, consuming time:62.0358 s\n",
      "[epoch 4421]: training loss: 826.922913, consuming time:62.5019 s\n",
      "[epoch 4422]: training loss: 1008.939697, consuming time:65.9518 s\n",
      "[epoch 4423]: training loss: 1079.490845, consuming time:64.7785 s\n",
      "[epoch 4424]: training loss: 1622.662964, consuming time:67.7488 s\n",
      "[epoch 4425]: training loss: 918.716919, consuming time:65.3401 s\n",
      "[epoch 4426]: training loss: 934.782471, consuming time:64.0391 s\n",
      "[epoch 4427]: training loss: 771.635620, consuming time:61.8881 s\n",
      "[epoch 4428]: training loss: 1001.902466, consuming time:60.9828 s\n",
      "[epoch 4429]: training loss: 866.091919, consuming time:63.6709 s\n",
      "[epoch 4430]: training loss: 1022.405151, consuming time:64.2742 s\n",
      "[epoch 4431]: training loss: 849.687744, consuming time:62.3176 s\n",
      "[epoch 4432]: training loss: 1348.499512, consuming time:68.2405 s\n",
      "[epoch 4433]: training loss: 826.741821, consuming time:66.4694 s\n",
      "[epoch 4434]: training loss: 1085.984741, consuming time:62.7403 s\n",
      "[epoch 4435]: training loss: 1050.327759, consuming time:66.6841 s\n",
      "[epoch 4436]: training loss: 908.574341, consuming time:64.3482 s\n",
      "[epoch 4437]: training loss: 1025.333496, consuming time:63.2692 s\n",
      "[epoch 4438]: training loss: 1028.598999, consuming time:65.7787 s\n",
      "[epoch 4439]: training loss: 1241.943726, consuming time:63.3008 s\n",
      "[epoch 4440]: training loss: 807.018738, consuming time:62.3643 s\n",
      "[epoch 4441]: training loss: 995.745850, consuming time:63.7988 s\n",
      "[epoch 4442]: training loss: 1166.337891, consuming time:64.0675 s\n",
      "[epoch 4443]: training loss: 780.860657, consuming time:67.2541 s\n",
      "[epoch 4444]: training loss: 979.397339, consuming time:64.2861 s\n",
      "[epoch 4445]: training loss: 952.084717, consuming time:65.5921 s\n",
      "[epoch 4446]: training loss: 899.289917, consuming time:64.4717 s\n",
      "[epoch 4447]: training loss: 1118.788208, consuming time:67.6253 s\n",
      "[epoch 4448]: training loss: 1186.182373, consuming time:66.5971 s\n",
      "[epoch 4449]: training loss: 977.640381, consuming time:65.6921 s\n",
      "[epoch 4450]: training loss: 984.037598, consuming time:66.5971 s\n",
      "[epoch 4451]: training loss: 886.773743, consuming time:65.9611 s\n",
      "[epoch 4452]: training loss: 760.992126, consuming time:68.9082 s\n",
      "[epoch 4453]: training loss: 1047.136475, consuming time:69.5136 s\n",
      "[epoch 4454]: training loss: 1066.022949, consuming time:70.0391 s\n",
      "[epoch 4455]: training loss: 812.933228, consuming time:67.2407 s\n",
      "[epoch 4456]: training loss: 1228.198364, consuming time:66.8560 s\n",
      "[epoch 4457]: training loss: 1049.614258, consuming time:67.8918 s\n",
      "[epoch 4458]: training loss: 1168.011841, consuming time:66.9414 s\n",
      "[epoch 4459]: training loss: 898.934326, consuming time:63.1325 s\n",
      "[epoch 4460]: training loss: 1074.490479, consuming time:61.3321 s\n",
      "[epoch 4461]: training loss: 973.435547, consuming time:60.7836 s\n",
      "[epoch 4462]: training loss: 838.843872, consuming time:61.1692 s\n",
      "[epoch 4463]: training loss: 966.517761, consuming time:61.2046 s\n",
      "[epoch 4464]: training loss: 866.910461, consuming time:61.0586 s\n",
      "[epoch 4465]: training loss: 1060.195435, consuming time:61.3505 s\n",
      "[epoch 4466]: training loss: 948.791321, consuming time:60.9967 s\n",
      "[epoch 4467]: training loss: 1050.687500, consuming time:60.8665 s\n",
      "[epoch 4468]: training loss: 700.545776, consuming time:61.2862 s\n",
      "[epoch 4469]: training loss: 709.386780, consuming time:60.8000 s\n",
      "[epoch 4470]: training loss: 1151.062012, consuming time:61.4064 s\n",
      "[epoch 4471]: training loss: 766.132080, consuming time:60.8715 s\n",
      "[epoch 4472]: training loss: 895.077148, consuming time:61.0789 s\n",
      "[epoch 4473]: training loss: 1060.562500, consuming time:60.8904 s\n",
      "[epoch 4474]: training loss: 926.262329, consuming time:61.2840 s\n",
      "[epoch 4475]: training loss: 683.276001, consuming time:60.8887 s\n",
      "[epoch 4476]: training loss: 770.418152, consuming time:61.4520 s\n",
      "[epoch 4477]: training loss: 1063.223511, consuming time:61.2530 s\n",
      "[epoch 4478]: training loss: 876.317871, consuming time:61.4069 s\n",
      "[epoch 4479]: training loss: 918.667908, consuming time:60.9424 s\n",
      "[epoch 4480]: training loss: 970.673645, consuming time:61.1650 s\n",
      "[epoch 4481]: training loss: 867.197266, consuming time:61.1919 s\n",
      "[epoch 4482]: training loss: 710.742920, consuming time:61.5186 s\n",
      "[epoch 4483]: training loss: 978.721313, consuming time:61.3463 s\n",
      "[epoch 4484]: training loss: 927.610229, consuming time:60.9719 s\n",
      "[epoch 4485]: training loss: 1212.232178, consuming time:60.8336 s\n",
      "[epoch 4486]: training loss: 1345.307495, consuming time:61.0225 s\n",
      "[epoch 4487]: training loss: 856.232849, consuming time:61.3049 s\n",
      "[epoch 4488]: training loss: 858.716431, consuming time:61.1513 s\n",
      "[epoch 4489]: training loss: 881.032349, consuming time:61.1486 s\n",
      "[epoch 4490]: training loss: 771.825562, consuming time:61.1274 s\n",
      "[epoch 4491]: training loss: 860.017883, consuming time:61.0729 s\n",
      "[epoch 4492]: training loss: 1009.119019, consuming time:61.3332 s\n",
      "[epoch 4493]: training loss: 957.591309, consuming time:60.7490 s\n",
      "[epoch 4494]: training loss: 1098.192505, consuming time:61.1541 s\n",
      "[epoch 4495]: training loss: 888.636108, consuming time:61.1799 s\n",
      "[epoch 4496]: training loss: 841.887329, consuming time:61.2124 s\n",
      "[epoch 4497]: training loss: 979.930664, consuming time:61.0488 s\n",
      "[epoch 4498]: training loss: 822.502197, consuming time:61.2678 s\n",
      "[epoch 4499]: training loss: 826.855042, consuming time:61.2775 s\n",
      "[epoch 4500]: training loss: 989.376709, consuming time:60.9611 s\n",
      "[epoch 4501]: training loss: 839.325806, consuming time:61.2720 s\n",
      "[epoch 4502]: training loss: 1005.316650, consuming time:61.1486 s\n",
      "[epoch 4503]: training loss: 1112.156738, consuming time:61.1479 s\n",
      "[epoch 4504]: training loss: 1034.262451, consuming time:61.4371 s\n",
      "[epoch 4505]: training loss: 852.721313, consuming time:61.4619 s\n",
      "[epoch 4506]: training loss: 1006.398315, consuming time:61.0790 s\n",
      "[epoch 4507]: training loss: 788.183044, consuming time:61.1323 s\n",
      "[epoch 4508]: training loss: 1103.271729, consuming time:61.4879 s\n",
      "[epoch 4509]: training loss: 997.350708, consuming time:61.2348 s\n",
      "[epoch 4510]: training loss: 1039.637207, consuming time:61.1507 s\n",
      "[epoch 4511]: training loss: 872.589111, consuming time:61.1779 s\n",
      "[epoch 4512]: training loss: 981.063293, consuming time:61.2162 s\n",
      "[epoch 4513]: training loss: 664.562256, consuming time:60.9804 s\n",
      "[epoch 4514]: training loss: 1071.119873, consuming time:61.1749 s\n",
      "[epoch 4515]: training loss: 958.742859, consuming time:61.5534 s\n",
      "[epoch 4516]: training loss: 881.082275, consuming time:61.5959 s\n",
      "[epoch 4517]: training loss: 773.620605, consuming time:61.4130 s\n",
      "[epoch 4518]: training loss: 956.439636, consuming time:66.9085 s\n",
      "[epoch 4519]: training loss: 964.113342, consuming time:71.3652 s\n",
      "[epoch 4520]: training loss: 1066.458252, consuming time:69.7573 s\n",
      "[epoch 4521]: training loss: 1085.972046, consuming time:68.7352 s\n",
      "[epoch 4522]: training loss: 1040.463379, consuming time:68.5944 s\n",
      "[epoch 4523]: training loss: 1064.099121, consuming time:67.6053 s\n",
      "[epoch 4524]: training loss: 934.471558, consuming time:68.5036 s\n",
      "[epoch 4525]: training loss: 870.432068, consuming time:68.1644 s\n",
      "[epoch 4526]: training loss: 1154.061523, consuming time:69.2843 s\n",
      "[epoch 4527]: training loss: 813.763794, consuming time:69.3324 s\n",
      "[epoch 4528]: training loss: 829.647339, consuming time:68.7320 s\n",
      "[epoch 4529]: training loss: 785.650024, consuming time:68.2882 s\n",
      "[epoch 4530]: training loss: 852.921936, consuming time:68.3712 s\n",
      "[epoch 4531]: training loss: 837.899414, consuming time:81.5589 s\n",
      "[epoch 4532]: training loss: 863.401855, consuming time:81.1595 s\n",
      "[epoch 4533]: training loss: 1007.747314, consuming time:70.5023 s\n",
      "[epoch 4534]: training loss: 880.403870, consuming time:125.9030 s\n",
      "[epoch 4535]: training loss: 1066.932861, consuming time:139.5176 s\n",
      "[epoch 4536]: training loss: 782.604309, consuming time:132.8093 s\n",
      "[epoch 4537]: training loss: 1018.582153, consuming time:137.7954 s\n",
      "[epoch 4538]: training loss: 871.759399, consuming time:132.2168 s\n",
      "[epoch 4539]: training loss: 924.325500, consuming time:134.7511 s\n",
      "[epoch 4540]: training loss: 976.566772, consuming time:136.8346 s\n",
      "[epoch 4541]: training loss: 1217.171753, consuming time:133.9178 s\n",
      "[epoch 4542]: training loss: 1129.800293, consuming time:131.3760 s\n",
      "[epoch 4543]: training loss: 849.024292, consuming time:128.5142 s\n",
      "[epoch 4544]: training loss: 1190.948242, consuming time:124.8683 s\n",
      "[epoch 4545]: training loss: 869.111328, consuming time:99.8329 s\n",
      "[epoch 4546]: training loss: 1042.264893, consuming time:74.5193 s\n",
      "[epoch 4547]: training loss: 764.882568, consuming time:74.6645 s\n",
      "[epoch 4548]: training loss: 805.175964, consuming time:71.1145 s\n",
      "[epoch 4549]: training loss: 743.446655, consuming time:120.4075 s\n",
      "[epoch 4550]: training loss: 948.500671, consuming time:141.2951 s\n",
      "[epoch 4551]: training loss: 926.561218, consuming time:143.6336 s\n",
      "[epoch 4552]: training loss: 952.183838, consuming time:146.5749 s\n",
      "[epoch 4553]: training loss: 980.968506, consuming time:147.1802 s\n",
      "[epoch 4554]: training loss: 1159.545532, consuming time:145.8088 s\n",
      "[epoch 4555]: training loss: 889.047729, consuming time:142.5709 s\n",
      "[epoch 4556]: training loss: 1062.147949, consuming time:144.6695 s\n",
      "[epoch 4557]: training loss: 1021.031372, consuming time:106.0892 s\n",
      "[epoch 4558]: training loss: 817.699524, consuming time:75.4264 s\n",
      "[epoch 4559]: training loss: 885.523682, consuming time:100.1802 s\n",
      "[epoch 4560]: training loss: 892.811035, consuming time:157.5906 s\n",
      "[epoch 4561]: training loss: 822.542786, consuming time:155.9027 s\n",
      "[epoch 4562]: training loss: 815.898376, consuming time:156.9445 s\n",
      "[epoch 4563]: training loss: 1029.451904, consuming time:159.8117 s\n",
      "[epoch 4564]: training loss: 883.942932, consuming time:165.2331 s\n",
      "[epoch 4565]: training loss: 1146.389282, consuming time:153.2740 s\n",
      "[epoch 4566]: training loss: 953.404358, consuming time:162.0553 s\n",
      "[epoch 4567]: training loss: 1007.946899, consuming time:162.6786 s\n",
      "[epoch 4568]: training loss: 1062.502441, consuming time:164.3661 s\n",
      "[epoch 4569]: training loss: 918.315979, consuming time:85.5138 s\n",
      "[epoch 4570]: training loss: 974.040466, consuming time:74.1735 s\n",
      "[epoch 4571]: training loss: 1217.523315, consuming time:70.6107 s\n",
      "[epoch 4572]: training loss: 1052.132935, consuming time:145.4433 s\n",
      "[epoch 4573]: training loss: 966.447937, consuming time:142.0701 s\n",
      "[epoch 4574]: training loss: 796.899414, consuming time:151.5573 s\n",
      "[epoch 4575]: training loss: 747.957581, consuming time:142.4512 s\n",
      "[epoch 4576]: training loss: 734.403625, consuming time:136.9802 s\n",
      "[epoch 4577]: training loss: 874.249146, consuming time:147.0811 s\n",
      "[epoch 4578]: training loss: 829.728027, consuming time:122.0939 s\n",
      "[epoch 4579]: training loss: 1023.821655, consuming time:75.1543 s\n",
      "[epoch 4580]: training loss: 1115.670166, consuming time:106.1665 s\n",
      "[epoch 4581]: training loss: 1038.392944, consuming time:135.9062 s\n",
      "[epoch 4582]: training loss: 1257.945679, consuming time:134.9445 s\n",
      "[epoch 4583]: training loss: 1057.257812, consuming time:136.0687 s\n",
      "[epoch 4584]: training loss: 884.124146, consuming time:138.6588 s\n",
      "[epoch 4585]: training loss: 1067.901123, consuming time:137.8969 s\n",
      "[epoch 4586]: training loss: 1093.390137, consuming time:140.7485 s\n",
      "[epoch 4587]: training loss: 946.585083, consuming time:151.9732 s\n",
      "[epoch 4588]: training loss: 817.323059, consuming time:155.2904 s\n",
      "[epoch 4589]: training loss: 1007.728638, consuming time:97.0027 s\n",
      "[epoch 4590]: training loss: 730.311707, consuming time:63.8221 s\n",
      "[epoch 4591]: training loss: 947.497437, consuming time:63.9921 s\n",
      "[epoch 4592]: training loss: 742.379395, consuming time:63.5059 s\n",
      "[epoch 4593]: training loss: 790.173584, consuming time:63.5618 s\n",
      "[epoch 4594]: training loss: 870.044678, consuming time:64.1877 s\n",
      "[epoch 4595]: training loss: 970.333740, consuming time:62.0977 s\n",
      "[epoch 4596]: training loss: 882.832886, consuming time:65.0884 s\n",
      "[epoch 4597]: training loss: 933.958740, consuming time:65.1030 s\n",
      "[epoch 4598]: training loss: 968.620239, consuming time:64.8709 s\n",
      "[epoch 4599]: training loss: 850.979004, consuming time:62.0260 s\n",
      "[epoch 4600]: training loss: 691.352417, consuming time:61.8947 s\n",
      "[epoch 4601]: training loss: 1118.868042, consuming time:61.8798 s\n",
      "[epoch 4602]: training loss: 791.513306, consuming time:62.4519 s\n",
      "[epoch 4603]: training loss: 935.616150, consuming time:61.7895 s\n",
      "[epoch 4604]: training loss: 946.596375, consuming time:60.4648 s\n",
      "[epoch 4605]: training loss: 1167.935425, consuming time:61.3741 s\n",
      "[epoch 4606]: training loss: 828.750610, consuming time:62.0024 s\n",
      "[epoch 4607]: training loss: 788.192688, consuming time:65.1019 s\n",
      "[epoch 4608]: training loss: 877.212158, consuming time:61.9217 s\n",
      "[epoch 4609]: training loss: 883.423950, consuming time:62.9233 s\n",
      "[epoch 4610]: training loss: 1073.319946, consuming time:60.1507 s\n",
      "[epoch 4611]: training loss: 1120.897217, consuming time:61.1452 s\n",
      "[epoch 4612]: training loss: 1291.432129, consuming time:67.7645 s\n",
      "[epoch 4613]: training loss: 1178.255493, consuming time:63.0098 s\n",
      "[epoch 4614]: training loss: 710.655579, consuming time:63.6634 s\n",
      "[epoch 4615]: training loss: 856.343933, consuming time:64.3025 s\n",
      "[epoch 4616]: training loss: 851.259460, consuming time:63.4849 s\n",
      "[epoch 4617]: training loss: 1008.255310, consuming time:62.7123 s\n",
      "[epoch 4618]: training loss: 1443.795532, consuming time:62.8169 s\n",
      "[epoch 4619]: training loss: 853.796387, consuming time:66.4783 s\n",
      "[epoch 4620]: training loss: 1060.315430, consuming time:64.9733 s\n",
      "[epoch 4621]: training loss: 1057.662476, consuming time:63.5034 s\n",
      "[epoch 4622]: training loss: 955.610474, consuming time:62.6981 s\n",
      "[epoch 4623]: training loss: 1003.803040, consuming time:65.1022 s\n",
      "[epoch 4624]: training loss: 1159.859741, consuming time:64.6899 s\n",
      "[epoch 4625]: training loss: 707.962646, consuming time:63.1375 s\n",
      "[epoch 4626]: training loss: 1048.824463, consuming time:61.3957 s\n",
      "[epoch 4627]: training loss: 894.163208, consuming time:61.3392 s\n",
      "[epoch 4628]: training loss: 1106.453369, consuming time:61.3901 s\n",
      "[epoch 4629]: training loss: 1102.609741, consuming time:61.0940 s\n",
      "[epoch 4630]: training loss: 1232.919800, consuming time:60.0088 s\n",
      "[epoch 4631]: training loss: 849.512085, consuming time:60.4641 s\n",
      "[epoch 4632]: training loss: 740.471680, consuming time:60.4067 s\n",
      "[epoch 4633]: training loss: 863.794617, consuming time:60.3977 s\n",
      "[epoch 4634]: training loss: 727.417603, consuming time:60.7000 s\n",
      "[epoch 4635]: training loss: 814.266968, consuming time:61.1041 s\n",
      "[epoch 4636]: training loss: 890.405212, consuming time:61.2125 s\n",
      "[epoch 4637]: training loss: 1011.967041, consuming time:61.0070 s\n",
      "[epoch 4638]: training loss: 1175.974609, consuming time:60.8303 s\n",
      "[epoch 4639]: training loss: 811.273804, consuming time:60.5677 s\n",
      "[epoch 4640]: training loss: 1075.069824, consuming time:60.6609 s\n",
      "[epoch 4641]: training loss: 710.535522, consuming time:60.7642 s\n",
      "[epoch 4642]: training loss: 869.537109, consuming time:60.4392 s\n",
      "[epoch 4643]: training loss: 1067.792480, consuming time:61.0036 s\n",
      "[epoch 4644]: training loss: 612.554810, consuming time:60.9112 s\n",
      "[epoch 4645]: training loss: 1083.044189, consuming time:60.7822 s\n",
      "[epoch 4646]: training loss: 1188.555176, consuming time:60.9700 s\n",
      "[epoch 4647]: training loss: 1056.868774, consuming time:61.2589 s\n",
      "[epoch 4648]: training loss: 1171.208374, consuming time:60.8167 s\n",
      "[epoch 4649]: training loss: 728.227051, consuming time:61.1974 s\n",
      "[epoch 4650]: training loss: 1042.619141, consuming time:60.9686 s\n",
      "[epoch 4651]: training loss: 1041.279297, consuming time:61.2100 s\n",
      "[epoch 4652]: training loss: 900.659241, consuming time:61.2437 s\n",
      "[epoch 4653]: training loss: 1024.348145, consuming time:61.8826 s\n",
      "[epoch 4654]: training loss: 891.015381, consuming time:62.7818 s\n",
      "[epoch 4655]: training loss: 1164.316162, consuming time:62.0120 s\n",
      "[epoch 4656]: training loss: 1045.823730, consuming time:60.6237 s\n",
      "[epoch 4657]: training loss: 1021.593384, consuming time:61.5116 s\n",
      "[epoch 4658]: training loss: 661.169678, consuming time:61.1452 s\n",
      "[epoch 4659]: training loss: 1105.646729, consuming time:61.5154 s\n",
      "[epoch 4660]: training loss: 823.961243, consuming time:60.9204 s\n",
      "[epoch 4661]: training loss: 1128.311279, consuming time:61.2583 s\n",
      "[epoch 4662]: training loss: 855.604858, consuming time:61.2826 s\n",
      "[epoch 4663]: training loss: 1033.289917, consuming time:61.1166 s\n",
      "[epoch 4664]: training loss: 951.173279, consuming time:61.2497 s\n",
      "[epoch 4665]: training loss: 796.773804, consuming time:61.0490 s\n",
      "[epoch 4666]: training loss: 1073.840088, consuming time:60.9616 s\n",
      "[epoch 4667]: training loss: 909.502869, consuming time:60.8364 s\n",
      "[epoch 4668]: training loss: 1129.336304, consuming time:60.9352 s\n",
      "[epoch 4669]: training loss: 853.287964, consuming time:60.9960 s\n",
      "[epoch 4670]: training loss: 1155.109619, consuming time:61.0969 s\n",
      "[epoch 4671]: training loss: 942.524170, consuming time:60.9738 s\n",
      "[epoch 4672]: training loss: 841.126831, consuming time:60.9101 s\n",
      "[epoch 4673]: training loss: 1014.425842, consuming time:61.2400 s\n",
      "[epoch 4674]: training loss: 769.709229, consuming time:61.0631 s\n",
      "[epoch 4675]: training loss: 1095.601807, consuming time:61.1570 s\n",
      "[epoch 4676]: training loss: 1010.486023, consuming time:61.0390 s\n",
      "[epoch 4677]: training loss: 1050.823608, consuming time:61.2007 s\n",
      "[epoch 4678]: training loss: 835.438049, consuming time:61.4283 s\n",
      "[epoch 4679]: training loss: 761.172852, consuming time:61.5202 s\n",
      "[epoch 4680]: training loss: 930.884033, consuming time:61.3295 s\n",
      "[epoch 4681]: training loss: 1018.312744, consuming time:64.2854 s\n",
      "[epoch 4682]: training loss: 748.912903, consuming time:65.1392 s\n",
      "[epoch 4683]: training loss: 951.439148, consuming time:64.9048 s\n",
      "[epoch 4684]: training loss: 799.065186, consuming time:61.5405 s\n",
      "[epoch 4685]: training loss: 812.054565, consuming time:61.8795 s\n",
      "[epoch 4686]: training loss: 924.358032, consuming time:61.5480 s\n",
      "[epoch 4687]: training loss: 1018.728882, consuming time:60.9375 s\n",
      "[epoch 4688]: training loss: 882.808105, consuming time:60.9511 s\n",
      "[epoch 4689]: training loss: 853.996765, consuming time:60.8030 s\n",
      "[epoch 4690]: training loss: 684.831299, consuming time:60.8255 s\n",
      "[epoch 4691]: training loss: 912.833252, consuming time:61.0494 s\n",
      "[epoch 4692]: training loss: 1191.556885, consuming time:61.0482 s\n",
      "[epoch 4693]: training loss: 857.292725, consuming time:60.4646 s\n",
      "[epoch 4694]: training loss: 1019.857483, consuming time:60.8194 s\n",
      "[epoch 4695]: training loss: 878.828735, consuming time:60.7325 s\n",
      "[epoch 4696]: training loss: 921.439636, consuming time:61.2140 s\n",
      "[epoch 4697]: training loss: 910.657349, consuming time:60.9000 s\n",
      "[epoch 4698]: training loss: 1363.261597, consuming time:61.0276 s\n",
      "[epoch 4699]: training loss: 932.739990, consuming time:60.7404 s\n",
      "[epoch 4700]: training loss: 691.943237, consuming time:60.5929 s\n",
      "[epoch 4701]: training loss: 817.420166, consuming time:60.6642 s\n",
      "[epoch 4702]: training loss: 968.478882, consuming time:60.9524 s\n",
      "[epoch 4703]: training loss: 826.432068, consuming time:60.8910 s\n",
      "[epoch 4704]: training loss: 939.946472, consuming time:61.0823 s\n",
      "[epoch 4705]: training loss: 961.970520, consuming time:60.9025 s\n",
      "[epoch 4706]: training loss: 810.842346, consuming time:60.7540 s\n",
      "[epoch 4707]: training loss: 957.399414, consuming time:60.9792 s\n",
      "[epoch 4708]: training loss: 1188.288086, consuming time:60.8955 s\n",
      "[epoch 4709]: training loss: 1296.278809, consuming time:61.0683 s\n",
      "[epoch 4710]: training loss: 864.837952, consuming time:60.8264 s\n",
      "[epoch 4711]: training loss: 717.247314, consuming time:60.8685 s\n",
      "[epoch 4712]: training loss: 992.311951, consuming time:60.6038 s\n",
      "[epoch 4713]: training loss: 922.268066, consuming time:60.9101 s\n",
      "[epoch 4714]: training loss: 909.406677, consuming time:61.3103 s\n",
      "[epoch 4715]: training loss: 993.760437, consuming time:61.0182 s\n",
      "[epoch 4716]: training loss: 904.013916, consuming time:61.0164 s\n",
      "[epoch 4717]: training loss: 833.439575, consuming time:60.9180 s\n",
      "[epoch 4718]: training loss: 874.554871, consuming time:60.7842 s\n",
      "[epoch 4719]: training loss: 1195.340332, consuming time:60.8208 s\n",
      "[epoch 4720]: training loss: 747.577820, consuming time:60.8976 s\n",
      "[epoch 4721]: training loss: 1224.127563, consuming time:61.0135 s\n",
      "[epoch 4722]: training loss: 775.747559, consuming time:60.9641 s\n",
      "[epoch 4723]: training loss: 957.335022, consuming time:60.6000 s\n",
      "[epoch 4724]: training loss: 993.924683, consuming time:60.8839 s\n",
      "[epoch 4725]: training loss: 996.474182, consuming time:60.9460 s\n",
      "[epoch 4726]: training loss: 886.955811, consuming time:61.0706 s\n",
      "[epoch 4727]: training loss: 966.793030, consuming time:60.6378 s\n",
      "[epoch 4728]: training loss: 941.832214, consuming time:60.7684 s\n",
      "[epoch 4729]: training loss: 737.901123, consuming time:60.6334 s\n",
      "[epoch 4730]: training loss: 650.871216, consuming time:60.9357 s\n",
      "[epoch 4731]: training loss: 967.856812, consuming time:61.1193 s\n",
      "[epoch 4732]: training loss: 1036.332275, consuming time:61.2115 s\n",
      "[epoch 4733]: training loss: 1051.244385, consuming time:60.4693 s\n",
      "[epoch 4734]: training loss: 916.193481, consuming time:61.0254 s\n",
      "[epoch 4735]: training loss: 898.839722, consuming time:60.9025 s\n",
      "[epoch 4736]: training loss: 852.812500, consuming time:60.8805 s\n",
      "[epoch 4737]: training loss: 1059.909180, consuming time:61.1674 s\n",
      "[epoch 4738]: training loss: 1181.694092, consuming time:61.0093 s\n",
      "[epoch 4739]: training loss: 1078.685791, consuming time:60.7521 s\n",
      "[epoch 4740]: training loss: 956.630615, consuming time:60.9185 s\n",
      "[epoch 4741]: training loss: 1088.634888, consuming time:60.8766 s\n",
      "[epoch 4742]: training loss: 781.214355, consuming time:60.9128 s\n",
      "[epoch 4743]: training loss: 947.366089, consuming time:61.3945 s\n",
      "[epoch 4744]: training loss: 929.520142, consuming time:60.9707 s\n",
      "[epoch 4745]: training loss: 1087.921143, consuming time:60.9691 s\n",
      "[epoch 4746]: training loss: 1020.956238, consuming time:60.8531 s\n",
      "[epoch 4747]: training loss: 1050.983398, consuming time:61.1475 s\n",
      "[epoch 4748]: training loss: 988.321167, consuming time:60.9568 s\n",
      "[epoch 4749]: training loss: 1121.735596, consuming time:60.8097 s\n",
      "[epoch 4750]: training loss: 1178.381226, consuming time:61.2616 s\n",
      "[epoch 4751]: training loss: 1056.591797, consuming time:61.0773 s\n",
      "[epoch 4752]: training loss: 655.595093, consuming time:60.8900 s\n",
      "[epoch 4753]: training loss: 957.508301, consuming time:60.9974 s\n",
      "[epoch 4754]: training loss: 954.335083, consuming time:60.9089 s\n",
      "[epoch 4755]: training loss: 742.621948, consuming time:60.9602 s\n",
      "[epoch 4756]: training loss: 757.768433, consuming time:60.9444 s\n",
      "[epoch 4757]: training loss: 918.273560, consuming time:60.8622 s\n",
      "[epoch 4758]: training loss: 1178.185547, consuming time:60.8280 s\n",
      "[epoch 4759]: training loss: 777.335938, consuming time:60.9709 s\n",
      "[epoch 4760]: training loss: 1150.207886, consuming time:61.2833 s\n",
      "[epoch 4761]: training loss: 934.962463, consuming time:61.1689 s\n",
      "[epoch 4762]: training loss: 903.336060, consuming time:60.8145 s\n",
      "[epoch 4763]: training loss: 1361.171387, consuming time:60.9927 s\n",
      "[epoch 4764]: training loss: 1054.623291, consuming time:60.8659 s\n",
      "[epoch 4765]: training loss: 983.675354, consuming time:60.9848 s\n",
      "[epoch 4766]: training loss: 943.654663, consuming time:60.9255 s\n",
      "[epoch 4767]: training loss: 819.757690, consuming time:60.8616 s\n",
      "[epoch 4768]: training loss: 658.058594, consuming time:61.0039 s\n",
      "[epoch 4769]: training loss: 1070.610229, consuming time:61.0432 s\n",
      "[epoch 4770]: training loss: 1107.891846, consuming time:61.0079 s\n",
      "[epoch 4771]: training loss: 802.865356, consuming time:60.9970 s\n",
      "[epoch 4772]: training loss: 1113.426270, consuming time:60.7391 s\n",
      "[epoch 4773]: training loss: 1165.357178, consuming time:60.8621 s\n",
      "[epoch 4774]: training loss: 704.334839, consuming time:60.8039 s\n",
      "[epoch 4775]: training loss: 1051.356445, consuming time:60.9029 s\n",
      "[epoch 4776]: training loss: 1231.185669, consuming time:60.9369 s\n",
      "[epoch 4777]: training loss: 983.921387, consuming time:61.1077 s\n",
      "[epoch 4778]: training loss: 1230.484741, consuming time:60.8257 s\n",
      "[epoch 4779]: training loss: 801.921997, consuming time:60.8650 s\n",
      "[epoch 4780]: training loss: 1055.143677, consuming time:61.0224 s\n",
      "[epoch 4781]: training loss: 887.790283, consuming time:60.9345 s\n",
      "[epoch 4782]: training loss: 1449.038574, consuming time:61.0380 s\n",
      "[epoch 4783]: training loss: 774.966919, consuming time:60.8376 s\n",
      "[epoch 4784]: training loss: 1106.539795, consuming time:60.9054 s\n",
      "[epoch 4785]: training loss: 1008.916748, consuming time:60.6510 s\n",
      "[epoch 4786]: training loss: 841.412964, consuming time:60.9870 s\n",
      "[epoch 4787]: training loss: 1046.034790, consuming time:61.0559 s\n",
      "[epoch 4788]: training loss: 1107.069824, consuming time:60.8459 s\n",
      "[epoch 4789]: training loss: 919.983887, consuming time:61.0573 s\n",
      "[epoch 4790]: training loss: 941.290405, consuming time:60.9797 s\n",
      "[epoch 4791]: training loss: 851.818115, consuming time:60.9220 s\n",
      "[epoch 4792]: training loss: 803.092163, consuming time:61.0218 s\n",
      "[epoch 4793]: training loss: 816.376953, consuming time:61.1088 s\n",
      "[epoch 4794]: training loss: 681.479004, consuming time:61.1596 s\n",
      "[epoch 4795]: training loss: 1461.575439, consuming time:61.0291 s\n",
      "[epoch 4796]: training loss: 1013.563660, consuming time:61.1418 s\n",
      "[epoch 4797]: training loss: 1085.144043, consuming time:60.4940 s\n",
      "[epoch 4798]: training loss: 918.234802, consuming time:61.0911 s\n",
      "[epoch 4799]: training loss: 962.494324, consuming time:61.2483 s\n",
      "[epoch 4800]: training loss: 891.505920, consuming time:61.0279 s\n",
      "[epoch 4801]: training loss: 876.250977, consuming time:60.9561 s\n",
      "[epoch 4802]: training loss: 993.518921, consuming time:60.8314 s\n",
      "[epoch 4803]: training loss: 812.440491, consuming time:60.5998 s\n",
      "[epoch 4804]: training loss: 1072.989746, consuming time:60.7189 s\n",
      "[epoch 4805]: training loss: 1337.459351, consuming time:60.8394 s\n",
      "[epoch 4806]: training loss: 960.452271, consuming time:60.7801 s\n",
      "[epoch 4807]: training loss: 1114.865723, consuming time:60.6851 s\n",
      "[epoch 4808]: training loss: 1158.838867, consuming time:60.8385 s\n",
      "[epoch 4809]: training loss: 995.987549, consuming time:60.9318 s\n",
      "[epoch 4810]: training loss: 886.789307, consuming time:60.9092 s\n",
      "[epoch 4811]: training loss: 762.570374, consuming time:61.0112 s\n",
      "[epoch 4812]: training loss: 867.692993, consuming time:60.8827 s\n",
      "[epoch 4813]: training loss: 986.537354, consuming time:60.3849 s\n",
      "[epoch 4814]: training loss: 1054.139404, consuming time:60.8276 s\n",
      "[epoch 4815]: training loss: 1033.953369, consuming time:60.5827 s\n",
      "[epoch 4816]: training loss: 962.833740, consuming time:61.1572 s\n",
      "[epoch 4817]: training loss: 961.514954, consuming time:61.1447 s\n",
      "[epoch 4818]: training loss: 1050.320190, consuming time:60.7489 s\n",
      "[epoch 4819]: training loss: 1097.033081, consuming time:60.6362 s\n",
      "[epoch 4820]: training loss: 858.433655, consuming time:60.7295 s\n",
      "[epoch 4821]: training loss: 1081.547485, consuming time:60.6856 s\n",
      "[epoch 4822]: training loss: 724.927246, consuming time:61.0966 s\n",
      "[epoch 4823]: training loss: 1059.515259, consuming time:60.8629 s\n",
      "[epoch 4824]: training loss: 978.949585, consuming time:60.7727 s\n",
      "[epoch 4825]: training loss: 963.120483, consuming time:60.6282 s\n",
      "[epoch 4826]: training loss: 946.081665, consuming time:60.7522 s\n",
      "[epoch 4827]: training loss: 1057.258545, consuming time:60.8660 s\n",
      "[epoch 4828]: training loss: 993.884521, consuming time:60.9038 s\n",
      "[epoch 4829]: training loss: 886.167847, consuming time:60.9401 s\n",
      "[epoch 4830]: training loss: 1174.924316, consuming time:60.8200 s\n",
      "[epoch 4831]: training loss: 1197.003906, consuming time:60.7761 s\n",
      "[epoch 4832]: training loss: 915.579346, consuming time:60.9702 s\n",
      "[epoch 4833]: training loss: 873.552124, consuming time:61.1270 s\n",
      "[epoch 4834]: training loss: 998.573853, consuming time:61.1526 s\n",
      "[epoch 4835]: training loss: 784.139099, consuming time:60.9453 s\n",
      "[epoch 4836]: training loss: 965.901123, consuming time:60.6781 s\n",
      "[epoch 4837]: training loss: 1164.228027, consuming time:60.6745 s\n",
      "[epoch 4838]: training loss: 878.263733, consuming time:60.7343 s\n",
      "[epoch 4839]: training loss: 949.765259, consuming time:60.9771 s\n",
      "[epoch 4840]: training loss: 1132.312500, consuming time:61.0841 s\n",
      "[epoch 4841]: training loss: 981.749390, consuming time:60.9548 s\n",
      "[epoch 4842]: training loss: 897.492920, consuming time:60.5359 s\n",
      "[epoch 4843]: training loss: 1012.770020, consuming time:60.6430 s\n",
      "[epoch 4844]: training loss: 541.258728, consuming time:61.0276 s\n",
      "[epoch 4845]: training loss: 980.502197, consuming time:61.0453 s\n",
      "[epoch 4846]: training loss: 914.016357, consuming time:61.1023 s\n",
      "[epoch 4847]: training loss: 972.755066, consuming time:60.8439 s\n",
      "[epoch 4848]: training loss: 933.036133, consuming time:60.8477 s\n",
      "[epoch 4849]: training loss: 916.359375, consuming time:60.9788 s\n",
      "[epoch 4850]: training loss: 1038.620850, consuming time:61.0039 s\n",
      "[epoch 4851]: training loss: 1040.388672, consuming time:60.9045 s\n",
      "[epoch 4852]: training loss: 893.857300, consuming time:60.9897 s\n",
      "[epoch 4853]: training loss: 871.878540, consuming time:60.7405 s\n",
      "[epoch 4854]: training loss: 905.928406, consuming time:60.7090 s\n",
      "[epoch 4855]: training loss: 938.187866, consuming time:60.7509 s\n",
      "[epoch 4856]: training loss: 984.376404, consuming time:60.8588 s\n",
      "[epoch 4857]: training loss: 691.030701, consuming time:61.1621 s\n",
      "[epoch 4858]: training loss: 939.207947, consuming time:61.0249 s\n",
      "[epoch 4859]: training loss: 909.815430, consuming time:60.8219 s\n",
      "[epoch 4860]: training loss: 879.096313, consuming time:60.7672 s\n",
      "[epoch 4861]: training loss: 989.715332, consuming time:60.9541 s\n",
      "[epoch 4862]: training loss: 1006.473389, consuming time:61.1838 s\n",
      "[epoch 4863]: training loss: 866.773743, consuming time:60.6792 s\n",
      "[epoch 4864]: training loss: 920.403564, consuming time:61.2998 s\n",
      "[epoch 4865]: training loss: 997.345398, consuming time:60.7552 s\n",
      "[epoch 4866]: training loss: 1057.511230, consuming time:60.6908 s\n",
      "[epoch 4867]: training loss: 1030.263794, consuming time:60.7802 s\n",
      "[epoch 4868]: training loss: 1061.621826, consuming time:60.8039 s\n",
      "[epoch 4869]: training loss: 905.407349, consuming time:61.1741 s\n",
      "[epoch 4870]: training loss: 765.691101, consuming time:61.1167 s\n",
      "[epoch 4871]: training loss: 1276.368896, consuming time:60.7058 s\n",
      "[epoch 4872]: training loss: 1150.109009, consuming time:60.3222 s\n",
      "[epoch 4873]: training loss: 1008.811157, consuming time:60.6962 s\n",
      "[epoch 4874]: training loss: 860.289612, consuming time:60.9032 s\n",
      "[epoch 4875]: training loss: 766.035278, consuming time:60.9709 s\n",
      "[epoch 4876]: training loss: 746.733826, consuming time:60.8136 s\n",
      "[epoch 4877]: training loss: 1125.978149, consuming time:60.9848 s\n",
      "[epoch 4878]: training loss: 1083.815674, consuming time:60.3997 s\n",
      "[epoch 4879]: training loss: 852.039062, consuming time:60.8869 s\n",
      "[epoch 4880]: training loss: 1029.454102, consuming time:60.9241 s\n",
      "[epoch 4881]: training loss: 1241.817505, consuming time:60.7433 s\n",
      "[epoch 4882]: training loss: 1005.304626, consuming time:60.5792 s\n",
      "[epoch 4883]: training loss: 1005.970032, consuming time:60.9017 s\n",
      "[epoch 4884]: training loss: 994.312988, consuming time:60.4768 s\n",
      "[epoch 4885]: training loss: 712.131470, consuming time:61.0218 s\n",
      "[epoch 4886]: training loss: 1070.842529, consuming time:60.7751 s\n",
      "[epoch 4887]: training loss: 872.992859, consuming time:61.1038 s\n",
      "[epoch 4888]: training loss: 1179.416992, consuming time:60.7897 s\n",
      "[epoch 4889]: training loss: 863.729004, consuming time:60.5983 s\n",
      "[epoch 4890]: training loss: 936.976196, consuming time:60.6609 s\n",
      "[epoch 4891]: training loss: 768.745483, consuming time:60.9674 s\n",
      "[epoch 4892]: training loss: 1142.710327, consuming time:61.2909 s\n",
      "[epoch 4893]: training loss: 692.171997, consuming time:60.6732 s\n",
      "[epoch 4894]: training loss: 1091.880005, consuming time:60.8676 s\n",
      "[epoch 4895]: training loss: 1093.076294, consuming time:60.5685 s\n",
      "[epoch 4896]: training loss: 914.917908, consuming time:60.7799 s\n",
      "[epoch 4897]: training loss: 835.308350, consuming time:61.0208 s\n",
      "[epoch 4898]: training loss: 996.201965, consuming time:61.0817 s\n",
      "[epoch 4899]: training loss: 1053.941284, consuming time:60.8089 s\n",
      "[epoch 4900]: training loss: 888.776489, consuming time:60.4659 s\n",
      "[epoch 4901]: training loss: 1057.679443, consuming time:60.5733 s\n",
      "[epoch 4902]: training loss: 1222.814453, consuming time:60.8546 s\n",
      "[epoch 4903]: training loss: 937.897095, consuming time:61.1282 s\n",
      "[epoch 4904]: training loss: 1142.333252, consuming time:61.1490 s\n",
      "[epoch 4905]: training loss: 1228.406738, consuming time:60.8399 s\n",
      "[epoch 4906]: training loss: 658.014343, consuming time:61.1665 s\n",
      "[epoch 4907]: training loss: 1047.571533, consuming time:60.7054 s\n",
      "[epoch 4908]: training loss: 807.893616, consuming time:60.8281 s\n",
      "[epoch 4909]: training loss: 969.003906, consuming time:60.8916 s\n",
      "[epoch 4910]: training loss: 1339.698242, consuming time:60.8200 s\n",
      "[epoch 4911]: training loss: 1179.971680, consuming time:61.0631 s\n",
      "[epoch 4912]: training loss: 1136.805664, consuming time:60.7840 s\n",
      "[epoch 4913]: training loss: 891.637329, consuming time:60.6440 s\n",
      "[epoch 4914]: training loss: 1240.134277, consuming time:60.9286 s\n",
      "[epoch 4915]: training loss: 990.322998, consuming time:60.9557 s\n",
      "[epoch 4916]: training loss: 618.514038, consuming time:61.0647 s\n",
      "[epoch 4917]: training loss: 854.229736, consuming time:60.8000 s\n",
      "[epoch 4918]: training loss: 972.475647, consuming time:60.5283 s\n",
      "[epoch 4919]: training loss: 933.311890, consuming time:61.0588 s\n",
      "[epoch 4920]: training loss: 844.621948, consuming time:60.8458 s\n",
      "[epoch 4921]: training loss: 1299.997559, consuming time:61.2411 s\n",
      "[epoch 4922]: training loss: 819.373413, consuming time:61.0801 s\n",
      "[epoch 4923]: training loss: 764.722046, consuming time:60.8100 s\n",
      "[epoch 4924]: training loss: 839.239380, consuming time:60.9381 s\n",
      "[epoch 4925]: training loss: 1059.144287, consuming time:60.9074 s\n",
      "[epoch 4926]: training loss: 921.810791, consuming time:61.1974 s\n",
      "[epoch 4927]: training loss: 911.368774, consuming time:60.9782 s\n",
      "[epoch 4928]: training loss: 819.956299, consuming time:60.9889 s\n",
      "[epoch 4929]: training loss: 1126.123535, consuming time:60.8261 s\n",
      "[epoch 4930]: training loss: 1078.586426, consuming time:60.7430 s\n",
      "[epoch 4931]: training loss: 1030.351807, consuming time:60.8400 s\n",
      "[epoch 4932]: training loss: 994.786255, consuming time:60.7111 s\n",
      "[epoch 4933]: training loss: 752.516602, consuming time:61.1933 s\n",
      "[epoch 4934]: training loss: 1081.772949, consuming time:61.2166 s\n",
      "[epoch 4935]: training loss: 1149.738037, consuming time:60.8102 s\n",
      "[epoch 4936]: training loss: 735.767273, consuming time:61.0878 s\n",
      "[epoch 4937]: training loss: 1104.652710, consuming time:60.9048 s\n",
      "[epoch 4938]: training loss: 975.081848, consuming time:61.0065 s\n",
      "[epoch 4939]: training loss: 997.553589, consuming time:61.2119 s\n",
      "[epoch 4940]: training loss: 1062.510986, consuming time:60.9828 s\n",
      "[epoch 4941]: training loss: 855.647827, consuming time:60.7380 s\n",
      "[epoch 4942]: training loss: 1021.235229, consuming time:60.7778 s\n",
      "[epoch 4943]: training loss: 876.333130, consuming time:60.8953 s\n",
      "[epoch 4944]: training loss: 1093.591553, consuming time:61.0319 s\n",
      "[epoch 4945]: training loss: 1092.432739, consuming time:60.8330 s\n",
      "[epoch 4946]: training loss: 710.553650, consuming time:60.8913 s\n",
      "[epoch 4947]: training loss: 809.725281, consuming time:60.8611 s\n",
      "[epoch 4948]: training loss: 999.645081, consuming time:61.1589 s\n",
      "[epoch 4949]: training loss: 1038.436035, consuming time:60.8072 s\n",
      "[epoch 4950]: training loss: 761.705200, consuming time:60.8326 s\n",
      "[epoch 4951]: training loss: 1052.907104, consuming time:61.0197 s\n",
      "[epoch 4952]: training loss: 1240.256836, consuming time:60.9193 s\n",
      "[epoch 4953]: training loss: 882.037354, consuming time:60.8344 s\n",
      "[epoch 4954]: training loss: 1011.422607, consuming time:61.1237 s\n",
      "[epoch 4955]: training loss: 693.580566, consuming time:61.0212 s\n",
      "[epoch 4956]: training loss: 820.885864, consuming time:60.7995 s\n",
      "[epoch 4957]: training loss: 1045.496460, consuming time:60.6413 s\n",
      "[epoch 4958]: training loss: 890.975159, consuming time:61.1278 s\n",
      "[epoch 4959]: training loss: 1104.442627, consuming time:61.0696 s\n",
      "[epoch 4960]: training loss: 915.410889, consuming time:60.9998 s\n",
      "[epoch 4961]: training loss: 967.357300, consuming time:60.5926 s\n",
      "[epoch 4962]: training loss: 930.693481, consuming time:60.7331 s\n",
      "[epoch 4963]: training loss: 965.392578, consuming time:60.8642 s\n",
      "[epoch 4964]: training loss: 1107.893921, consuming time:60.8558 s\n",
      "[epoch 4965]: training loss: 700.697632, consuming time:60.3894 s\n",
      "[epoch 4966]: training loss: 1089.592041, consuming time:60.9641 s\n",
      "[epoch 4967]: training loss: 737.862244, consuming time:60.6035 s\n",
      "[epoch 4968]: training loss: 742.492188, consuming time:60.8792 s\n",
      "[epoch 4969]: training loss: 772.315002, consuming time:60.7879 s\n",
      "[epoch 4970]: training loss: 1164.173950, consuming time:60.8281 s\n",
      "[epoch 4971]: training loss: 987.509460, consuming time:60.7644 s\n",
      "[epoch 4972]: training loss: 1043.371338, consuming time:60.6894 s\n",
      "[epoch 4973]: training loss: 827.959229, consuming time:60.7791 s\n",
      "[epoch 4974]: training loss: 675.993469, consuming time:60.7543 s\n",
      "[epoch 4975]: training loss: 1008.483154, consuming time:60.6830 s\n",
      "[epoch 4976]: training loss: 1011.073059, consuming time:60.6480 s\n",
      "[epoch 4977]: training loss: 803.502441, consuming time:60.9210 s\n",
      "[epoch 4978]: training loss: 930.850830, consuming time:60.9623 s\n",
      "[epoch 4979]: training loss: 739.375977, consuming time:65.7630 s\n",
      "[epoch 4980]: training loss: 981.041016, consuming time:60.9479 s\n",
      "[epoch 4981]: training loss: 1000.857239, consuming time:61.3122 s\n",
      "[epoch 4982]: training loss: 958.131714, consuming time:61.0426 s\n",
      "[epoch 4983]: training loss: 874.349854, consuming time:60.7600 s\n",
      "[epoch 4984]: training loss: 727.319214, consuming time:60.7153 s\n",
      "[epoch 4985]: training loss: 685.425171, consuming time:60.8438 s\n",
      "[epoch 4986]: training loss: 970.391357, consuming time:61.0491 s\n",
      "[epoch 4987]: training loss: 1073.992188, consuming time:60.9050 s\n",
      "[epoch 4988]: training loss: 1369.392822, consuming time:61.0174 s\n",
      "[epoch 4989]: training loss: 1070.514648, consuming time:60.7748 s\n",
      "[epoch 4990]: training loss: 1124.131104, consuming time:60.8776 s\n",
      "[epoch 4991]: training loss: 705.482361, consuming time:60.9384 s\n",
      "[epoch 4992]: training loss: 732.495117, consuming time:60.8773 s\n",
      "[epoch 4993]: training loss: 985.980164, consuming time:60.7970 s\n",
      "[epoch 4994]: training loss: 880.794678, consuming time:60.9092 s\n",
      "[epoch 4995]: training loss: 1182.521729, consuming time:60.6784 s\n",
      "[epoch 4996]: training loss: 1373.543701, consuming time:60.5626 s\n",
      "[epoch 4997]: training loss: 894.964294, consuming time:60.8822 s\n",
      "[epoch 4998]: training loss: 744.439941, consuming time:60.8337 s\n",
      "[epoch 4999]: training loss: 690.613770, consuming time:60.7471 s\n",
      "[epoch 5000]: training loss: 773.229492, consuming time:61.0005 s\n",
      "[epoch 5001]: training loss: 1165.785400, consuming time:60.7635 s\n",
      "[epoch 5002]: training loss: 887.216492, consuming time:60.4420 s\n",
      "[epoch 5003]: training loss: 938.391357, consuming time:60.4421 s\n",
      "[epoch 5004]: training loss: 895.147583, consuming time:60.5643 s\n",
      "[epoch 5005]: training loss: 1086.259033, consuming time:60.5821 s\n",
      "[epoch 5006]: training loss: 926.210022, consuming time:61.1487 s\n",
      "[epoch 5007]: training loss: 1117.478882, consuming time:60.8836 s\n",
      "[epoch 5008]: training loss: 1068.039307, consuming time:60.5057 s\n",
      "[epoch 5009]: training loss: 1161.600830, consuming time:60.9000 s\n",
      "[epoch 5010]: training loss: 634.060730, consuming time:60.6036 s\n",
      "[epoch 5011]: training loss: 1015.406860, consuming time:60.6522 s\n",
      "[epoch 5012]: training loss: 751.427734, consuming time:60.7590 s\n",
      "[epoch 5013]: training loss: 848.540649, consuming time:60.7756 s\n",
      "[epoch 5014]: training loss: 1333.065552, consuming time:60.4632 s\n",
      "[epoch 5015]: training loss: 958.746155, consuming time:60.8253 s\n",
      "[epoch 5016]: training loss: 864.911743, consuming time:60.6523 s\n",
      "[epoch 5017]: training loss: 954.242554, consuming time:60.8070 s\n",
      "[epoch 5018]: training loss: 957.181152, consuming time:60.8854 s\n",
      "[epoch 5019]: training loss: 764.353516, consuming time:60.8489 s\n",
      "[epoch 5020]: training loss: 907.016235, consuming time:60.6577 s\n",
      "[epoch 5021]: training loss: 965.372131, consuming time:60.8271 s\n",
      "[epoch 5022]: training loss: 1126.712891, consuming time:60.9762 s\n",
      "[epoch 5023]: training loss: 913.306946, consuming time:60.8256 s\n",
      "[epoch 5024]: training loss: 1216.390991, consuming time:61.0291 s\n",
      "[epoch 5025]: training loss: 1124.604248, consuming time:60.6083 s\n",
      "[epoch 5026]: training loss: 1078.078369, consuming time:60.2744 s\n",
      "[epoch 5027]: training loss: 847.437927, consuming time:60.7214 s\n",
      "[epoch 5028]: training loss: 970.197632, consuming time:60.4789 s\n",
      "[epoch 5029]: training loss: 856.148193, consuming time:60.8567 s\n",
      "[epoch 5030]: training loss: 874.750854, consuming time:60.8167 s\n",
      "[epoch 5031]: training loss: 851.569946, consuming time:60.7031 s\n",
      "[epoch 5032]: training loss: 871.913330, consuming time:60.3279 s\n",
      "[epoch 5033]: training loss: 980.837097, consuming time:60.7279 s\n",
      "[epoch 5034]: training loss: 694.641846, consuming time:60.5581 s\n",
      "[epoch 5035]: training loss: 942.036011, consuming time:60.5603 s\n",
      "[epoch 5036]: training loss: 991.490967, consuming time:61.0594 s\n",
      "[epoch 5037]: training loss: 1058.877197, consuming time:60.6881 s\n",
      "[epoch 5038]: training loss: 768.640564, consuming time:60.6141 s\n",
      "[epoch 5039]: training loss: 864.317993, consuming time:60.5658 s\n",
      "[epoch 5040]: training loss: 899.930786, consuming time:60.7301 s\n",
      "[epoch 5041]: training loss: 831.709717, consuming time:60.8748 s\n",
      "[epoch 5042]: training loss: 1030.653320, consuming time:60.7308 s\n",
      "[epoch 5043]: training loss: 1308.904663, consuming time:60.8503 s\n",
      "[epoch 5044]: training loss: 768.857300, consuming time:60.3985 s\n",
      "[epoch 5045]: training loss: 961.697754, consuming time:60.8222 s\n",
      "[epoch 5046]: training loss: 985.352173, consuming time:60.7311 s\n",
      "[epoch 5047]: training loss: 845.121033, consuming time:60.8079 s\n",
      "[epoch 5048]: training loss: 877.269653, consuming time:60.8257 s\n",
      "[epoch 5049]: training loss: 973.214905, consuming time:60.5067 s\n",
      "[epoch 5050]: training loss: 1101.177246, consuming time:60.3678 s\n",
      "[epoch 5051]: training loss: 819.048462, consuming time:60.6378 s\n",
      "[epoch 5052]: training loss: 999.422363, consuming time:61.1550 s\n",
      "[epoch 5053]: training loss: 1349.614014, consuming time:60.8520 s\n",
      "[epoch 5054]: training loss: 773.221497, consuming time:60.7654 s\n",
      "[epoch 5055]: training loss: 1080.137695, consuming time:60.7838 s\n",
      "[epoch 5056]: training loss: 1087.695068, consuming time:60.7408 s\n",
      "[epoch 5057]: training loss: 922.531982, consuming time:61.1060 s\n",
      "[epoch 5058]: training loss: 756.089966, consuming time:60.8430 s\n",
      "[epoch 5059]: training loss: 1227.492188, consuming time:61.0184 s\n",
      "[epoch 5060]: training loss: 1014.001343, consuming time:61.0777 s\n",
      "[epoch 5061]: training loss: 928.947815, consuming time:60.3448 s\n",
      "[epoch 5062]: training loss: 1113.652344, consuming time:60.7251 s\n",
      "[epoch 5063]: training loss: 728.955322, consuming time:60.6649 s\n",
      "[epoch 5064]: training loss: 985.312927, consuming time:61.0380 s\n",
      "[epoch 5065]: training loss: 910.326538, consuming time:61.0554 s\n",
      "[epoch 5066]: training loss: 906.403687, consuming time:60.8589 s\n",
      "[epoch 5067]: training loss: 1038.660522, consuming time:60.4099 s\n",
      "[epoch 5068]: training loss: 917.965881, consuming time:60.8533 s\n",
      "[epoch 5069]: training loss: 868.303528, consuming time:60.8987 s\n",
      "[epoch 5070]: training loss: 1174.606445, consuming time:60.9804 s\n",
      "[epoch 5071]: training loss: 1176.340210, consuming time:61.5366 s\n",
      "[epoch 5072]: training loss: 1059.614624, consuming time:60.7136 s\n",
      "[epoch 5073]: training loss: 1008.493774, consuming time:60.7211 s\n",
      "[epoch 5074]: training loss: 918.636536, consuming time:60.9054 s\n",
      "[epoch 5075]: training loss: 973.005981, consuming time:60.8034 s\n",
      "[epoch 5076]: training loss: 843.443481, consuming time:60.7361 s\n",
      "[epoch 5077]: training loss: 920.493774, consuming time:60.7492 s\n",
      "[epoch 5078]: training loss: 1034.498779, consuming time:60.5150 s\n",
      "[epoch 5079]: training loss: 1107.059204, consuming time:60.3089 s\n",
      "[epoch 5080]: training loss: 1108.211670, consuming time:60.5566 s\n",
      "[epoch 5081]: training loss: 721.411377, consuming time:60.6922 s\n",
      "[epoch 5082]: training loss: 1033.013550, consuming time:60.9592 s\n",
      "[epoch 5083]: training loss: 1039.826172, consuming time:61.1566 s\n",
      "[epoch 5084]: training loss: 1188.655273, consuming time:60.6292 s\n",
      "[epoch 5085]: training loss: 1061.660645, consuming time:60.3569 s\n",
      "[epoch 5086]: training loss: 1354.297607, consuming time:60.7375 s\n",
      "[epoch 5087]: training loss: 1198.703491, consuming time:61.0122 s\n",
      "[epoch 5088]: training loss: 884.210999, consuming time:60.9990 s\n",
      "[epoch 5089]: training loss: 1124.184082, consuming time:60.5836 s\n",
      "[epoch 5090]: training loss: 1077.227783, consuming time:60.5488 s\n",
      "[epoch 5091]: training loss: 1008.454102, consuming time:60.3341 s\n",
      "[epoch 5092]: training loss: 763.315369, consuming time:60.8867 s\n",
      "[epoch 5093]: training loss: 800.800903, consuming time:61.1444 s\n",
      "[epoch 5094]: training loss: 933.434631, consuming time:60.8961 s\n",
      "[epoch 5095]: training loss: 974.187073, consuming time:60.8347 s\n",
      "[epoch 5096]: training loss: 1086.570068, consuming time:60.7291 s\n",
      "[epoch 5097]: training loss: 978.013916, consuming time:60.0729 s\n",
      "[epoch 5098]: training loss: 812.626038, consuming time:60.7581 s\n",
      "[epoch 5099]: training loss: 1045.308716, consuming time:60.9122 s\n",
      "[epoch 5100]: training loss: 929.923706, consuming time:61.1529 s\n",
      "[epoch 5101]: training loss: 800.798767, consuming time:60.6863 s\n",
      "[epoch 5102]: training loss: 1165.393555, consuming time:60.7021 s\n",
      "[epoch 5103]: training loss: 983.700378, consuming time:60.5252 s\n",
      "[epoch 5104]: training loss: 1068.003418, consuming time:60.9149 s\n",
      "[epoch 5105]: training loss: 1066.486084, consuming time:61.0130 s\n",
      "[epoch 5106]: training loss: 1313.631714, consuming time:60.9920 s\n",
      "[epoch 5107]: training loss: 964.762878, consuming time:60.7682 s\n",
      "[epoch 5108]: training loss: 947.305786, consuming time:60.6896 s\n",
      "[epoch 5109]: training loss: 935.599731, consuming time:60.6277 s\n",
      "[epoch 5110]: training loss: 898.811768, consuming time:60.9135 s\n",
      "[epoch 5111]: training loss: 658.958496, consuming time:60.9140 s\n",
      "[epoch 5112]: training loss: 789.295044, consuming time:61.0829 s\n",
      "[epoch 5113]: training loss: 870.925110, consuming time:60.9637 s\n",
      "[epoch 5114]: training loss: 971.633423, consuming time:60.6460 s\n",
      "[epoch 5115]: training loss: 958.060120, consuming time:60.5462 s\n",
      "[epoch 5116]: training loss: 786.895935, consuming time:61.0446 s\n",
      "[epoch 5117]: training loss: 1245.176880, consuming time:60.8910 s\n",
      "[epoch 5118]: training loss: 1083.714966, consuming time:60.9232 s\n",
      "[epoch 5119]: training loss: 1124.449951, consuming time:60.6899 s\n",
      "[epoch 5120]: training loss: 1123.022461, consuming time:60.4426 s\n",
      "[epoch 5121]: training loss: 929.980103, consuming time:60.8132 s\n",
      "[epoch 5122]: training loss: 1107.670410, consuming time:60.9222 s\n",
      "[epoch 5123]: training loss: 691.127808, consuming time:60.9428 s\n",
      "[epoch 5124]: training loss: 1099.104736, consuming time:60.8751 s\n",
      "[epoch 5125]: training loss: 803.113342, consuming time:60.3890 s\n",
      "[epoch 5126]: training loss: 790.368652, consuming time:60.6616 s\n",
      "[epoch 5127]: training loss: 1160.263184, consuming time:60.8600 s\n",
      "[epoch 5128]: training loss: 829.049805, consuming time:60.8032 s\n",
      "[epoch 5129]: training loss: 934.221375, consuming time:60.9521 s\n",
      "[epoch 5130]: training loss: 966.683044, consuming time:60.8886 s\n",
      "[epoch 5131]: training loss: 1188.979614, consuming time:60.3158 s\n",
      "[epoch 5132]: training loss: 947.750488, consuming time:60.5869 s\n",
      "[epoch 5133]: training loss: 955.069214, consuming time:60.7951 s\n",
      "[epoch 5134]: training loss: 884.622925, consuming time:60.8247 s\n",
      "[epoch 5135]: training loss: 762.399231, consuming time:60.6804 s\n",
      "[epoch 5136]: training loss: 956.548706, consuming time:60.7541 s\n",
      "[epoch 5137]: training loss: 808.909668, consuming time:60.4510 s\n",
      "[epoch 5138]: training loss: 1010.243347, consuming time:60.5778 s\n",
      "[epoch 5139]: training loss: 1045.371582, consuming time:60.8255 s\n",
      "[epoch 5140]: training loss: 920.111572, consuming time:60.7588 s\n",
      "[epoch 5141]: training loss: 1020.556763, consuming time:60.8854 s\n",
      "[epoch 5142]: training loss: 987.841370, consuming time:60.9833 s\n",
      "[epoch 5143]: training loss: 858.462952, consuming time:60.4957 s\n",
      "[epoch 5144]: training loss: 853.843750, consuming time:60.6130 s\n",
      "[epoch 5145]: training loss: 867.554199, consuming time:60.4850 s\n",
      "[epoch 5146]: training loss: 952.037842, consuming time:61.0598 s\n",
      "[epoch 5147]: training loss: 1124.485229, consuming time:60.8988 s\n",
      "[epoch 5148]: training loss: 852.428894, consuming time:60.7708 s\n",
      "[epoch 5149]: training loss: 964.872681, consuming time:60.8583 s\n",
      "[epoch 5150]: training loss: 1169.019897, consuming time:60.6222 s\n",
      "[epoch 5151]: training loss: 847.400879, consuming time:60.8212 s\n",
      "[epoch 5152]: training loss: 1262.918945, consuming time:60.9548 s\n",
      "[epoch 5153]: training loss: 879.773193, consuming time:61.2198 s\n",
      "[epoch 5154]: training loss: 1047.298828, consuming time:60.8193 s\n",
      "[epoch 5155]: training loss: 717.026733, consuming time:60.5574 s\n",
      "[epoch 5156]: training loss: 870.137817, consuming time:60.4225 s\n",
      "[epoch 5157]: training loss: 736.939026, consuming time:60.9112 s\n",
      "[epoch 5158]: training loss: 834.184875, consuming time:61.0283 s\n",
      "[epoch 5159]: training loss: 914.807861, consuming time:60.7252 s\n",
      "[epoch 5160]: training loss: 905.691040, consuming time:60.7750 s\n",
      "[epoch 5161]: training loss: 800.582764, consuming time:60.9628 s\n",
      "[epoch 5162]: training loss: 988.757446, consuming time:60.7246 s\n",
      "[epoch 5163]: training loss: 832.770142, consuming time:61.0484 s\n",
      "[epoch 5164]: training loss: 901.199829, consuming time:61.0040 s\n",
      "[epoch 5165]: training loss: 1106.697266, consuming time:60.7183 s\n",
      "[epoch 5166]: training loss: 976.447205, consuming time:60.6404 s\n",
      "[epoch 5167]: training loss: 1524.330078, consuming time:60.5289 s\n",
      "[epoch 5168]: training loss: 853.898193, consuming time:60.3875 s\n",
      "[epoch 5169]: training loss: 912.541260, consuming time:60.6854 s\n",
      "[epoch 5170]: training loss: 909.626953, consuming time:60.9209 s\n",
      "[epoch 5171]: training loss: 1004.980347, consuming time:64.7692 s\n",
      "[epoch 5172]: training loss: 759.579285, consuming time:60.6391 s\n",
      "[epoch 5173]: training loss: 875.579407, consuming time:60.8122 s\n",
      "[epoch 5174]: training loss: 917.253174, consuming time:60.7309 s\n",
      "[epoch 5175]: training loss: 912.196106, consuming time:60.9708 s\n",
      "[epoch 5176]: training loss: 1013.318848, consuming time:60.8664 s\n",
      "[epoch 5177]: training loss: 869.493652, consuming time:61.1712 s\n",
      "[epoch 5178]: training loss: 1333.226807, consuming time:60.8880 s\n",
      "[epoch 5179]: training loss: 1190.582764, consuming time:60.5294 s\n",
      "[epoch 5180]: training loss: 638.119507, consuming time:60.6301 s\n",
      "[epoch 5181]: training loss: 1181.181519, consuming time:61.0707 s\n",
      "[epoch 5182]: training loss: 867.043030, consuming time:60.7718 s\n",
      "[epoch 5183]: training loss: 954.006897, consuming time:60.6311 s\n",
      "[epoch 5184]: training loss: 1039.814209, consuming time:60.8107 s\n",
      "[epoch 5185]: training loss: 807.727539, consuming time:60.7437 s\n",
      "[epoch 5186]: training loss: 838.820190, consuming time:60.5460 s\n",
      "[epoch 5187]: training loss: 777.212402, consuming time:60.5626 s\n",
      "[epoch 5188]: training loss: 804.874390, consuming time:60.5352 s\n",
      "[epoch 5189]: training loss: 859.378967, consuming time:60.5471 s\n",
      "[epoch 5190]: training loss: 859.850830, consuming time:60.4468 s\n",
      "[epoch 5191]: training loss: 822.769287, consuming time:60.8793 s\n",
      "[epoch 5192]: training loss: 897.137939, consuming time:60.5058 s\n",
      "[epoch 5193]: training loss: 1113.764771, consuming time:60.7512 s\n",
      "[epoch 5194]: training loss: 977.845947, consuming time:60.4331 s\n",
      "[epoch 5195]: training loss: 744.907166, consuming time:60.7123 s\n",
      "[epoch 5196]: training loss: 1070.305908, consuming time:60.5791 s\n",
      "[epoch 5197]: training loss: 911.880249, consuming time:60.6628 s\n",
      "[epoch 5198]: training loss: 911.171997, consuming time:60.7441 s\n",
      "[epoch 5199]: training loss: 790.585327, consuming time:60.7074 s\n",
      "[epoch 5200]: training loss: 1069.191406, consuming time:60.8212 s\n",
      "[epoch 5201]: training loss: 916.690125, consuming time:60.7349 s\n",
      "[epoch 5202]: training loss: 1018.282471, consuming time:60.9501 s\n",
      "[epoch 5203]: training loss: 1118.747437, consuming time:60.9889 s\n",
      "[epoch 5204]: training loss: 1241.905518, consuming time:60.8477 s\n",
      "[epoch 5205]: training loss: 1044.880371, consuming time:60.5763 s\n",
      "[epoch 5206]: training loss: 977.142700, consuming time:61.1117 s\n",
      "[epoch 5207]: training loss: 1052.673096, consuming time:60.9789 s\n",
      "[epoch 5208]: training loss: 744.985962, consuming time:60.8653 s\n",
      "[epoch 5209]: training loss: 950.723938, consuming time:61.0423 s\n",
      "[epoch 5210]: training loss: 1004.374878, consuming time:61.4089 s\n",
      "[epoch 5211]: training loss: 788.960938, consuming time:60.8983 s\n",
      "[epoch 5212]: training loss: 1084.030762, consuming time:60.8069 s\n",
      "[epoch 5213]: training loss: 819.283691, consuming time:60.7433 s\n",
      "[epoch 5214]: training loss: 911.310730, consuming time:60.7798 s\n",
      "[epoch 5215]: training loss: 781.679810, consuming time:60.9555 s\n",
      "[epoch 5216]: training loss: 1064.398315, consuming time:61.0346 s\n",
      "[epoch 5217]: training loss: 1100.159912, consuming time:60.9173 s\n",
      "[epoch 5218]: training loss: 841.531250, consuming time:60.9154 s\n",
      "[epoch 5219]: training loss: 954.293945, consuming time:61.1112 s\n",
      "[epoch 5220]: training loss: 1126.928345, consuming time:60.7932 s\n",
      "[epoch 5221]: training loss: 735.654968, consuming time:60.7066 s\n",
      "[epoch 5222]: training loss: 1411.957520, consuming time:60.8148 s\n",
      "[epoch 5223]: training loss: 880.559570, consuming time:61.0476 s\n",
      "[epoch 5224]: training loss: 902.457886, consuming time:60.5394 s\n",
      "[epoch 5225]: training loss: 936.341858, consuming time:61.0159 s\n",
      "[epoch 5226]: training loss: 1004.242432, consuming time:60.8735 s\n",
      "[epoch 5227]: training loss: 1191.813843, consuming time:60.6557 s\n",
      "[epoch 5228]: training loss: 749.823669, consuming time:60.7261 s\n",
      "[epoch 5229]: training loss: 1101.539429, consuming time:60.6660 s\n",
      "[epoch 5230]: training loss: 1266.097778, consuming time:60.8618 s\n",
      "[epoch 5231]: training loss: 960.488770, consuming time:60.8010 s\n",
      "[epoch 5232]: training loss: 1093.992432, consuming time:60.8545 s\n",
      "[epoch 5233]: training loss: 950.733582, consuming time:60.4835 s\n",
      "[epoch 5234]: training loss: 1145.528809, consuming time:60.7593 s\n",
      "[epoch 5235]: training loss: 921.184692, consuming time:60.6833 s\n",
      "[epoch 5236]: training loss: 1233.961426, consuming time:60.7491 s\n",
      "[epoch 5237]: training loss: 1204.874512, consuming time:61.0075 s\n",
      "[epoch 5238]: training loss: 763.503662, consuming time:60.7988 s\n",
      "[epoch 5239]: training loss: 728.481567, consuming time:60.6711 s\n",
      "[epoch 5240]: training loss: 744.291870, consuming time:60.6654 s\n",
      "[epoch 5241]: training loss: 1004.118652, consuming time:60.6581 s\n",
      "[epoch 5242]: training loss: 846.285217, consuming time:60.7941 s\n",
      "[epoch 5243]: training loss: 735.692566, consuming time:60.7947 s\n",
      "[epoch 5244]: training loss: 871.546509, consuming time:60.7962 s\n",
      "[epoch 5245]: training loss: 913.169067, consuming time:60.6115 s\n",
      "[epoch 5246]: training loss: 978.918640, consuming time:60.8346 s\n",
      "[epoch 5247]: training loss: 898.123413, consuming time:60.8271 s\n",
      "[epoch 5248]: training loss: 952.625854, consuming time:60.9501 s\n",
      "[epoch 5249]: training loss: 634.011536, consuming time:61.1533 s\n",
      "[epoch 5250]: training loss: 825.949829, consuming time:60.9185 s\n",
      "[epoch 5251]: training loss: 1352.726318, consuming time:60.6082 s\n",
      "[epoch 5252]: training loss: 1000.367188, consuming time:60.6866 s\n",
      "[epoch 5253]: training loss: 972.297363, consuming time:60.7349 s\n",
      "[epoch 5254]: training loss: 1114.830688, consuming time:60.6643 s\n",
      "[epoch 5255]: training loss: 672.414551, consuming time:60.8755 s\n",
      "[epoch 5256]: training loss: 919.387634, consuming time:60.8287 s\n",
      "[epoch 5257]: training loss: 1167.648071, consuming time:60.4690 s\n",
      "[epoch 5258]: training loss: 957.481384, consuming time:60.9089 s\n",
      "[epoch 5259]: training loss: 928.704590, consuming time:60.8308 s\n",
      "[epoch 5260]: training loss: 907.779297, consuming time:61.0190 s\n",
      "[epoch 5261]: training loss: 886.277100, consuming time:60.8083 s\n",
      "[epoch 5262]: training loss: 910.762878, consuming time:60.7990 s\n",
      "[epoch 5263]: training loss: 1030.192627, consuming time:60.3300 s\n",
      "[epoch 5264]: training loss: 1057.052490, consuming time:60.8322 s\n",
      "[epoch 5265]: training loss: 778.837769, consuming time:60.7837 s\n",
      "[epoch 5266]: training loss: 1008.255371, consuming time:60.8605 s\n",
      "[epoch 5267]: training loss: 988.984619, consuming time:60.7045 s\n",
      "[epoch 5268]: training loss: 1125.739990, consuming time:60.7250 s\n",
      "[epoch 5269]: training loss: 1063.635498, consuming time:60.3532 s\n",
      "[epoch 5270]: training loss: 972.339600, consuming time:60.5198 s\n",
      "[epoch 5271]: training loss: 924.725037, consuming time:60.6460 s\n",
      "[epoch 5272]: training loss: 1068.381104, consuming time:60.8362 s\n",
      "[epoch 5273]: training loss: 1006.041382, consuming time:60.8540 s\n",
      "[epoch 5274]: training loss: 863.141907, consuming time:60.5120 s\n",
      "[epoch 5275]: training loss: 1000.008789, consuming time:60.4545 s\n",
      "[epoch 5276]: training loss: 1002.742188, consuming time:60.7835 s\n",
      "[epoch 5277]: training loss: 905.892883, consuming time:60.8531 s\n",
      "[epoch 5278]: training loss: 876.180420, consuming time:60.8202 s\n",
      "[epoch 5279]: training loss: 863.302856, consuming time:60.7257 s\n",
      "[epoch 5280]: training loss: 780.965942, consuming time:60.6579 s\n",
      "[epoch 5281]: training loss: 935.940308, consuming time:60.4043 s\n",
      "[epoch 5282]: training loss: 1199.201660, consuming time:60.3899 s\n",
      "[epoch 5283]: training loss: 824.677368, consuming time:60.9538 s\n",
      "[epoch 5284]: training loss: 1046.415283, consuming time:60.6901 s\n",
      "[epoch 5285]: training loss: 843.068237, consuming time:60.8663 s\n",
      "[epoch 5286]: training loss: 768.732117, consuming time:60.6199 s\n",
      "[epoch 5287]: training loss: 821.816650, consuming time:60.5202 s\n",
      "[epoch 5288]: training loss: 935.138733, consuming time:60.8565 s\n",
      "[epoch 5289]: training loss: 772.546509, consuming time:60.7371 s\n",
      "[epoch 5290]: training loss: 948.898987, consuming time:60.8995 s\n",
      "[epoch 5291]: training loss: 921.754272, consuming time:60.7513 s\n",
      "[epoch 5292]: training loss: 988.730896, consuming time:60.5698 s\n",
      "[epoch 5293]: training loss: 991.982178, consuming time:60.5719 s\n",
      "[epoch 5294]: training loss: 1108.691772, consuming time:60.9908 s\n",
      "[epoch 5295]: training loss: 962.173828, consuming time:61.1065 s\n",
      "[epoch 5296]: training loss: 835.081238, consuming time:60.8910 s\n",
      "[epoch 5297]: training loss: 1052.018799, consuming time:60.9021 s\n",
      "[epoch 5298]: training loss: 1144.094482, consuming time:60.4929 s\n",
      "[epoch 5299]: training loss: 1077.698486, consuming time:60.5209 s\n",
      "[epoch 5300]: training loss: 979.491943, consuming time:60.8790 s\n",
      "[epoch 5301]: training loss: 868.430786, consuming time:60.7512 s\n",
      "[epoch 5302]: training loss: 708.701660, consuming time:60.8881 s\n",
      "[epoch 5303]: training loss: 1281.036499, consuming time:60.9621 s\n",
      "[epoch 5304]: training loss: 860.454529, consuming time:60.6525 s\n",
      "[epoch 5305]: training loss: 856.127808, consuming time:60.6402 s\n",
      "[epoch 5306]: training loss: 754.480774, consuming time:60.6082 s\n",
      "[epoch 5307]: training loss: 1101.034424, consuming time:61.1890 s\n",
      "[epoch 5308]: training loss: 869.896606, consuming time:60.9583 s\n",
      "[epoch 5309]: training loss: 926.874878, consuming time:60.7646 s\n",
      "[epoch 5310]: training loss: 906.867249, consuming time:60.6342 s\n",
      "[epoch 5311]: training loss: 956.016968, consuming time:60.7182 s\n",
      "[epoch 5312]: training loss: 1129.294189, consuming time:60.9966 s\n",
      "[epoch 5313]: training loss: 1103.968872, consuming time:60.8289 s\n",
      "[epoch 5314]: training loss: 1068.750732, consuming time:60.9294 s\n",
      "[epoch 5315]: training loss: 1408.355957, consuming time:60.6401 s\n",
      "[epoch 5316]: training loss: 901.010132, consuming time:60.2628 s\n",
      "[epoch 5317]: training loss: 846.554688, consuming time:60.4627 s\n",
      "[epoch 5318]: training loss: 1130.716064, consuming time:60.8404 s\n",
      "[epoch 5319]: training loss: 1140.091064, consuming time:60.8670 s\n",
      "[epoch 5320]: training loss: 841.980713, consuming time:60.9374 s\n",
      "[epoch 5321]: training loss: 1088.656372, consuming time:60.4442 s\n",
      "[epoch 5322]: training loss: 874.897705, consuming time:60.4963 s\n",
      "[epoch 5323]: training loss: 974.843567, consuming time:60.8215 s\n",
      "[epoch 5324]: training loss: 867.735840, consuming time:61.1875 s\n",
      "[epoch 5325]: training loss: 863.735962, consuming time:61.1413 s\n",
      "[epoch 5326]: training loss: 1077.089478, consuming time:60.8415 s\n",
      "[epoch 5327]: training loss: 951.450012, consuming time:60.5470 s\n",
      "[epoch 5328]: training loss: 895.526611, consuming time:60.0503 s\n",
      "[epoch 5329]: training loss: 1011.196228, consuming time:61.0135 s\n",
      "[epoch 5330]: training loss: 992.001770, consuming time:60.8762 s\n",
      "[epoch 5331]: training loss: 920.358643, consuming time:61.1078 s\n",
      "[epoch 5332]: training loss: 827.209290, consuming time:61.0514 s\n",
      "[epoch 5333]: training loss: 1103.307373, consuming time:60.3977 s\n",
      "[epoch 5334]: training loss: 704.569885, consuming time:60.5308 s\n",
      "[epoch 5335]: training loss: 1178.974121, consuming time:61.1559 s\n",
      "[epoch 5336]: training loss: 952.545044, consuming time:61.1272 s\n",
      "[epoch 5337]: training loss: 783.114868, consuming time:61.0452 s\n",
      "[epoch 5338]: training loss: 907.272583, consuming time:61.0057 s\n",
      "[epoch 5339]: training loss: 823.635986, consuming time:60.7723 s\n",
      "[epoch 5340]: training loss: 932.762390, consuming time:60.6297 s\n",
      "[epoch 5341]: training loss: 721.303711, consuming time:60.7245 s\n",
      "[epoch 5342]: training loss: 993.095825, consuming time:61.2400 s\n",
      "[epoch 5343]: training loss: 816.660706, consuming time:60.9927 s\n",
      "[epoch 5344]: training loss: 735.205200, consuming time:60.6935 s\n",
      "[epoch 5345]: training loss: 811.234741, consuming time:60.6558 s\n",
      "[epoch 5346]: training loss: 901.470581, consuming time:60.4772 s\n",
      "[epoch 5347]: training loss: 1469.825195, consuming time:60.9929 s\n",
      "[epoch 5348]: training loss: 883.045288, consuming time:61.0120 s\n",
      "[epoch 5349]: training loss: 930.413757, consuming time:60.8210 s\n",
      "[epoch 5350]: training loss: 844.109619, consuming time:60.8169 s\n",
      "[epoch 5351]: training loss: 823.214111, consuming time:61.2020 s\n",
      "[epoch 5352]: training loss: 1018.647888, consuming time:61.3715 s\n",
      "[epoch 5353]: training loss: 843.943542, consuming time:61.1414 s\n",
      "[epoch 5354]: training loss: 691.330750, consuming time:61.8936 s\n",
      "[epoch 5355]: training loss: 746.583618, consuming time:62.0133 s\n",
      "[epoch 5356]: training loss: 841.790710, consuming time:61.8407 s\n",
      "[epoch 5357]: training loss: 1328.088257, consuming time:61.7753 s\n",
      "[epoch 5358]: training loss: 809.792603, consuming time:61.7814 s\n",
      "[epoch 5359]: training loss: 864.143433, consuming time:61.6708 s\n",
      "[epoch 5360]: training loss: 854.159058, consuming time:61.8602 s\n",
      "[epoch 5361]: training loss: 889.197266, consuming time:62.9138 s\n",
      "[epoch 5362]: training loss: 790.937927, consuming time:62.6120 s\n",
      "[epoch 5363]: training loss: 1192.937988, consuming time:62.5762 s\n",
      "[epoch 5364]: training loss: 1072.239990, consuming time:62.4860 s\n",
      "[epoch 5365]: training loss: 927.797607, consuming time:62.4426 s\n",
      "[epoch 5366]: training loss: 997.087646, consuming time:62.2937 s\n",
      "[epoch 5367]: training loss: 922.731018, consuming time:61.1549 s\n",
      "[epoch 5368]: training loss: 1325.586914, consuming time:60.4141 s\n",
      "[epoch 5369]: training loss: 1330.055908, consuming time:60.6122 s\n",
      "[epoch 5370]: training loss: 931.945496, consuming time:60.4176 s\n",
      "[epoch 5371]: training loss: 1150.802368, consuming time:61.0776 s\n",
      "[epoch 5372]: training loss: 1004.343201, consuming time:60.7513 s\n",
      "[epoch 5373]: training loss: 1161.609131, consuming time:60.6693 s\n",
      "[epoch 5374]: training loss: 942.450317, consuming time:60.8325 s\n",
      "[epoch 5375]: training loss: 804.034851, consuming time:60.7523 s\n",
      "[epoch 5376]: training loss: 1006.345703, consuming time:60.8091 s\n",
      "[epoch 5377]: training loss: 1237.414429, consuming time:60.7836 s\n",
      "[epoch 5378]: training loss: 876.524780, consuming time:60.9087 s\n",
      "[epoch 5379]: training loss: 640.669678, consuming time:61.5288 s\n",
      "[epoch 5380]: training loss: 718.781372, consuming time:61.1729 s\n",
      "[epoch 5381]: training loss: 980.100159, consuming time:61.3199 s\n",
      "[epoch 5382]: training loss: 1316.895508, consuming time:61.3068 s\n",
      "[epoch 5383]: training loss: 1195.657837, consuming time:61.0413 s\n",
      "[epoch 5384]: training loss: 1058.514648, consuming time:61.0555 s\n",
      "[epoch 5385]: training loss: 697.226746, consuming time:61.3452 s\n",
      "[epoch 5386]: training loss: 819.881348, consuming time:61.2173 s\n",
      "[epoch 5387]: training loss: 737.478271, consuming time:61.4357 s\n",
      "[epoch 5388]: training loss: 803.670532, consuming time:61.2431 s\n",
      "[epoch 5389]: training loss: 816.154785, consuming time:61.3311 s\n",
      "[epoch 5390]: training loss: 841.049561, consuming time:61.2223 s\n",
      "[epoch 5391]: training loss: 1108.264160, consuming time:61.3595 s\n",
      "[epoch 5392]: training loss: 745.934509, consuming time:61.6572 s\n",
      "[epoch 5393]: training loss: 1077.155884, consuming time:61.6463 s\n",
      "[epoch 5394]: training loss: 1060.653564, consuming time:61.2849 s\n",
      "[epoch 5395]: training loss: 819.307739, consuming time:61.4680 s\n",
      "[epoch 5396]: training loss: 1042.940186, consuming time:61.4475 s\n",
      "[epoch 5397]: training loss: 1048.989868, consuming time:61.2434 s\n",
      "[epoch 5398]: training loss: 839.534363, consuming time:61.3436 s\n",
      "[epoch 5399]: training loss: 688.765503, consuming time:61.1211 s\n",
      "[epoch 5400]: training loss: 1178.571777, consuming time:61.5212 s\n",
      "[epoch 5401]: training loss: 947.786011, consuming time:61.5819 s\n",
      "[epoch 5402]: training loss: 1417.315186, consuming time:61.4617 s\n",
      "[epoch 5403]: training loss: 918.605286, consuming time:61.2232 s\n",
      "[epoch 5404]: training loss: 960.269409, consuming time:61.6539 s\n",
      "[epoch 5405]: training loss: 797.846680, consuming time:61.5538 s\n",
      "[epoch 5406]: training loss: 783.495605, consuming time:63.7761 s\n",
      "[epoch 5407]: training loss: 826.639038, consuming time:63.7625 s\n",
      "[epoch 5408]: training loss: 887.889832, consuming time:64.8787 s\n",
      "[epoch 5409]: training loss: 829.837524, consuming time:61.1023 s\n",
      "[epoch 5410]: training loss: 769.064941, consuming time:60.8899 s\n",
      "[epoch 5411]: training loss: 1064.509766, consuming time:61.1115 s\n",
      "[epoch 5412]: training loss: 1151.079102, consuming time:62.2332 s\n",
      "[epoch 5413]: training loss: 1019.081421, consuming time:61.1527 s\n",
      "[epoch 5414]: training loss: 1230.078613, consuming time:61.1864 s\n",
      "[epoch 5415]: training loss: 918.353210, consuming time:61.2827 s\n",
      "[epoch 5416]: training loss: 866.324951, consuming time:61.2253 s\n",
      "[epoch 5417]: training loss: 996.411743, consuming time:61.4636 s\n",
      "[epoch 5418]: training loss: 983.176453, consuming time:61.5557 s\n",
      "[epoch 5419]: training loss: 715.552612, consuming time:63.5250 s\n",
      "[epoch 5420]: training loss: 856.795776, consuming time:61.0073 s\n",
      "[epoch 5421]: training loss: 993.586975, consuming time:64.0747 s\n",
      "[epoch 5422]: training loss: 993.030396, consuming time:62.1082 s\n",
      "[epoch 5423]: training loss: 1272.137695, consuming time:60.9745 s\n",
      "[epoch 5424]: training loss: 916.074951, consuming time:61.4666 s\n",
      "[epoch 5425]: training loss: 1152.770508, consuming time:64.5950 s\n",
      "[epoch 5426]: training loss: 990.031128, consuming time:62.5982 s\n",
      "[epoch 5427]: training loss: 1159.212769, consuming time:62.6311 s\n",
      "[epoch 5428]: training loss: 1184.500122, consuming time:66.5987 s\n",
      "[epoch 5429]: training loss: 948.758545, consuming time:66.5449 s\n",
      "[epoch 5430]: training loss: 1251.080566, consuming time:63.5706 s\n",
      "[epoch 5431]: training loss: 773.622986, consuming time:62.0366 s\n",
      "[epoch 5432]: training loss: 1021.343750, consuming time:66.0162 s\n",
      "[epoch 5433]: training loss: 858.976929, consuming time:64.1190 s\n",
      "[epoch 5434]: training loss: 1098.861450, consuming time:68.0629 s\n",
      "[epoch 5435]: training loss: 1076.152954, consuming time:62.6065 s\n",
      "[epoch 5436]: training loss: 812.716553, consuming time:61.4119 s\n",
      "[epoch 5437]: training loss: 998.285278, consuming time:61.1381 s\n",
      "[epoch 5438]: training loss: 1023.060669, consuming time:61.4222 s\n",
      "[epoch 5439]: training loss: 864.215820, consuming time:60.7598 s\n",
      "[epoch 5440]: training loss: 869.393738, consuming time:60.8132 s\n",
      "[epoch 5441]: training loss: 979.639038, consuming time:61.2407 s\n",
      "[epoch 5442]: training loss: 940.639771, consuming time:65.1940 s\n",
      "[epoch 5443]: training loss: 906.404602, consuming time:64.0213 s\n",
      "[epoch 5444]: training loss: 988.190369, consuming time:62.5129 s\n",
      "[epoch 5445]: training loss: 1451.916748, consuming time:63.1873 s\n",
      "[epoch 5446]: training loss: 896.263184, consuming time:63.3288 s\n",
      "[epoch 5447]: training loss: 1026.378296, consuming time:63.8196 s\n",
      "[epoch 5448]: training loss: 1275.840088, consuming time:61.9152 s\n",
      "[epoch 5449]: training loss: 1064.881836, consuming time:61.8038 s\n",
      "[epoch 5450]: training loss: 1008.208313, consuming time:61.5905 s\n",
      "[epoch 5451]: training loss: 866.782104, consuming time:61.4442 s\n",
      "[epoch 5452]: training loss: 860.330566, consuming time:61.8635 s\n",
      "[epoch 5453]: training loss: 1201.997314, consuming time:61.6932 s\n",
      "[epoch 5454]: training loss: 839.468811, consuming time:61.7321 s\n",
      "[epoch 5455]: training loss: 747.501099, consuming time:61.7287 s\n",
      "[epoch 5456]: training loss: 1024.039795, consuming time:61.6424 s\n",
      "[epoch 5457]: training loss: 760.103638, consuming time:61.6338 s\n",
      "[epoch 5458]: training loss: 1028.335083, consuming time:61.6437 s\n",
      "[epoch 5459]: training loss: 1162.274780, consuming time:61.6262 s\n",
      "[epoch 5460]: training loss: 878.656555, consuming time:61.5195 s\n",
      "[epoch 5461]: training loss: 828.505005, consuming time:61.7261 s\n",
      "[epoch 5462]: training loss: 903.293701, consuming time:61.5873 s\n",
      "[epoch 5463]: training loss: 1033.585449, consuming time:61.6679 s\n",
      "[epoch 5464]: training loss: 1006.806091, consuming time:61.5495 s\n",
      "[epoch 5465]: training loss: 1112.654541, consuming time:61.6497 s\n",
      "[epoch 5466]: training loss: 1020.097473, consuming time:61.5982 s\n",
      "[epoch 5467]: training loss: 1021.010193, consuming time:61.6019 s\n",
      "[epoch 5468]: training loss: 1162.919189, consuming time:61.9546 s\n",
      "[epoch 5469]: training loss: 841.255066, consuming time:61.5578 s\n",
      "[epoch 5470]: training loss: 799.780334, consuming time:61.7530 s\n",
      "[epoch 5471]: training loss: 1071.093262, consuming time:61.5845 s\n",
      "[epoch 5472]: training loss: 1189.670166, consuming time:61.6468 s\n",
      "[epoch 5473]: training loss: 629.320679, consuming time:61.5096 s\n",
      "[epoch 5474]: training loss: 1235.187744, consuming time:61.5229 s\n",
      "[epoch 5475]: training loss: 921.886353, consuming time:61.6314 s\n",
      "[epoch 5476]: training loss: 720.576294, consuming time:61.6957 s\n",
      "[epoch 5477]: training loss: 696.216187, consuming time:61.7383 s\n",
      "[epoch 5478]: training loss: 1107.318115, consuming time:61.5855 s\n",
      "[epoch 5479]: training loss: 1215.071045, consuming time:61.6398 s\n",
      "[epoch 5480]: training loss: 596.508057, consuming time:61.4753 s\n",
      "[epoch 5481]: training loss: 925.775269, consuming time:61.8127 s\n",
      "[epoch 5482]: training loss: 1142.132568, consuming time:61.6894 s\n",
      "[epoch 5483]: training loss: 752.668335, consuming time:61.6313 s\n",
      "[epoch 5484]: training loss: 1022.126953, consuming time:61.5834 s\n",
      "[epoch 5485]: training loss: 826.873840, consuming time:61.7483 s\n",
      "[epoch 5486]: training loss: 810.630554, consuming time:61.7586 s\n",
      "[epoch 5487]: training loss: 831.774597, consuming time:61.5020 s\n",
      "[epoch 5488]: training loss: 961.051575, consuming time:61.9239 s\n",
      "[epoch 5489]: training loss: 751.112183, consuming time:64.2065 s\n",
      "[epoch 5490]: training loss: 891.279114, consuming time:61.6114 s\n",
      "[epoch 5491]: training loss: 1159.690430, consuming time:61.6200 s\n",
      "[epoch 5492]: training loss: 826.667419, consuming time:63.4939 s\n",
      "[epoch 5493]: training loss: 1093.929443, consuming time:65.8386 s\n",
      "[epoch 5494]: training loss: 989.007690, consuming time:66.9358 s\n",
      "[epoch 5495]: training loss: 986.385376, consuming time:66.8011 s\n",
      "[epoch 5496]: training loss: 880.449402, consuming time:65.8084 s\n",
      "[epoch 5497]: training loss: 1051.407104, consuming time:69.3420 s\n",
      "[epoch 5498]: training loss: 981.131958, consuming time:63.8882 s\n",
      "[epoch 5499]: training loss: 828.964600, consuming time:62.3393 s\n",
      "[epoch 5500]: training loss: 684.222351, consuming time:62.3956 s\n",
      "[epoch 5501]: training loss: 685.257141, consuming time:62.5812 s\n",
      "[epoch 5502]: training loss: 809.370972, consuming time:62.7201 s\n",
      "[epoch 5503]: training loss: 978.689148, consuming time:62.5846 s\n",
      "[epoch 5504]: training loss: 986.067566, consuming time:62.4495 s\n",
      "[epoch 5505]: training loss: 1086.280762, consuming time:62.3720 s\n",
      "[epoch 5506]: training loss: 1166.248291, consuming time:62.3655 s\n",
      "[epoch 5507]: training loss: 887.813782, consuming time:62.5716 s\n",
      "[epoch 5508]: training loss: 983.041931, consuming time:62.5003 s\n",
      "[epoch 5509]: training loss: 946.107300, consuming time:62.3317 s\n",
      "[epoch 5510]: training loss: 1043.751465, consuming time:61.8087 s\n",
      "[epoch 5511]: training loss: 677.791321, consuming time:61.6130 s\n",
      "[epoch 5512]: training loss: 1236.182373, consuming time:61.7659 s\n",
      "[epoch 5513]: training loss: 1034.086670, consuming time:61.9082 s\n",
      "[epoch 5514]: training loss: 928.735474, consuming time:61.7382 s\n",
      "[epoch 5515]: training loss: 908.213989, consuming time:61.8150 s\n",
      "[epoch 5516]: training loss: 941.534973, consuming time:61.6659 s\n",
      "[epoch 5517]: training loss: 1031.025879, consuming time:63.1320 s\n",
      "[epoch 5518]: training loss: 989.641113, consuming time:61.8371 s\n",
      "[epoch 5519]: training loss: 1023.955444, consuming time:61.2069 s\n",
      "[epoch 5520]: training loss: 937.221008, consuming time:61.1659 s\n",
      "[epoch 5521]: training loss: 996.664368, consuming time:61.4869 s\n",
      "[epoch 5522]: training loss: 885.616943, consuming time:63.0047 s\n",
      "[epoch 5523]: training loss: 1042.269165, consuming time:64.6384 s\n",
      "[epoch 5524]: training loss: 999.373657, consuming time:61.1173 s\n",
      "[epoch 5525]: training loss: 980.969482, consuming time:61.1417 s\n",
      "[epoch 5526]: training loss: 879.638916, consuming time:60.9445 s\n",
      "[epoch 5527]: training loss: 938.024414, consuming time:60.9411 s\n",
      "[epoch 5528]: training loss: 1140.247437, consuming time:63.6754 s\n",
      "[epoch 5529]: training loss: 698.420349, consuming time:60.9731 s\n",
      "[epoch 5530]: training loss: 847.976929, consuming time:60.6355 s\n",
      "[epoch 5531]: training loss: 856.436829, consuming time:60.2235 s\n",
      "[epoch 5532]: training loss: 730.394043, consuming time:60.6994 s\n",
      "[epoch 5533]: training loss: 1033.593140, consuming time:60.7080 s\n",
      "[epoch 5534]: training loss: 746.616089, consuming time:63.0150 s\n",
      "[epoch 5535]: training loss: 839.150269, consuming time:61.7231 s\n",
      "[epoch 5536]: training loss: 926.227478, consuming time:61.4871 s\n",
      "[epoch 5537]: training loss: 974.812256, consuming time:62.1399 s\n",
      "[epoch 5538]: training loss: 1158.788330, consuming time:61.0358 s\n",
      "[epoch 5539]: training loss: 883.701050, consuming time:63.6585 s\n",
      "[epoch 5540]: training loss: 887.275513, consuming time:65.2201 s\n",
      "[epoch 5541]: training loss: 898.232910, consuming time:61.4414 s\n",
      "[epoch 5542]: training loss: 1059.951172, consuming time:61.2623 s\n",
      "[epoch 5543]: training loss: 981.091553, consuming time:61.1057 s\n",
      "[epoch 5544]: training loss: 833.945557, consuming time:61.0713 s\n",
      "[epoch 5545]: training loss: 1094.758057, consuming time:61.5503 s\n",
      "[epoch 5546]: training loss: 1044.636719, consuming time:61.0578 s\n",
      "[epoch 5547]: training loss: 1081.350098, consuming time:61.1417 s\n",
      "[epoch 5548]: training loss: 996.583740, consuming time:62.9400 s\n",
      "[epoch 5549]: training loss: 860.685181, consuming time:62.1366 s\n",
      "[epoch 5550]: training loss: 862.630432, consuming time:61.7459 s\n",
      "[epoch 5551]: training loss: 998.476746, consuming time:61.4978 s\n",
      "[epoch 5552]: training loss: 1045.333252, consuming time:61.9943 s\n",
      "[epoch 5553]: training loss: 957.237915, consuming time:62.1812 s\n",
      "[epoch 5554]: training loss: 854.688110, consuming time:61.0247 s\n",
      "[epoch 5555]: training loss: 825.222534, consuming time:61.2609 s\n",
      "[epoch 5556]: training loss: 903.942322, consuming time:62.5369 s\n",
      "[epoch 5557]: training loss: 1213.491943, consuming time:62.5183 s\n",
      "[epoch 5558]: training loss: 868.311096, consuming time:62.8120 s\n",
      "[epoch 5559]: training loss: 730.043945, consuming time:61.8530 s\n",
      "[epoch 5560]: training loss: 947.167969, consuming time:61.1065 s\n",
      "[epoch 5561]: training loss: 797.106689, consuming time:61.6812 s\n",
      "[epoch 5562]: training loss: 1032.454224, consuming time:60.9361 s\n",
      "[epoch 5563]: training loss: 1166.857544, consuming time:60.9919 s\n",
      "[epoch 5564]: training loss: 895.057129, consuming time:61.0223 s\n",
      "[epoch 5565]: training loss: 980.523865, consuming time:60.6382 s\n",
      "[epoch 5566]: training loss: 1006.062744, consuming time:60.9621 s\n",
      "[epoch 5567]: training loss: 979.739258, consuming time:61.0660 s\n",
      "[epoch 5568]: training loss: 882.232483, consuming time:61.1589 s\n",
      "[epoch 5569]: training loss: 821.290649, consuming time:60.9138 s\n",
      "[epoch 5570]: training loss: 988.965576, consuming time:61.1056 s\n",
      "[epoch 5571]: training loss: 912.707092, consuming time:60.9637 s\n",
      "[epoch 5572]: training loss: 906.083984, consuming time:61.0096 s\n",
      "[epoch 5573]: training loss: 1537.287109, consuming time:61.7965 s\n",
      "[epoch 5574]: training loss: 858.763489, consuming time:61.3379 s\n",
      "[epoch 5575]: training loss: 1043.977783, consuming time:63.0051 s\n",
      "[epoch 5576]: training loss: 700.139526, consuming time:64.0323 s\n",
      "[epoch 5577]: training loss: 968.861877, consuming time:62.8310 s\n",
      "[epoch 5578]: training loss: 1058.875732, consuming time:64.4621 s\n",
      "[epoch 5579]: training loss: 1253.558594, consuming time:62.0322 s\n",
      "[epoch 5580]: training loss: 900.580322, consuming time:61.9596 s\n",
      "[epoch 5581]: training loss: 1185.264771, consuming time:61.9188 s\n",
      "[epoch 5582]: training loss: 615.509583, consuming time:62.2475 s\n",
      "[epoch 5583]: training loss: 913.472656, consuming time:61.9228 s\n",
      "[epoch 5584]: training loss: 748.196838, consuming time:61.9808 s\n",
      "[epoch 5585]: training loss: 854.608887, consuming time:62.0882 s\n",
      "[epoch 5586]: training loss: 905.329956, consuming time:61.7342 s\n",
      "[epoch 5587]: training loss: 1419.423340, consuming time:61.7574 s\n",
      "[epoch 5588]: training loss: 871.573181, consuming time:61.4687 s\n",
      "[epoch 5589]: training loss: 1078.241577, consuming time:63.5991 s\n",
      "[epoch 5590]: training loss: 850.967834, consuming time:68.5358 s\n",
      "[epoch 5591]: training loss: 753.109619, consuming time:62.5361 s\n",
      "[epoch 5592]: training loss: 855.696533, consuming time:61.6262 s\n",
      "[epoch 5593]: training loss: 876.624084, consuming time:61.4341 s\n",
      "[epoch 5594]: training loss: 1030.733398, consuming time:61.5248 s\n",
      "[epoch 5595]: training loss: 1066.696167, consuming time:62.7126 s\n",
      "[epoch 5596]: training loss: 662.052307, consuming time:61.8080 s\n",
      "[epoch 5597]: training loss: 1001.480469, consuming time:61.7194 s\n",
      "[epoch 5598]: training loss: 915.654724, consuming time:61.8546 s\n",
      "[epoch 5599]: training loss: 1061.048828, consuming time:62.1148 s\n",
      "[epoch 5600]: training loss: 808.006226, consuming time:61.8725 s\n",
      "[epoch 5601]: training loss: 804.461548, consuming time:61.6127 s\n",
      "[epoch 5602]: training loss: 1184.804932, consuming time:61.3870 s\n",
      "[epoch 5603]: training loss: 770.303101, consuming time:61.3359 s\n",
      "[epoch 5604]: training loss: 859.245850, consuming time:67.6842 s\n",
      "[epoch 5605]: training loss: 1181.602295, consuming time:65.2333 s\n",
      "[epoch 5606]: training loss: 986.528442, consuming time:61.4560 s\n",
      "[epoch 5607]: training loss: 901.138184, consuming time:61.3208 s\n",
      "[epoch 5608]: training loss: 1127.407837, consuming time:61.1990 s\n",
      "[epoch 5609]: training loss: 834.679321, consuming time:61.1996 s\n",
      "[epoch 5610]: training loss: 1016.697327, consuming time:61.1692 s\n",
      "[epoch 5611]: training loss: 847.986877, consuming time:61.5101 s\n",
      "[epoch 5612]: training loss: 959.113770, consuming time:62.2940 s\n",
      "[epoch 5613]: training loss: 998.669556, consuming time:63.1429 s\n",
      "[epoch 5614]: training loss: 905.396851, consuming time:61.5260 s\n",
      "[epoch 5615]: training loss: 851.589355, consuming time:64.2450 s\n",
      "[epoch 5616]: training loss: 792.946106, consuming time:64.0891 s\n",
      "[epoch 5617]: training loss: 861.462891, consuming time:61.3039 s\n",
      "[epoch 5618]: training loss: 850.643372, consuming time:61.7925 s\n",
      "[epoch 5619]: training loss: 971.285706, consuming time:61.5510 s\n",
      "[epoch 5620]: training loss: 959.375488, consuming time:61.3676 s\n",
      "[epoch 5621]: training loss: 950.336548, consuming time:61.6367 s\n",
      "[epoch 5622]: training loss: 1182.519531, consuming time:69.9948 s\n",
      "[epoch 5623]: training loss: 1155.220215, consuming time:70.1986 s\n",
      "[epoch 5624]: training loss: 1006.009277, consuming time:66.5275 s\n",
      "[epoch 5625]: training loss: 985.492065, consuming time:66.4782 s\n",
      "[epoch 5626]: training loss: 954.464600, consuming time:63.6452 s\n",
      "[epoch 5627]: training loss: 938.834473, consuming time:61.3425 s\n",
      "[epoch 5628]: training loss: 911.351013, consuming time:61.2115 s\n",
      "[epoch 5629]: training loss: 787.469421, consuming time:61.0454 s\n",
      "[epoch 5630]: training loss: 1113.158081, consuming time:61.3715 s\n",
      "[epoch 5631]: training loss: 924.413208, consuming time:60.8434 s\n",
      "[epoch 5632]: training loss: 799.266235, consuming time:61.1322 s\n",
      "[epoch 5633]: training loss: 907.863647, consuming time:60.8997 s\n",
      "[epoch 5634]: training loss: 905.474121, consuming time:60.5746 s\n",
      "[epoch 5635]: training loss: 1150.270630, consuming time:60.5837 s\n",
      "[epoch 5636]: training loss: 1063.251831, consuming time:60.8745 s\n",
      "[epoch 5637]: training loss: 1137.578735, consuming time:61.0073 s\n",
      "[epoch 5638]: training loss: 964.436646, consuming time:61.1425 s\n",
      "[epoch 5639]: training loss: 1355.020020, consuming time:61.1739 s\n",
      "[epoch 5640]: training loss: 1171.288574, consuming time:61.3201 s\n",
      "[epoch 5641]: training loss: 913.068909, consuming time:61.6501 s\n",
      "[epoch 5642]: training loss: 926.231445, consuming time:61.5162 s\n",
      "[epoch 5643]: training loss: 978.551941, consuming time:61.3358 s\n",
      "[epoch 5644]: training loss: 806.939819, consuming time:61.2093 s\n",
      "[epoch 5645]: training loss: 725.158020, consuming time:61.0406 s\n",
      "[epoch 5646]: training loss: 845.798523, consuming time:61.2752 s\n",
      "[epoch 5647]: training loss: 854.912476, consuming time:62.1038 s\n",
      "[epoch 5648]: training loss: 806.250488, consuming time:60.8713 s\n",
      "[epoch 5649]: training loss: 888.364990, consuming time:61.4372 s\n",
      "[epoch 5650]: training loss: 808.414307, consuming time:61.4294 s\n",
      "[epoch 5651]: training loss: 1199.299194, consuming time:61.0644 s\n",
      "[epoch 5652]: training loss: 866.700439, consuming time:61.2328 s\n",
      "[epoch 5653]: training loss: 749.380737, consuming time:61.1431 s\n",
      "[epoch 5654]: training loss: 768.088013, consuming time:61.5099 s\n",
      "[epoch 5655]: training loss: 1062.939697, consuming time:61.2764 s\n",
      "[epoch 5656]: training loss: 1119.388550, consuming time:61.0328 s\n",
      "[epoch 5657]: training loss: 1090.912598, consuming time:61.1566 s\n",
      "[epoch 5658]: training loss: 719.601135, consuming time:60.9146 s\n",
      "[epoch 5659]: training loss: 928.539673, consuming time:61.1585 s\n",
      "[epoch 5660]: training loss: 1094.428711, consuming time:61.0399 s\n",
      "[epoch 5661]: training loss: 1060.731201, consuming time:61.0197 s\n",
      "[epoch 5662]: training loss: 729.771667, consuming time:60.9909 s\n",
      "[epoch 5663]: training loss: 942.336914, consuming time:60.7956 s\n",
      "[epoch 5664]: training loss: 860.692749, consuming time:60.7394 s\n",
      "[epoch 5665]: training loss: 1621.109131, consuming time:60.7617 s\n",
      "[epoch 5666]: training loss: 1060.764404, consuming time:60.6546 s\n",
      "[epoch 5667]: training loss: 1043.654297, consuming time:60.7580 s\n",
      "[epoch 5668]: training loss: 994.624023, consuming time:61.1893 s\n",
      "[epoch 5669]: training loss: 1052.839478, consuming time:60.9174 s\n",
      "[epoch 5670]: training loss: 819.574219, consuming time:60.8504 s\n",
      "[epoch 5671]: training loss: 862.123962, consuming time:60.4843 s\n",
      "[epoch 5672]: training loss: 1079.224121, consuming time:60.6659 s\n",
      "[epoch 5673]: training loss: 760.292664, consuming time:61.0033 s\n",
      "[epoch 5674]: training loss: 770.910767, consuming time:60.9344 s\n",
      "[epoch 5675]: training loss: 977.637756, consuming time:60.8083 s\n",
      "[epoch 5676]: training loss: 1061.327148, consuming time:60.5374 s\n",
      "[epoch 5677]: training loss: 1001.473267, consuming time:60.5888 s\n",
      "[epoch 5678]: training loss: 1074.249023, consuming time:60.7949 s\n",
      "[epoch 5679]: training loss: 945.729614, consuming time:60.8591 s\n",
      "[epoch 5680]: training loss: 1114.769043, consuming time:60.9444 s\n",
      "[epoch 5681]: training loss: 947.862427, consuming time:60.6175 s\n",
      "[epoch 5682]: training loss: 747.809570, consuming time:61.0038 s\n",
      "[epoch 5683]: training loss: 1306.872803, consuming time:60.9130 s\n",
      "[epoch 5684]: training loss: 1096.852783, consuming time:60.8384 s\n",
      "[epoch 5685]: training loss: 874.595825, consuming time:60.9636 s\n",
      "[epoch 5686]: training loss: 808.202820, consuming time:61.1579 s\n",
      "[epoch 5687]: training loss: 842.800537, consuming time:61.0362 s\n",
      "[epoch 5688]: training loss: 1016.306274, consuming time:61.0575 s\n",
      "[epoch 5689]: training loss: 1046.933350, consuming time:60.7932 s\n",
      "[epoch 5690]: training loss: 1023.906616, consuming time:61.1075 s\n",
      "[epoch 5691]: training loss: 613.131348, consuming time:61.1973 s\n",
      "[epoch 5692]: training loss: 874.998596, consuming time:60.9144 s\n",
      "[epoch 5693]: training loss: 1009.415039, consuming time:60.9293 s\n",
      "[epoch 5694]: training loss: 907.382446, consuming time:60.6869 s\n",
      "[epoch 5695]: training loss: 767.453552, consuming time:60.7066 s\n",
      "[epoch 5696]: training loss: 892.262756, consuming time:61.0332 s\n",
      "[epoch 5697]: training loss: 842.123657, consuming time:61.2547 s\n",
      "[epoch 5698]: training loss: 976.740112, consuming time:60.9317 s\n",
      "[epoch 5699]: training loss: 999.585022, consuming time:60.5940 s\n",
      "[epoch 5700]: training loss: 866.851013, consuming time:60.4856 s\n",
      "[epoch 5701]: training loss: 925.480225, consuming time:60.5229 s\n",
      "[epoch 5702]: training loss: 862.371216, consuming time:60.6400 s\n",
      "[epoch 5703]: training loss: 759.791504, consuming time:60.9057 s\n",
      "[epoch 5704]: training loss: 819.241943, consuming time:60.7563 s\n",
      "[epoch 5705]: training loss: 945.103699, consuming time:60.6533 s\n",
      "[epoch 5706]: training loss: 1076.591553, consuming time:60.1366 s\n",
      "[epoch 5707]: training loss: 857.328369, consuming time:60.2835 s\n",
      "[epoch 5708]: training loss: 973.456055, consuming time:60.7905 s\n",
      "[epoch 5709]: training loss: 967.245972, consuming time:60.7513 s\n",
      "[epoch 5710]: training loss: 1319.110840, consuming time:60.6731 s\n",
      "[epoch 5711]: training loss: 1110.449951, consuming time:60.6249 s\n",
      "[epoch 5712]: training loss: 1161.306763, consuming time:60.5065 s\n",
      "[epoch 5713]: training loss: 997.059082, consuming time:60.2472 s\n",
      "[epoch 5714]: training loss: 1042.898071, consuming time:60.5127 s\n",
      "[epoch 5715]: training loss: 947.701050, consuming time:60.5137 s\n",
      "[epoch 5716]: training loss: 800.301941, consuming time:60.3833 s\n",
      "[epoch 5717]: training loss: 796.659973, consuming time:60.4240 s\n",
      "[epoch 5718]: training loss: 853.884644, consuming time:60.2195 s\n",
      "[epoch 5719]: training loss: 941.537415, consuming time:60.6028 s\n",
      "[epoch 5720]: training loss: 847.954712, consuming time:60.9341 s\n",
      "[epoch 5721]: training loss: 998.445496, consuming time:60.4622 s\n",
      "[epoch 5722]: training loss: 872.020264, consuming time:60.5262 s\n",
      "[epoch 5723]: training loss: 742.873474, consuming time:60.3289 s\n",
      "[epoch 5724]: training loss: 896.672119, consuming time:60.3072 s\n",
      "[epoch 5725]: training loss: 703.095825, consuming time:60.2413 s\n",
      "[epoch 5726]: training loss: 1002.058228, consuming time:60.4000 s\n",
      "[epoch 5727]: training loss: 1012.614868, consuming time:60.5640 s\n",
      "[epoch 5728]: training loss: 1008.057068, consuming time:60.5603 s\n",
      "[epoch 5729]: training loss: 775.414429, consuming time:60.3139 s\n",
      "[epoch 5730]: training loss: 1075.169434, consuming time:60.1093 s\n",
      "[epoch 5731]: training loss: 1074.219116, consuming time:60.5569 s\n",
      "[epoch 5732]: training loss: 911.725891, consuming time:60.2018 s\n",
      "[epoch 5733]: training loss: 842.840942, consuming time:60.6025 s\n",
      "[epoch 5734]: training loss: 945.305054, consuming time:60.6306 s\n",
      "[epoch 5735]: training loss: 1044.083496, consuming time:60.4563 s\n",
      "[epoch 5736]: training loss: 877.910767, consuming time:60.3661 s\n",
      "[epoch 5737]: training loss: 988.614258, consuming time:60.3784 s\n",
      "[epoch 5738]: training loss: 1093.108276, consuming time:60.6644 s\n",
      "[epoch 5739]: training loss: 1055.065796, consuming time:60.6058 s\n",
      "[epoch 5740]: training loss: 1111.626099, consuming time:60.8596 s\n",
      "[epoch 5741]: training loss: 960.674438, consuming time:60.5030 s\n",
      "[epoch 5742]: training loss: 952.337891, consuming time:60.3584 s\n",
      "[epoch 5743]: training loss: 933.224487, consuming time:60.2530 s\n",
      "[epoch 5744]: training loss: 1085.728027, consuming time:60.4355 s\n",
      "[epoch 5745]: training loss: 999.175903, consuming time:60.6410 s\n",
      "[epoch 5746]: training loss: 1156.316650, consuming time:60.5551 s\n",
      "[epoch 5747]: training loss: 940.391174, consuming time:60.5508 s\n",
      "[epoch 5748]: training loss: 882.170288, consuming time:60.0948 s\n",
      "[epoch 5749]: training loss: 924.337402, consuming time:60.5642 s\n",
      "[epoch 5750]: training loss: 910.450867, consuming time:63.7017 s\n",
      "[epoch 5751]: training loss: 590.015564, consuming time:61.0714 s\n",
      "[epoch 5752]: training loss: 755.896912, consuming time:60.4896 s\n",
      "[epoch 5753]: training loss: 1044.001221, consuming time:60.3453 s\n",
      "[epoch 5754]: training loss: 943.036987, consuming time:59.8692 s\n",
      "[epoch 5755]: training loss: 765.141968, consuming time:60.5647 s\n",
      "[epoch 5756]: training loss: 894.661926, consuming time:60.5896 s\n",
      "[epoch 5757]: training loss: 841.920166, consuming time:60.7303 s\n",
      "[epoch 5758]: training loss: 965.207642, consuming time:60.8608 s\n",
      "[epoch 5759]: training loss: 955.862915, consuming time:60.2796 s\n",
      "[epoch 5760]: training loss: 997.224915, consuming time:60.0505 s\n",
      "[epoch 5761]: training loss: 1165.573486, consuming time:60.5739 s\n",
      "[epoch 5762]: training loss: 919.565308, consuming time:60.3369 s\n",
      "[epoch 5763]: training loss: 893.252319, consuming time:60.7464 s\n",
      "[epoch 5764]: training loss: 899.301636, consuming time:60.8138 s\n",
      "[epoch 5765]: training loss: 923.733459, consuming time:60.8987 s\n",
      "[epoch 5766]: training loss: 854.317017, consuming time:60.2299 s\n",
      "[epoch 5767]: training loss: 752.113525, consuming time:60.3750 s\n",
      "[epoch 5768]: training loss: 936.386780, consuming time:60.8301 s\n",
      "[epoch 5769]: training loss: 843.605896, consuming time:60.9439 s\n",
      "[epoch 5770]: training loss: 1284.329224, consuming time:61.5383 s\n",
      "[epoch 5771]: training loss: 951.004761, consuming time:61.1892 s\n",
      "[epoch 5772]: training loss: 909.757568, consuming time:60.4618 s\n",
      "[epoch 5773]: training loss: 951.073730, consuming time:61.4719 s\n",
      "[epoch 5774]: training loss: 898.525513, consuming time:61.4104 s\n",
      "[epoch 5775]: training loss: 1124.416504, consuming time:61.3281 s\n",
      "[epoch 5776]: training loss: 1008.310303, consuming time:62.0883 s\n",
      "[epoch 5777]: training loss: 1084.638916, consuming time:61.7750 s\n",
      "[epoch 5778]: training loss: 1104.078857, consuming time:63.3945 s\n",
      "[epoch 5779]: training loss: 883.074463, consuming time:63.4591 s\n",
      "[epoch 5780]: training loss: 1008.498596, consuming time:65.9302 s\n",
      "[epoch 5781]: training loss: 844.436157, consuming time:64.2524 s\n",
      "[epoch 5782]: training loss: 739.536438, consuming time:61.4007 s\n",
      "[epoch 5783]: training loss: 1035.395142, consuming time:60.8375 s\n",
      "[epoch 5784]: training loss: 822.310730, consuming time:64.4033 s\n",
      "[epoch 5785]: training loss: 1035.628662, consuming time:64.3977 s\n",
      "[epoch 5786]: training loss: 1112.681396, consuming time:61.4280 s\n",
      "[epoch 5787]: training loss: 1022.553467, consuming time:64.6993 s\n",
      "[epoch 5788]: training loss: 1127.917847, consuming time:64.5769 s\n",
      "[epoch 5789]: training loss: 888.970215, consuming time:61.1057 s\n",
      "[epoch 5790]: training loss: 1027.348877, consuming time:63.2707 s\n",
      "[epoch 5791]: training loss: 829.493286, consuming time:64.4332 s\n",
      "[epoch 5792]: training loss: 847.717346, consuming time:64.4643 s\n",
      "[epoch 5793]: training loss: 1255.866455, consuming time:63.4864 s\n",
      "[epoch 5794]: training loss: 1121.693848, consuming time:62.5715 s\n",
      "[epoch 5795]: training loss: 964.333252, consuming time:62.5146 s\n",
      "[epoch 5796]: training loss: 1128.685059, consuming time:61.5610 s\n",
      "[epoch 5797]: training loss: 785.681641, consuming time:61.2471 s\n",
      "[epoch 5798]: training loss: 952.822754, consuming time:61.9135 s\n",
      "[epoch 5799]: training loss: 884.376526, consuming time:61.8070 s\n",
      "[epoch 5800]: training loss: 854.334473, consuming time:61.9188 s\n",
      "[epoch 5801]: training loss: 1209.723145, consuming time:61.3183 s\n",
      "[epoch 5802]: training loss: 745.879456, consuming time:61.6210 s\n",
      "[epoch 5803]: training loss: 1002.509338, consuming time:61.8190 s\n",
      "[epoch 5804]: training loss: 922.072266, consuming time:61.1909 s\n",
      "[epoch 5805]: training loss: 935.185791, consuming time:61.1582 s\n",
      "[epoch 5806]: training loss: 966.478394, consuming time:61.9842 s\n",
      "[epoch 5807]: training loss: 1127.101074, consuming time:61.9693 s\n",
      "[epoch 5808]: training loss: 961.356689, consuming time:60.9602 s\n",
      "[epoch 5809]: training loss: 1063.717773, consuming time:61.1811 s\n",
      "[epoch 5810]: training loss: 1033.367188, consuming time:61.2765 s\n",
      "[epoch 5811]: training loss: 919.527039, consuming time:61.1752 s\n",
      "[epoch 5812]: training loss: 842.395630, consuming time:61.0071 s\n",
      "[epoch 5813]: training loss: 984.529663, consuming time:60.9535 s\n",
      "[epoch 5814]: training loss: 658.814697, consuming time:60.9069 s\n",
      "[epoch 5815]: training loss: 1026.829346, consuming time:61.1253 s\n",
      "[epoch 5816]: training loss: 1338.494263, consuming time:61.3151 s\n",
      "[epoch 5817]: training loss: 836.365112, consuming time:61.1735 s\n",
      "[epoch 5818]: training loss: 1045.844482, consuming time:60.5202 s\n",
      "[epoch 5819]: training loss: 889.238586, consuming time:60.8662 s\n",
      "[epoch 5820]: training loss: 828.040344, consuming time:61.2156 s\n",
      "[epoch 5821]: training loss: 890.820251, consuming time:61.6030 s\n",
      "[epoch 5822]: training loss: 795.109131, consuming time:61.2896 s\n",
      "[epoch 5823]: training loss: 648.266479, consuming time:61.2183 s\n",
      "[epoch 5824]: training loss: 1358.102051, consuming time:60.8517 s\n",
      "[epoch 5825]: training loss: 1302.157104, consuming time:61.1417 s\n",
      "[epoch 5826]: training loss: 867.859314, consuming time:61.2551 s\n",
      "[epoch 5827]: training loss: 1050.476685, consuming time:61.2049 s\n",
      "[epoch 5828]: training loss: 1051.518921, consuming time:60.8630 s\n",
      "[epoch 5829]: training loss: 836.155273, consuming time:61.0733 s\n",
      "[epoch 5830]: training loss: 1002.600830, consuming time:61.2580 s\n",
      "[epoch 5831]: training loss: 764.127258, consuming time:61.0046 s\n",
      "[epoch 5832]: training loss: 983.198059, consuming time:61.0955 s\n",
      "[epoch 5833]: training loss: 943.425415, consuming time:61.2669 s\n",
      "[epoch 5834]: training loss: 974.976990, consuming time:60.8765 s\n",
      "[epoch 5835]: training loss: 936.501343, consuming time:61.6197 s\n",
      "[epoch 5836]: training loss: 727.060242, consuming time:61.0360 s\n",
      "[epoch 5837]: training loss: 727.750366, consuming time:60.9278 s\n",
      "[epoch 5838]: training loss: 936.420227, consuming time:61.2094 s\n",
      "[epoch 5839]: training loss: 1075.093018, consuming time:61.1223 s\n",
      "[epoch 5840]: training loss: 697.591187, consuming time:61.1246 s\n",
      "[epoch 5841]: training loss: 1088.514893, consuming time:61.0445 s\n",
      "[epoch 5842]: training loss: 891.761658, consuming time:61.0577 s\n",
      "[epoch 5843]: training loss: 1150.307861, consuming time:60.8845 s\n",
      "[epoch 5844]: training loss: 1140.044189, consuming time:61.3315 s\n",
      "[epoch 5845]: training loss: 1054.089478, consuming time:60.8790 s\n",
      "[epoch 5846]: training loss: 776.406250, consuming time:61.0159 s\n",
      "[epoch 5847]: training loss: 1028.830444, consuming time:60.9268 s\n",
      "[epoch 5848]: training loss: 1071.808960, consuming time:61.1768 s\n",
      "[epoch 5849]: training loss: 903.046204, consuming time:61.2310 s\n",
      "[epoch 5850]: training loss: 753.118530, consuming time:61.7750 s\n",
      "[epoch 5851]: training loss: 745.202271, consuming time:67.2189 s\n",
      "[epoch 5852]: training loss: 837.538330, consuming time:73.0160 s\n",
      "[epoch 5853]: training loss: 1433.885742, consuming time:72.7802 s\n",
      "[epoch 5854]: training loss: 946.580750, consuming time:94.0432 s\n",
      "[epoch 5855]: training loss: 1265.418701, consuming time:117.2222 s\n",
      "[epoch 5856]: training loss: 1034.316772, consuming time:113.7077 s\n",
      "[epoch 5857]: training loss: 933.036987, consuming time:114.2199 s\n",
      "[epoch 5858]: training loss: 1003.388306, consuming time:112.8309 s\n",
      "[epoch 5859]: training loss: 995.073669, consuming time:112.6878 s\n",
      "[epoch 5860]: training loss: 892.453247, consuming time:115.4751 s\n",
      "[epoch 5861]: training loss: 1046.126709, consuming time:117.2727 s\n",
      "[epoch 5862]: training loss: 1073.315674, consuming time:112.3171 s\n",
      "[epoch 5863]: training loss: 886.440674, consuming time:114.2404 s\n",
      "[epoch 5864]: training loss: 1229.250977, consuming time:110.7868 s\n",
      "[epoch 5865]: training loss: 829.865356, consuming time:113.5029 s\n",
      "[epoch 5866]: training loss: 825.870239, consuming time:112.5983 s\n",
      "[epoch 5867]: training loss: 786.275635, consuming time:88.1181 s\n",
      "[epoch 5868]: training loss: 800.371216, consuming time:71.9921 s\n",
      "[epoch 5869]: training loss: 853.179993, consuming time:121.2715 s\n",
      "[epoch 5870]: training loss: 979.532166, consuming time:119.0507 s\n",
      "[epoch 5871]: training loss: 838.643311, consuming time:116.9265 s\n",
      "[epoch 5872]: training loss: 889.706848, consuming time:123.3842 s\n",
      "[epoch 5873]: training loss: 816.316650, consuming time:121.0265 s\n",
      "[epoch 5874]: training loss: 956.618896, consuming time:118.8395 s\n",
      "[epoch 5875]: training loss: 936.318481, consuming time:118.8200 s\n",
      "[epoch 5876]: training loss: 1235.713623, consuming time:119.6658 s\n",
      "[epoch 5877]: training loss: 1068.328857, consuming time:73.9211 s\n",
      "[epoch 5878]: training loss: 854.424988, consuming time:70.4796 s\n",
      "[epoch 5879]: training loss: 932.575439, consuming time:66.6224 s\n",
      "[epoch 5880]: training loss: 810.778381, consuming time:66.3766 s\n",
      "[epoch 5881]: training loss: 693.963989, consuming time:113.4801 s\n",
      "[epoch 5882]: training loss: 1293.457275, consuming time:126.9501 s\n",
      "[epoch 5883]: training loss: 847.011963, consuming time:123.8741 s\n",
      "[epoch 5884]: training loss: 866.674133, consuming time:127.1410 s\n",
      "[epoch 5885]: training loss: 1109.353027, consuming time:126.7745 s\n",
      "[epoch 5886]: training loss: 968.472168, consuming time:120.1394 s\n",
      "[epoch 5887]: training loss: 603.638184, consuming time:121.6390 s\n",
      "[epoch 5888]: training loss: 795.491394, consuming time:126.2800 s\n",
      "[epoch 5889]: training loss: 754.703857, consuming time:126.7353 s\n",
      "[epoch 5890]: training loss: 885.246338, consuming time:95.2256 s\n",
      "[epoch 5891]: training loss: 972.930420, consuming time:72.9763 s\n",
      "[epoch 5892]: training loss: 866.136597, consuming time:70.0750 s\n",
      "[epoch 5893]: training loss: 880.808044, consuming time:94.1700 s\n",
      "[epoch 5894]: training loss: 883.620789, consuming time:132.9244 s\n",
      "[epoch 5895]: training loss: 946.943237, consuming time:131.2685 s\n",
      "[epoch 5896]: training loss: 869.393738, consuming time:138.1756 s\n",
      "[epoch 5897]: training loss: 769.606201, consuming time:135.2868 s\n",
      "[epoch 5898]: training loss: 879.364990, consuming time:129.4361 s\n",
      "[epoch 5899]: training loss: 889.136230, consuming time:142.3679 s\n",
      "[epoch 5900]: training loss: 946.658325, consuming time:127.8851 s\n",
      "[epoch 5901]: training loss: 846.741455, consuming time:73.9279 s\n",
      "[epoch 5902]: training loss: 712.692505, consuming time:70.4219 s\n",
      "[epoch 5903]: training loss: 802.022583, consuming time:129.2277 s\n",
      "[epoch 5904]: training loss: 1000.020386, consuming time:123.3841 s\n",
      "[epoch 5905]: training loss: 813.860962, consuming time:124.8867 s\n",
      "[epoch 5906]: training loss: 913.937927, consuming time:125.9171 s\n",
      "[epoch 5907]: training loss: 997.900085, consuming time:120.5485 s\n",
      "[epoch 5908]: training loss: 773.151550, consuming time:127.8317 s\n",
      "[epoch 5909]: training loss: 807.771729, consuming time:122.2342 s\n",
      "[epoch 5910]: training loss: 1054.898682, consuming time:122.6969 s\n",
      "[epoch 5911]: training loss: 1038.870728, consuming time:121.9605 s\n",
      "[epoch 5912]: training loss: 968.177368, consuming time:121.2342 s\n",
      "[epoch 5913]: training loss: 1148.317139, consuming time:70.4835 s\n",
      "[epoch 5914]: training loss: 747.511963, consuming time:76.4652 s\n",
      "[epoch 5915]: training loss: 1214.789551, consuming time:73.1956 s\n",
      "[epoch 5916]: training loss: 1026.049683, consuming time:99.3189 s\n",
      "[epoch 5917]: training loss: 1074.821167, consuming time:133.8287 s\n",
      "[epoch 5918]: training loss: 823.017517, consuming time:139.5195 s\n",
      "[epoch 5919]: training loss: 965.968567, consuming time:141.3831 s\n",
      "[epoch 5920]: training loss: 1276.084717, consuming time:135.0899 s\n",
      "[epoch 5921]: training loss: 1145.883911, consuming time:134.6962 s\n",
      "[epoch 5922]: training loss: 1335.955688, consuming time:136.4119 s\n",
      "[epoch 5923]: training loss: 845.822021, consuming time:137.2352 s\n",
      "[epoch 5924]: training loss: 710.493408, consuming time:139.0026 s\n",
      "[epoch 5925]: training loss: 1089.016846, consuming time:142.8409 s\n",
      "[epoch 5926]: training loss: 783.272461, consuming time:146.4885 s\n",
      "[epoch 5927]: training loss: 954.788086, consuming time:132.6811 s\n",
      "[epoch 5928]: training loss: 937.578613, consuming time:110.9041 s\n",
      "[epoch 5929]: training loss: 1106.152466, consuming time:73.2605 s\n",
      "[epoch 5930]: training loss: 1035.650269, consuming time:68.9973 s\n",
      "[epoch 5931]: training loss: 1103.613647, consuming time:68.2984 s\n",
      "[epoch 5932]: training loss: 795.222717, consuming time:123.6683 s\n",
      "[epoch 5933]: training loss: 948.781372, consuming time:131.0147 s\n",
      "[epoch 5934]: training loss: 1046.779297, consuming time:135.0991 s\n",
      "[epoch 5935]: training loss: 869.940002, consuming time:133.3940 s\n",
      "[epoch 5936]: training loss: 1155.772949, consuming time:132.0574 s\n",
      "[epoch 5937]: training loss: 1044.788696, consuming time:128.0792 s\n",
      "[epoch 5938]: training loss: 654.034851, consuming time:133.0841 s\n",
      "[epoch 5939]: training loss: 1176.315063, consuming time:128.9868 s\n",
      "[epoch 5940]: training loss: 1195.365112, consuming time:139.5023 s\n",
      "[epoch 5941]: training loss: 869.910156, consuming time:86.0546 s\n",
      "[epoch 5942]: training loss: 873.192505, consuming time:72.9651 s\n",
      "[epoch 5943]: training loss: 1050.007324, consuming time:139.6978 s\n",
      "[epoch 5944]: training loss: 998.188843, consuming time:141.9665 s\n",
      "[epoch 5945]: training loss: 599.461182, consuming time:145.8060 s\n",
      "[epoch 5946]: training loss: 1130.302246, consuming time:133.4616 s\n",
      "[epoch 5947]: training loss: 1313.447754, consuming time:138.0094 s\n",
      "[epoch 5948]: training loss: 865.133789, consuming time:133.7896 s\n",
      "[epoch 5949]: training loss: 1087.545288, consuming time:75.0650 s\n",
      "[epoch 5950]: training loss: 948.576904, consuming time:71.6707 s\n",
      "[epoch 5951]: training loss: 694.276733, consuming time:134.1792 s\n",
      "[epoch 5952]: training loss: 1039.244141, consuming time:135.5343 s\n",
      "[epoch 5953]: training loss: 1023.347961, consuming time:133.3108 s\n",
      "[epoch 5954]: training loss: 791.640259, consuming time:133.9941 s\n",
      "[epoch 5955]: training loss: 1113.649414, consuming time:127.9975 s\n",
      "[epoch 5956]: training loss: 1276.406982, consuming time:133.4936 s\n",
      "[epoch 5957]: training loss: 732.364197, consuming time:134.0123 s\n",
      "[epoch 5958]: training loss: 1268.775146, consuming time:134.2259 s\n",
      "[epoch 5959]: training loss: 781.628662, consuming time:140.7942 s\n",
      "[epoch 5960]: training loss: 789.109070, consuming time:86.1315 s\n",
      "[epoch 5961]: training loss: 1223.858398, consuming time:74.5728 s\n",
      "[epoch 5962]: training loss: 975.762634, consuming time:111.3335 s\n",
      "[epoch 5963]: training loss: 1171.924072, consuming time:124.8343 s\n",
      "[epoch 5964]: training loss: 773.077637, consuming time:129.4752 s\n",
      "[epoch 5965]: training loss: 1071.017822, consuming time:127.0688 s\n",
      "[epoch 5966]: training loss: 783.282288, consuming time:126.2788 s\n",
      "[epoch 5967]: training loss: 887.001038, consuming time:138.1434 s\n",
      "[epoch 5968]: training loss: 958.535156, consuming time:129.9245 s\n",
      "[epoch 5969]: training loss: 1102.609619, consuming time:135.4003 s\n",
      "[epoch 5970]: training loss: 988.682434, consuming time:123.3763 s\n",
      "[epoch 5971]: training loss: 1251.981079, consuming time:129.6397 s\n",
      "[epoch 5972]: training loss: 1191.845947, consuming time:95.4241 s\n",
      "[epoch 5973]: training loss: 954.208984, consuming time:72.5939 s\n",
      "[epoch 5974]: training loss: 1251.431396, consuming time:105.8955 s\n",
      "[epoch 5975]: training loss: 838.515198, consuming time:129.4526 s\n",
      "[epoch 5976]: training loss: 825.449036, consuming time:126.3075 s\n",
      "[epoch 5977]: training loss: 844.886169, consuming time:126.3412 s\n",
      "[epoch 5978]: training loss: 1021.950562, consuming time:129.0007 s\n",
      "[epoch 5979]: training loss: 1013.598511, consuming time:128.7485 s\n",
      "[epoch 5980]: training loss: 734.345642, consuming time:131.8970 s\n",
      "[epoch 5981]: training loss: 689.035156, consuming time:129.6094 s\n",
      "[epoch 5982]: training loss: 847.169556, consuming time:139.0351 s\n",
      "[epoch 5983]: training loss: 837.122375, consuming time:83.9490 s\n",
      "[epoch 5984]: training loss: 1159.997803, consuming time:72.0368 s\n",
      "[epoch 5985]: training loss: 825.896912, consuming time:68.7199 s\n",
      "[epoch 5986]: training loss: 1078.799438, consuming time:107.9758 s\n",
      "[epoch 5987]: training loss: 845.040222, consuming time:133.0976 s\n",
      "[epoch 5988]: training loss: 1009.897156, consuming time:136.0356 s\n",
      "[epoch 5989]: training loss: 707.417236, consuming time:129.1438 s\n",
      "[epoch 5990]: training loss: 839.175171, consuming time:128.7351 s\n",
      "[epoch 5991]: training loss: 767.225098, consuming time:133.5633 s\n",
      "[epoch 5992]: training loss: 754.456238, consuming time:92.7513 s\n",
      "[epoch 5993]: training loss: 887.102661, consuming time:62.0726 s\n",
      "[epoch 5994]: training loss: 807.421387, consuming time:62.1084 s\n",
      "[epoch 5995]: training loss: 1003.850708, consuming time:62.1004 s\n",
      "[epoch 5996]: training loss: 933.117432, consuming time:62.3301 s\n",
      "[epoch 5997]: training loss: 1021.130615, consuming time:62.3003 s\n",
      "[epoch 5998]: training loss: 1004.462158, consuming time:62.0510 s\n",
      "[epoch 5999]: training loss: 787.411987, consuming time:61.9835 s\n",
      "[epoch 6000]: training loss: 873.484375, consuming time:62.2155 s\n",
      "[epoch 6001]: training loss: 1135.005493, consuming time:62.2060 s\n",
      "[epoch 6002]: training loss: 863.049500, consuming time:62.4276 s\n",
      "[epoch 6003]: training loss: 879.988586, consuming time:62.4081 s\n",
      "[epoch 6004]: training loss: 1047.636597, consuming time:61.9257 s\n",
      "[epoch 6005]: training loss: 978.010254, consuming time:62.0567 s\n",
      "[epoch 6006]: training loss: 967.873779, consuming time:61.7977 s\n",
      "[epoch 6007]: training loss: 771.722168, consuming time:61.9411 s\n",
      "[epoch 6008]: training loss: 1162.848145, consuming time:62.0390 s\n",
      "[epoch 6009]: training loss: 1035.340210, consuming time:62.1667 s\n",
      "[epoch 6010]: training loss: 925.288025, consuming time:62.0876 s\n",
      "[epoch 6011]: training loss: 957.209595, consuming time:62.0000 s\n",
      "[epoch 6012]: training loss: 859.976318, consuming time:62.0411 s\n",
      "[epoch 6013]: training loss: 1250.572998, consuming time:61.8617 s\n",
      "[epoch 6014]: training loss: 932.721558, consuming time:62.1086 s\n",
      "[epoch 6015]: training loss: 743.318359, consuming time:62.0944 s\n",
      "[epoch 6016]: training loss: 904.540894, consuming time:61.9392 s\n",
      "[epoch 6017]: training loss: 1062.083374, consuming time:61.9895 s\n",
      "[epoch 6018]: training loss: 913.123291, consuming time:61.9740 s\n",
      "[epoch 6019]: training loss: 1251.760132, consuming time:61.9057 s\n",
      "[epoch 6020]: training loss: 848.619385, consuming time:62.0752 s\n",
      "[epoch 6021]: training loss: 831.264832, consuming time:62.1003 s\n",
      "[epoch 6022]: training loss: 794.589844, consuming time:61.9934 s\n",
      "[epoch 6023]: training loss: 999.120239, consuming time:61.9924 s\n",
      "[epoch 6024]: training loss: 846.008057, consuming time:62.0876 s\n",
      "[epoch 6025]: training loss: 1076.366211, consuming time:62.1902 s\n",
      "[epoch 6026]: training loss: 847.409668, consuming time:62.1567 s\n",
      "[epoch 6027]: training loss: 960.237000, consuming time:62.3044 s\n",
      "[epoch 6028]: training loss: 1203.859375, consuming time:61.9661 s\n",
      "[epoch 6029]: training loss: 821.822205, consuming time:62.0396 s\n",
      "[epoch 6030]: training loss: 900.146484, consuming time:62.1764 s\n",
      "[epoch 6031]: training loss: 832.422485, consuming time:62.2080 s\n",
      "[epoch 6032]: training loss: 974.558105, consuming time:62.3384 s\n",
      "[epoch 6033]: training loss: 936.977112, consuming time:62.1489 s\n",
      "[epoch 6034]: training loss: 874.825806, consuming time:62.1678 s\n",
      "[epoch 6035]: training loss: 1035.978638, consuming time:62.1800 s\n",
      "[epoch 6036]: training loss: 1057.673340, consuming time:62.1521 s\n",
      "[epoch 6037]: training loss: 1028.843628, consuming time:62.1673 s\n",
      "[epoch 6038]: training loss: 1221.442139, consuming time:62.1248 s\n",
      "[epoch 6039]: training loss: 821.263184, consuming time:61.9874 s\n",
      "[epoch 6040]: training loss: 768.591919, consuming time:62.0090 s\n",
      "[epoch 6041]: training loss: 812.787048, consuming time:61.9937 s\n",
      "[epoch 6042]: training loss: 1171.197754, consuming time:61.9829 s\n",
      "[epoch 6043]: training loss: 1128.836426, consuming time:62.0265 s\n",
      "[epoch 6044]: training loss: 911.440796, consuming time:62.2559 s\n",
      "[epoch 6045]: training loss: 931.583679, consuming time:62.0494 s\n",
      "[epoch 6046]: training loss: 992.064087, consuming time:61.9360 s\n",
      "[epoch 6047]: training loss: 1106.709473, consuming time:61.9076 s\n",
      "[epoch 6048]: training loss: 820.072998, consuming time:62.0429 s\n",
      "[epoch 6049]: training loss: 1150.183472, consuming time:61.9689 s\n",
      "[epoch 6050]: training loss: 716.821594, consuming time:62.0871 s\n",
      "[epoch 6051]: training loss: 1041.640503, consuming time:61.9625 s\n",
      "[epoch 6052]: training loss: 858.025635, consuming time:62.0016 s\n",
      "[epoch 6053]: training loss: 1019.370728, consuming time:61.9935 s\n",
      "[epoch 6054]: training loss: 1021.090088, consuming time:62.0727 s\n",
      "[epoch 6055]: training loss: 921.409302, consuming time:62.0806 s\n",
      "[epoch 6056]: training loss: 987.474731, consuming time:61.8653 s\n",
      "[epoch 6057]: training loss: 1068.848145, consuming time:61.9232 s\n",
      "[epoch 6058]: training loss: 968.792114, consuming time:61.9205 s\n",
      "[epoch 6059]: training loss: 984.701233, consuming time:61.8763 s\n",
      "[epoch 6060]: training loss: 1143.429199, consuming time:61.9323 s\n",
      "[epoch 6061]: training loss: 980.509644, consuming time:62.0899 s\n",
      "[epoch 6062]: training loss: 756.298584, consuming time:61.9024 s\n",
      "[epoch 6063]: training loss: 1279.277832, consuming time:62.0325 s\n",
      "[epoch 6064]: training loss: 829.848206, consuming time:61.9164 s\n",
      "[epoch 6065]: training loss: 672.557312, consuming time:62.0160 s\n",
      "[epoch 6066]: training loss: 964.055054, consuming time:62.0037 s\n",
      "[epoch 6067]: training loss: 1037.764771, consuming time:62.3808 s\n",
      "[epoch 6068]: training loss: 761.276001, consuming time:61.9024 s\n",
      "[epoch 6069]: training loss: 1021.464294, consuming time:61.9427 s\n",
      "[epoch 6070]: training loss: 1047.488647, consuming time:62.0638 s\n",
      "[epoch 6071]: training loss: 737.785461, consuming time:61.8803 s\n",
      "[epoch 6072]: training loss: 951.682678, consuming time:62.1122 s\n",
      "[epoch 6073]: training loss: 918.223267, consuming time:62.0579 s\n",
      "[epoch 6074]: training loss: 774.748169, consuming time:61.9972 s\n",
      "[epoch 6075]: training loss: 799.837402, consuming time:62.0407 s\n",
      "[epoch 6076]: training loss: 669.985596, consuming time:61.8597 s\n",
      "[epoch 6077]: training loss: 932.327087, consuming time:61.8500 s\n",
      "[epoch 6078]: training loss: 913.170776, consuming time:62.1166 s\n",
      "[epoch 6079]: training loss: 893.982666, consuming time:62.0530 s\n",
      "[epoch 6080]: training loss: 1033.381714, consuming time:61.9137 s\n",
      "[epoch 6081]: training loss: 974.982849, consuming time:62.0529 s\n",
      "[epoch 6082]: training loss: 869.124573, consuming time:61.8665 s\n",
      "[epoch 6083]: training loss: 1043.795654, consuming time:62.0789 s\n",
      "[epoch 6084]: training loss: 820.457764, consuming time:62.1598 s\n",
      "[epoch 6085]: training loss: 1028.180908, consuming time:62.0063 s\n",
      "[epoch 6086]: training loss: 923.352051, consuming time:61.9669 s\n",
      "[epoch 6087]: training loss: 929.403259, consuming time:61.9588 s\n",
      "[epoch 6088]: training loss: 917.215088, consuming time:61.9248 s\n",
      "[epoch 6089]: training loss: 921.941223, consuming time:61.8132 s\n",
      "[epoch 6090]: training loss: 849.761719, consuming time:62.0463 s\n",
      "[epoch 6091]: training loss: 947.154297, consuming time:61.9178 s\n",
      "[epoch 6092]: training loss: 1259.799805, consuming time:61.9900 s\n",
      "[epoch 6093]: training loss: 1081.356201, consuming time:61.9343 s\n",
      "[epoch 6094]: training loss: 978.185730, consuming time:61.9536 s\n",
      "[epoch 6095]: training loss: 769.965393, consuming time:61.8644 s\n",
      "[epoch 6096]: training loss: 896.656860, consuming time:61.9948 s\n",
      "[epoch 6097]: training loss: 1235.000244, consuming time:61.8861 s\n",
      "[epoch 6098]: training loss: 976.949463, consuming time:62.0514 s\n",
      "[epoch 6099]: training loss: 834.605774, consuming time:61.8599 s\n",
      "[epoch 6100]: training loss: 1077.838257, consuming time:61.8452 s\n",
      "[epoch 6101]: training loss: 972.051697, consuming time:61.9133 s\n",
      "[epoch 6102]: training loss: 774.393738, consuming time:62.0821 s\n",
      "[epoch 6103]: training loss: 827.261475, consuming time:61.8558 s\n",
      "[epoch 6104]: training loss: 1006.280334, consuming time:61.7627 s\n",
      "[epoch 6105]: training loss: 827.520508, consuming time:61.9011 s\n",
      "[epoch 6106]: training loss: 990.767395, consuming time:62.0604 s\n",
      "[epoch 6107]: training loss: 820.321960, consuming time:61.9964 s\n",
      "[epoch 6108]: training loss: 717.602539, consuming time:61.9772 s\n",
      "[epoch 6109]: training loss: 789.651428, consuming time:62.0762 s\n",
      "[epoch 6110]: training loss: 1180.328979, consuming time:61.9853 s\n",
      "[epoch 6111]: training loss: 738.148499, consuming time:62.0105 s\n",
      "[epoch 6112]: training loss: 736.337097, consuming time:61.9882 s\n",
      "[epoch 6113]: training loss: 705.204590, consuming time:62.2208 s\n",
      "[epoch 6114]: training loss: 899.419922, consuming time:62.2177 s\n",
      "[epoch 6115]: training loss: 845.457458, consuming time:62.0046 s\n",
      "[epoch 6116]: training loss: 799.641907, consuming time:61.9897 s\n",
      "[epoch 6117]: training loss: 798.497253, consuming time:61.9993 s\n",
      "[epoch 6118]: training loss: 1025.860229, consuming time:61.9100 s\n",
      "[epoch 6119]: training loss: 1266.671875, consuming time:62.1446 s\n",
      "[epoch 6120]: training loss: 1014.679688, consuming time:62.1285 s\n",
      "[epoch 6121]: training loss: 715.431702, consuming time:61.9087 s\n",
      "[epoch 6122]: training loss: 1031.723877, consuming time:61.9406 s\n",
      "[epoch 6123]: training loss: 867.203735, consuming time:62.0556 s\n",
      "[epoch 6124]: training loss: 1086.366333, consuming time:61.9672 s\n",
      "[epoch 6125]: training loss: 735.392334, consuming time:62.0156 s\n",
      "[epoch 6126]: training loss: 1106.767700, consuming time:61.8394 s\n",
      "[epoch 6127]: training loss: 928.039368, consuming time:61.9895 s\n",
      "[epoch 6128]: training loss: 1080.301758, consuming time:61.9820 s\n",
      "[epoch 6129]: training loss: 852.249451, consuming time:62.1001 s\n",
      "[epoch 6130]: training loss: 1048.753174, consuming time:61.9308 s\n",
      "[epoch 6131]: training loss: 747.183228, consuming time:62.1397 s\n",
      "[epoch 6132]: training loss: 993.376587, consuming time:62.0593 s\n",
      "[epoch 6133]: training loss: 970.643066, consuming time:61.8565 s\n",
      "[epoch 6134]: training loss: 691.696899, consuming time:62.0908 s\n",
      "[epoch 6135]: training loss: 735.886230, consuming time:61.8951 s\n",
      "[epoch 6136]: training loss: 967.384216, consuming time:62.0519 s\n",
      "[epoch 6137]: training loss: 933.538086, consuming time:62.0536 s\n",
      "[epoch 6138]: training loss: 887.950439, consuming time:62.1475 s\n",
      "[epoch 6139]: training loss: 1160.264648, consuming time:62.1435 s\n",
      "[epoch 6140]: training loss: 937.338989, consuming time:61.8758 s\n",
      "[epoch 6141]: training loss: 1211.853271, consuming time:62.0040 s\n",
      "[epoch 6142]: training loss: 1013.033142, consuming time:62.0403 s\n",
      "[epoch 6143]: training loss: 1147.008057, consuming time:62.0071 s\n",
      "[epoch 6144]: training loss: 798.625793, consuming time:62.0167 s\n",
      "[epoch 6145]: training loss: 1148.329224, consuming time:61.7809 s\n",
      "[epoch 6146]: training loss: 695.772522, consuming time:61.9576 s\n",
      "[epoch 6147]: training loss: 1050.491211, consuming time:61.8618 s\n",
      "[epoch 6148]: training loss: 628.312317, consuming time:62.1644 s\n",
      "[epoch 6149]: training loss: 823.360352, consuming time:61.9484 s\n",
      "[epoch 6150]: training loss: 1012.780029, consuming time:61.9703 s\n",
      "[epoch 6151]: training loss: 988.135803, consuming time:61.9972 s\n",
      "[epoch 6152]: training loss: 707.330566, consuming time:62.0714 s\n",
      "[epoch 6153]: training loss: 1104.146484, consuming time:61.8872 s\n",
      "[epoch 6154]: training loss: 1029.658081, consuming time:62.0744 s\n",
      "[epoch 6155]: training loss: 1048.437378, consuming time:61.9957 s\n",
      "[epoch 6156]: training loss: 943.358337, consuming time:61.9018 s\n",
      "[epoch 6157]: training loss: 956.751221, consuming time:61.9663 s\n",
      "[epoch 6158]: training loss: 894.314575, consuming time:62.0313 s\n",
      "[epoch 6159]: training loss: 749.885742, consuming time:62.0785 s\n",
      "[epoch 6160]: training loss: 1098.752197, consuming time:62.0493 s\n",
      "[epoch 6161]: training loss: 1073.808350, consuming time:62.2270 s\n",
      "[epoch 6162]: training loss: 1229.524902, consuming time:62.1359 s\n",
      "[epoch 6163]: training loss: 944.267395, consuming time:61.8378 s\n",
      "[epoch 6164]: training loss: 995.316711, consuming time:62.1360 s\n",
      "[epoch 6165]: training loss: 1162.715332, consuming time:62.1984 s\n",
      "[epoch 6166]: training loss: 884.103516, consuming time:62.3285 s\n",
      "[epoch 6167]: training loss: 972.674988, consuming time:61.9771 s\n",
      "[epoch 6168]: training loss: 948.847534, consuming time:61.9830 s\n",
      "[epoch 6169]: training loss: 795.278442, consuming time:61.9218 s\n",
      "[epoch 6170]: training loss: 762.363281, consuming time:62.0096 s\n",
      "[epoch 6171]: training loss: 1226.109741, consuming time:62.0257 s\n",
      "[epoch 6172]: training loss: 822.911560, consuming time:62.0220 s\n",
      "[epoch 6173]: training loss: 984.756836, consuming time:61.7485 s\n",
      "[epoch 6174]: training loss: 974.717468, consuming time:61.9539 s\n",
      "[epoch 6175]: training loss: 904.403320, consuming time:62.0637 s\n",
      "[epoch 6176]: training loss: 886.394653, consuming time:61.9779 s\n",
      "[epoch 6177]: training loss: 1028.869385, consuming time:62.1815 s\n",
      "[epoch 6178]: training loss: 860.032227, consuming time:62.0183 s\n",
      "[epoch 6179]: training loss: 757.136414, consuming time:62.0029 s\n",
      "[epoch 6180]: training loss: 891.802734, consuming time:62.0132 s\n",
      "[epoch 6181]: training loss: 820.002625, consuming time:61.8327 s\n",
      "[epoch 6182]: training loss: 935.150513, consuming time:61.9930 s\n",
      "[epoch 6183]: training loss: 1058.313477, consuming time:62.1258 s\n",
      "[epoch 6184]: training loss: 1021.528381, consuming time:61.7501 s\n",
      "[epoch 6185]: training loss: 985.524780, consuming time:61.9481 s\n",
      "[epoch 6186]: training loss: 734.422241, consuming time:61.9303 s\n",
      "[epoch 6187]: training loss: 870.067322, consuming time:62.0539 s\n",
      "[epoch 6188]: training loss: 1102.094971, consuming time:61.9527 s\n",
      "[epoch 6189]: training loss: 1015.596252, consuming time:61.8162 s\n",
      "[epoch 6190]: training loss: 1160.975098, consuming time:62.0106 s\n",
      "[epoch 6191]: training loss: 872.633728, consuming time:61.9669 s\n",
      "[epoch 6192]: training loss: 937.795471, consuming time:61.8842 s\n",
      "[epoch 6193]: training loss: 1102.617676, consuming time:62.0986 s\n",
      "[epoch 6194]: training loss: 935.889160, consuming time:61.8772 s\n",
      "[epoch 6195]: training loss: 912.398865, consuming time:62.0264 s\n",
      "[epoch 6196]: training loss: 717.454834, consuming time:62.1084 s\n",
      "[epoch 6197]: training loss: 808.060059, consuming time:62.0470 s\n",
      "[epoch 6198]: training loss: 1000.818787, consuming time:61.8929 s\n",
      "[epoch 6199]: training loss: 1041.315796, consuming time:62.0129 s\n",
      "[epoch 6200]: training loss: 759.842896, consuming time:61.9940 s\n",
      "[epoch 6201]: training loss: 894.304932, consuming time:62.0133 s\n",
      "[epoch 6202]: training loss: 939.064636, consuming time:61.7809 s\n",
      "[epoch 6203]: training loss: 816.673523, consuming time:61.7990 s\n",
      "[epoch 6204]: training loss: 825.772339, consuming time:61.9045 s\n",
      "[epoch 6205]: training loss: 983.157593, consuming time:61.9363 s\n",
      "[epoch 6206]: training loss: 971.591125, consuming time:61.9893 s\n",
      "[epoch 6207]: training loss: 1078.887207, consuming time:61.9326 s\n",
      "[epoch 6208]: training loss: 736.607788, consuming time:61.9661 s\n",
      "[epoch 6209]: training loss: 933.035034, consuming time:62.0057 s\n",
      "[epoch 6210]: training loss: 999.979980, consuming time:61.9388 s\n",
      "[epoch 6211]: training loss: 1032.712036, consuming time:61.8050 s\n",
      "[epoch 6212]: training loss: 752.062378, consuming time:62.0276 s\n",
      "[epoch 6213]: training loss: 859.278809, consuming time:61.9340 s\n",
      "[epoch 6214]: training loss: 895.534058, consuming time:61.9737 s\n",
      "[epoch 6215]: training loss: 805.633179, consuming time:61.9848 s\n",
      "[epoch 6216]: training loss: 700.646851, consuming time:61.8318 s\n",
      "[epoch 6217]: training loss: 719.660400, consuming time:61.8514 s\n",
      "[epoch 6218]: training loss: 965.429565, consuming time:62.1182 s\n",
      "[epoch 6219]: training loss: 1126.236450, consuming time:61.8955 s\n",
      "[epoch 6220]: training loss: 1038.122803, consuming time:61.9772 s\n",
      "[epoch 6221]: training loss: 1035.138550, consuming time:62.0139 s\n",
      "[epoch 6222]: training loss: 901.836548, consuming time:62.0092 s\n",
      "[epoch 6223]: training loss: 976.411621, consuming time:61.9701 s\n",
      "[epoch 6224]: training loss: 1032.916748, consuming time:62.0856 s\n",
      "[epoch 6225]: training loss: 1004.992554, consuming time:61.7404 s\n",
      "[epoch 6226]: training loss: 1161.443115, consuming time:61.9931 s\n",
      "[epoch 6227]: training loss: 882.476440, consuming time:61.8278 s\n",
      "[epoch 6228]: training loss: 1014.270447, consuming time:62.0516 s\n",
      "[epoch 6229]: training loss: 939.572388, consuming time:61.9490 s\n",
      "[epoch 6230]: training loss: 916.115479, consuming time:61.9937 s\n",
      "[epoch 6231]: training loss: 880.105347, consuming time:61.8014 s\n",
      "[epoch 6232]: training loss: 717.502808, consuming time:61.8828 s\n",
      "[epoch 6233]: training loss: 1034.848877, consuming time:61.9602 s\n",
      "[epoch 6234]: training loss: 785.052490, consuming time:62.0438 s\n",
      "[epoch 6235]: training loss: 829.801758, consuming time:61.9290 s\n",
      "[epoch 6236]: training loss: 786.278320, consuming time:61.9920 s\n",
      "[epoch 6237]: training loss: 696.374878, consuming time:61.9061 s\n",
      "[epoch 6238]: training loss: 984.541382, consuming time:61.8303 s\n",
      "[epoch 6239]: training loss: 804.308350, consuming time:61.9584 s\n",
      "[epoch 6240]: training loss: 832.564697, consuming time:62.0899 s\n",
      "[epoch 6241]: training loss: 914.829651, consuming time:61.9127 s\n",
      "[epoch 6242]: training loss: 823.855042, consuming time:61.9935 s\n",
      "[epoch 6243]: training loss: 967.582397, consuming time:61.9597 s\n",
      "[epoch 6244]: training loss: 921.786987, consuming time:61.8741 s\n",
      "[epoch 6245]: training loss: 1108.454102, consuming time:61.8260 s\n",
      "[epoch 6246]: training loss: 836.921753, consuming time:62.0493 s\n",
      "[epoch 6247]: training loss: 947.240784, consuming time:62.0324 s\n",
      "[epoch 6248]: training loss: 937.611145, consuming time:61.8644 s\n",
      "[epoch 6249]: training loss: 996.325012, consuming time:61.9908 s\n",
      "[epoch 6250]: training loss: 1042.722778, consuming time:62.1419 s\n",
      "[epoch 6251]: training loss: 1126.681641, consuming time:61.8111 s\n",
      "[epoch 6252]: training loss: 1022.634216, consuming time:62.0801 s\n",
      "[epoch 6253]: training loss: 1019.093872, consuming time:62.0269 s\n",
      "[epoch 6254]: training loss: 1138.882202, consuming time:62.0205 s\n",
      "[epoch 6255]: training loss: 816.073181, consuming time:61.9995 s\n",
      "[epoch 6256]: training loss: 1059.243164, consuming time:61.9924 s\n",
      "[epoch 6257]: training loss: 926.860962, consuming time:61.8375 s\n",
      "[epoch 6258]: training loss: 903.960022, consuming time:62.0284 s\n",
      "[epoch 6259]: training loss: 1012.062866, consuming time:62.0540 s\n",
      "[epoch 6260]: training loss: 1150.280396, consuming time:62.0440 s\n",
      "[epoch 6261]: training loss: 971.718506, consuming time:61.8912 s\n",
      "[epoch 6262]: training loss: 829.323486, consuming time:61.8470 s\n",
      "[epoch 6263]: training loss: 876.913818, consuming time:61.9793 s\n",
      "[epoch 6264]: training loss: 853.962708, consuming time:61.9408 s\n",
      "[epoch 6265]: training loss: 944.567261, consuming time:62.0507 s\n",
      "[epoch 6266]: training loss: 1034.747559, consuming time:61.9007 s\n",
      "[epoch 6267]: training loss: 1074.795532, consuming time:61.8771 s\n",
      "[epoch 6268]: training loss: 890.358521, consuming time:62.0495 s\n",
      "[epoch 6269]: training loss: 857.756470, consuming time:61.8737 s\n",
      "[epoch 6270]: training loss: 1030.672241, consuming time:61.9781 s\n",
      "[epoch 6271]: training loss: 1099.305542, consuming time:62.0171 s\n",
      "[epoch 6272]: training loss: 911.073059, consuming time:61.8320 s\n",
      "[epoch 6273]: training loss: 1121.755371, consuming time:61.9503 s\n",
      "[epoch 6274]: training loss: 917.777954, consuming time:61.8458 s\n",
      "[epoch 6275]: training loss: 893.509338, consuming time:61.9818 s\n",
      "[epoch 6276]: training loss: 1018.055420, consuming time:62.0025 s\n",
      "[epoch 6277]: training loss: 1061.278809, consuming time:61.9015 s\n",
      "[epoch 6278]: training loss: 1041.663086, consuming time:62.0320 s\n",
      "[epoch 6279]: training loss: 995.799072, consuming time:61.8802 s\n",
      "[epoch 6280]: training loss: 810.402039, consuming time:61.9925 s\n",
      "[epoch 6281]: training loss: 877.764832, consuming time:62.0162 s\n",
      "[epoch 6282]: training loss: 1118.486084, consuming time:62.0313 s\n",
      "[epoch 6283]: training loss: 885.280518, consuming time:61.9871 s\n",
      "[epoch 6284]: training loss: 1087.211182, consuming time:61.8750 s\n",
      "[epoch 6285]: training loss: 624.348633, consuming time:61.8368 s\n",
      "[epoch 6286]: training loss: 909.737122, consuming time:62.0117 s\n",
      "[epoch 6287]: training loss: 937.774048, consuming time:61.9497 s\n",
      "[epoch 6288]: training loss: 1131.106323, consuming time:62.0425 s\n",
      "[epoch 6289]: training loss: 991.924194, consuming time:61.7837 s\n",
      "[epoch 6290]: training loss: 890.048706, consuming time:61.7650 s\n",
      "[epoch 6291]: training loss: 1008.500793, consuming time:61.7844 s\n",
      "[epoch 6292]: training loss: 1218.417480, consuming time:61.8741 s\n",
      "[epoch 6293]: training loss: 1165.259766, consuming time:62.1265 s\n",
      "[epoch 6294]: training loss: 1082.893311, consuming time:62.1030 s\n",
      "[epoch 6295]: training loss: 1141.136841, consuming time:61.9728 s\n",
      "[epoch 6296]: training loss: 1180.500000, consuming time:61.8877 s\n",
      "[epoch 6297]: training loss: 1115.175537, consuming time:62.0509 s\n",
      "[epoch 6298]: training loss: 1048.703857, consuming time:62.1514 s\n",
      "[epoch 6299]: training loss: 874.656250, consuming time:61.9621 s\n",
      "[epoch 6300]: training loss: 920.608765, consuming time:61.9303 s\n",
      "[epoch 6301]: training loss: 975.243408, consuming time:62.0547 s\n",
      "[epoch 6302]: training loss: 874.646423, consuming time:62.0251 s\n",
      "[epoch 6303]: training loss: 786.257446, consuming time:61.8604 s\n",
      "[epoch 6304]: training loss: 881.512390, consuming time:62.0648 s\n",
      "[epoch 6305]: training loss: 951.342407, consuming time:61.9617 s\n",
      "[epoch 6306]: training loss: 1160.970337, consuming time:61.9454 s\n",
      "[epoch 6307]: training loss: 963.536377, consuming time:62.0966 s\n",
      "[epoch 6308]: training loss: 1034.448730, consuming time:62.0152 s\n",
      "[epoch 6309]: training loss: 866.167847, consuming time:62.0284 s\n",
      "[epoch 6310]: training loss: 761.755554, consuming time:61.9089 s\n",
      "[epoch 6311]: training loss: 1122.425171, consuming time:62.0278 s\n",
      "[epoch 6312]: training loss: 954.023315, consuming time:62.0360 s\n",
      "[epoch 6313]: training loss: 1162.439209, consuming time:61.8825 s\n",
      "[epoch 6314]: training loss: 1001.243225, consuming time:61.9383 s\n",
      "[epoch 6315]: training loss: 1095.437744, consuming time:61.9006 s\n",
      "[epoch 6316]: training loss: 878.741699, consuming time:62.0164 s\n",
      "[epoch 6317]: training loss: 999.149658, consuming time:61.9330 s\n",
      "[epoch 6318]: training loss: 891.364685, consuming time:62.0222 s\n",
      "[epoch 6319]: training loss: 851.047302, consuming time:61.8187 s\n",
      "[epoch 6320]: training loss: 980.490479, consuming time:61.8449 s\n",
      "[epoch 6321]: training loss: 757.889404, consuming time:61.9001 s\n",
      "[epoch 6322]: training loss: 944.731140, consuming time:62.0388 s\n",
      "[epoch 6323]: training loss: 1088.727295, consuming time:61.9558 s\n",
      "[epoch 6324]: training loss: 1045.001343, consuming time:62.0731 s\n",
      "[epoch 6325]: training loss: 1037.223877, consuming time:61.8116 s\n",
      "[epoch 6326]: training loss: 1141.485352, consuming time:61.8833 s\n",
      "[epoch 6327]: training loss: 843.629700, consuming time:61.9540 s\n",
      "[epoch 6328]: training loss: 1164.436768, consuming time:62.0715 s\n",
      "[epoch 6329]: training loss: 1094.417969, consuming time:62.1598 s\n",
      "[epoch 6330]: training loss: 964.020508, consuming time:61.9518 s\n",
      "[epoch 6331]: training loss: 1155.882812, consuming time:61.8746 s\n",
      "[epoch 6332]: training loss: 1157.233643, consuming time:61.8951 s\n",
      "[epoch 6333]: training loss: 1030.130859, consuming time:61.9440 s\n",
      "[epoch 6334]: training loss: 1137.743652, consuming time:62.0674 s\n",
      "[epoch 6335]: training loss: 839.641357, consuming time:61.8045 s\n",
      "[epoch 6336]: training loss: 784.746826, consuming time:62.0797 s\n",
      "[epoch 6337]: training loss: 1028.336670, consuming time:61.9303 s\n",
      "[epoch 6338]: training loss: 978.835083, consuming time:62.0726 s\n",
      "[epoch 6339]: training loss: 957.306946, consuming time:62.2212 s\n",
      "[epoch 6340]: training loss: 991.038269, consuming time:62.1505 s\n",
      "[epoch 6341]: training loss: 960.227722, consuming time:61.9636 s\n",
      "[epoch 6342]: training loss: 882.376099, consuming time:62.0064 s\n",
      "[epoch 6343]: training loss: 762.317749, consuming time:62.1068 s\n",
      "[epoch 6344]: training loss: 758.968628, consuming time:61.9478 s\n",
      "[epoch 6345]: training loss: 872.675049, consuming time:61.9255 s\n",
      "[epoch 6346]: training loss: 907.340942, consuming time:62.0547 s\n",
      "[epoch 6347]: training loss: 1064.077759, consuming time:61.8893 s\n",
      "[epoch 6348]: training loss: 994.617371, consuming time:61.8069 s\n",
      "[epoch 6349]: training loss: 1049.375000, consuming time:62.0711 s\n",
      "[epoch 6350]: training loss: 787.368286, consuming time:61.8560 s\n",
      "[epoch 6351]: training loss: 731.793335, consuming time:62.0170 s\n",
      "[epoch 6352]: training loss: 660.466736, consuming time:61.8498 s\n",
      "[epoch 6353]: training loss: 1392.140869, consuming time:61.9021 s\n",
      "[epoch 6354]: training loss: 977.014648, consuming time:61.9268 s\n",
      "[epoch 6355]: training loss: 1111.073730, consuming time:61.8484 s\n",
      "[epoch 6356]: training loss: 767.071350, consuming time:61.9115 s\n",
      "[epoch 6357]: training loss: 1116.354004, consuming time:62.0684 s\n",
      "[epoch 6358]: training loss: 917.450439, consuming time:62.1577 s\n",
      "[epoch 6359]: training loss: 1013.352844, consuming time:61.8744 s\n",
      "[epoch 6360]: training loss: 718.782898, consuming time:61.7997 s\n",
      "[epoch 6361]: training loss: 885.493958, consuming time:62.0179 s\n",
      "[epoch 6362]: training loss: 1027.548828, consuming time:61.8732 s\n",
      "[epoch 6363]: training loss: 936.789185, consuming time:62.0146 s\n",
      "[epoch 6364]: training loss: 870.837769, consuming time:62.0908 s\n",
      "[epoch 6365]: training loss: 651.518188, consuming time:61.8054 s\n",
      "[epoch 6366]: training loss: 741.480713, consuming time:61.9818 s\n",
      "[epoch 6367]: training loss: 965.526123, consuming time:61.8048 s\n",
      "[epoch 6368]: training loss: 1263.736328, consuming time:61.9837 s\n",
      "[epoch 6369]: training loss: 835.121948, consuming time:62.1569 s\n",
      "[epoch 6370]: training loss: 1183.722412, consuming time:61.8406 s\n",
      "[epoch 6371]: training loss: 814.467529, consuming time:62.0285 s\n",
      "[epoch 6372]: training loss: 1090.940674, consuming time:61.8943 s\n",
      "[epoch 6373]: training loss: 945.042725, consuming time:61.8501 s\n",
      "[epoch 6374]: training loss: 756.404236, consuming time:61.9778 s\n",
      "[epoch 6375]: training loss: 976.896851, consuming time:61.9966 s\n",
      "[epoch 6376]: training loss: 1242.763550, consuming time:62.0412 s\n",
      "[epoch 6377]: training loss: 1143.056641, consuming time:62.0497 s\n",
      "[epoch 6378]: training loss: 902.006897, consuming time:61.8554 s\n",
      "[epoch 6379]: training loss: 1068.931274, consuming time:61.9290 s\n",
      "[epoch 6380]: training loss: 977.855957, consuming time:61.9634 s\n",
      "[epoch 6381]: training loss: 931.048584, consuming time:61.9771 s\n",
      "[epoch 6382]: training loss: 828.092834, consuming time:61.9667 s\n",
      "[epoch 6383]: training loss: 783.735718, consuming time:62.0083 s\n",
      "[epoch 6384]: training loss: 809.497925, consuming time:61.9798 s\n",
      "[epoch 6385]: training loss: 995.387756, consuming time:61.9077 s\n",
      "[epoch 6386]: training loss: 1086.875732, consuming time:62.1716 s\n",
      "[epoch 6387]: training loss: 857.921753, consuming time:62.0922 s\n",
      "[epoch 6388]: training loss: 1083.723877, consuming time:61.9387 s\n",
      "[epoch 6389]: training loss: 912.114502, consuming time:61.9705 s\n",
      "[epoch 6390]: training loss: 1011.977173, consuming time:61.9537 s\n",
      "[epoch 6391]: training loss: 805.857056, consuming time:61.9074 s\n",
      "[epoch 6392]: training loss: 967.948303, consuming time:61.9504 s\n",
      "[epoch 6393]: training loss: 651.394653, consuming time:61.9169 s\n",
      "[epoch 6394]: training loss: 692.850281, consuming time:62.0126 s\n",
      "[epoch 6395]: training loss: 829.039307, consuming time:61.8450 s\n",
      "[epoch 6396]: training loss: 870.011169, consuming time:61.9389 s\n",
      "[epoch 6397]: training loss: 1077.702515, consuming time:61.9447 s\n",
      "[epoch 6398]: training loss: 866.506592, consuming time:62.0288 s\n",
      "[epoch 6399]: training loss: 983.478760, consuming time:61.6821 s\n",
      "[epoch 6400]: training loss: 816.984985, consuming time:61.9092 s\n",
      "[epoch 6401]: training loss: 1408.899414, consuming time:61.9574 s\n",
      "[epoch 6402]: training loss: 1155.717773, consuming time:61.8053 s\n",
      "[epoch 6403]: training loss: 655.933105, consuming time:62.0656 s\n",
      "[epoch 6404]: training loss: 1122.470215, consuming time:61.7861 s\n",
      "[epoch 6405]: training loss: 1150.684692, consuming time:61.7696 s\n",
      "[epoch 6406]: training loss: 954.860840, consuming time:61.8670 s\n",
      "[epoch 6407]: training loss: 861.682617, consuming time:62.1154 s\n",
      "[epoch 6408]: training loss: 1020.545715, consuming time:61.8551 s\n",
      "[epoch 6409]: training loss: 1082.030029, consuming time:61.9400 s\n",
      "[epoch 6410]: training loss: 881.644287, consuming time:62.0258 s\n",
      "[epoch 6411]: training loss: 1028.889160, consuming time:61.8263 s\n",
      "[epoch 6412]: training loss: 1010.660400, consuming time:61.9302 s\n",
      "[epoch 6413]: training loss: 950.280762, consuming time:61.8977 s\n",
      "[epoch 6414]: training loss: 1248.749756, consuming time:61.9088 s\n",
      "[epoch 6415]: training loss: 826.600952, consuming time:62.1098 s\n",
      "[epoch 6416]: training loss: 837.346497, consuming time:62.0173 s\n",
      "[epoch 6417]: training loss: 821.140991, consuming time:61.8548 s\n",
      "[epoch 6418]: training loss: 803.618347, consuming time:61.8342 s\n",
      "[epoch 6419]: training loss: 903.029907, consuming time:62.0304 s\n",
      "[epoch 6420]: training loss: 1033.506104, consuming time:61.9779 s\n",
      "[epoch 6421]: training loss: 933.455444, consuming time:62.0159 s\n",
      "[epoch 6422]: training loss: 999.915894, consuming time:61.9412 s\n",
      "[epoch 6423]: training loss: 934.050171, consuming time:61.9095 s\n",
      "[epoch 6424]: training loss: 899.828125, consuming time:62.1111 s\n",
      "[epoch 6425]: training loss: 898.663696, consuming time:61.9363 s\n",
      "[epoch 6426]: training loss: 782.983521, consuming time:61.9169 s\n",
      "[epoch 6427]: training loss: 982.889771, consuming time:62.1108 s\n",
      "[epoch 6428]: training loss: 898.246948, consuming time:61.9277 s\n",
      "[epoch 6429]: training loss: 909.382080, consuming time:61.9028 s\n",
      "[epoch 6430]: training loss: 888.255798, consuming time:61.9419 s\n",
      "[epoch 6431]: training loss: 1154.043457, consuming time:62.0881 s\n",
      "[epoch 6432]: training loss: 1077.642090, consuming time:62.1994 s\n",
      "[epoch 6433]: training loss: 853.188843, consuming time:62.2309 s\n",
      "[epoch 6434]: training loss: 809.074951, consuming time:62.0312 s\n",
      "[epoch 6435]: training loss: 827.768921, consuming time:61.9240 s\n",
      "[epoch 6436]: training loss: 1062.432983, consuming time:62.0756 s\n",
      "[epoch 6437]: training loss: 856.412048, consuming time:62.1669 s\n",
      "[epoch 6438]: training loss: 918.899292, consuming time:62.0922 s\n",
      "[epoch 6439]: training loss: 899.737610, consuming time:62.0529 s\n",
      "[epoch 6440]: training loss: 976.869873, consuming time:61.9508 s\n",
      "[epoch 6441]: training loss: 1135.146118, consuming time:62.1040 s\n",
      "[epoch 6442]: training loss: 941.951904, consuming time:61.6874 s\n",
      "[epoch 6443]: training loss: 1173.284912, consuming time:61.9972 s\n",
      "[epoch 6444]: training loss: 805.010559, consuming time:62.0220 s\n",
      "[epoch 6445]: training loss: 1023.042114, consuming time:61.9232 s\n",
      "[epoch 6446]: training loss: 791.756592, consuming time:61.8215 s\n",
      "[epoch 6447]: training loss: 715.892700, consuming time:61.8114 s\n",
      "[epoch 6448]: training loss: 672.502930, consuming time:61.8567 s\n",
      "[epoch 6449]: training loss: 1021.783142, consuming time:61.9100 s\n",
      "[epoch 6450]: training loss: 1110.491211, consuming time:62.0077 s\n",
      "[epoch 6451]: training loss: 775.644775, consuming time:62.1105 s\n",
      "[epoch 6452]: training loss: 1089.953857, consuming time:61.8418 s\n",
      "[epoch 6453]: training loss: 856.975464, consuming time:61.9029 s\n",
      "[epoch 6454]: training loss: 935.393616, consuming time:61.8497 s\n",
      "[epoch 6455]: training loss: 701.976807, consuming time:61.8933 s\n",
      "[epoch 6456]: training loss: 794.266296, consuming time:61.9411 s\n",
      "[epoch 6457]: training loss: 741.135986, consuming time:61.9110 s\n",
      "[epoch 6458]: training loss: 964.562500, consuming time:61.8614 s\n",
      "[epoch 6459]: training loss: 821.751648, consuming time:61.8885 s\n",
      "[epoch 6460]: training loss: 995.967468, consuming time:61.9457 s\n",
      "[epoch 6461]: training loss: 811.379272, consuming time:62.1498 s\n",
      "[epoch 6462]: training loss: 1039.752441, consuming time:62.1532 s\n",
      "[epoch 6463]: training loss: 959.287109, consuming time:61.8332 s\n",
      "[epoch 6464]: training loss: 893.127808, consuming time:62.0441 s\n",
      "[epoch 6465]: training loss: 932.344055, consuming time:61.9618 s\n",
      "[epoch 6466]: training loss: 965.346130, consuming time:61.9193 s\n",
      "[epoch 6467]: training loss: 946.673584, consuming time:61.8274 s\n",
      "[epoch 6468]: training loss: 809.567871, consuming time:61.9464 s\n",
      "[epoch 6469]: training loss: 833.647583, consuming time:62.0497 s\n",
      "[epoch 6470]: training loss: 896.787720, consuming time:62.0437 s\n",
      "[epoch 6471]: training loss: 887.043884, consuming time:61.8011 s\n",
      "[epoch 6472]: training loss: 942.679626, consuming time:61.8770 s\n",
      "[epoch 6473]: training loss: 943.658691, consuming time:61.9527 s\n",
      "[epoch 6474]: training loss: 777.925049, consuming time:62.0125 s\n",
      "[epoch 6475]: training loss: 927.187378, consuming time:61.8227 s\n",
      "[epoch 6476]: training loss: 1432.932617, consuming time:61.9478 s\n",
      "[epoch 6477]: training loss: 876.612793, consuming time:61.8878 s\n",
      "[epoch 6478]: training loss: 999.659241, consuming time:61.9515 s\n",
      "[epoch 6479]: training loss: 904.919434, consuming time:62.1980 s\n",
      "[epoch 6480]: training loss: 958.031372, consuming time:61.9916 s\n",
      "[epoch 6481]: training loss: 1250.176025, consuming time:62.0020 s\n",
      "[epoch 6482]: training loss: 735.825012, consuming time:61.9668 s\n",
      "[epoch 6483]: training loss: 792.596802, consuming time:62.0421 s\n",
      "[epoch 6484]: training loss: 1114.220947, consuming time:61.8354 s\n",
      "[epoch 6485]: training loss: 1055.872559, consuming time:62.0466 s\n",
      "[epoch 6486]: training loss: 785.368347, consuming time:62.0268 s\n",
      "[epoch 6487]: training loss: 951.414062, consuming time:61.9706 s\n",
      "[epoch 6488]: training loss: 755.838745, consuming time:61.9278 s\n",
      "[epoch 6489]: training loss: 910.825195, consuming time:61.9481 s\n",
      "[epoch 6490]: training loss: 1139.968750, consuming time:62.0629 s\n",
      "[epoch 6491]: training loss: 857.275146, consuming time:62.1003 s\n",
      "[epoch 6492]: training loss: 773.901245, consuming time:61.9475 s\n",
      "[epoch 6493]: training loss: 960.330750, consuming time:62.0364 s\n",
      "[epoch 6494]: training loss: 987.751465, consuming time:61.9069 s\n",
      "[epoch 6495]: training loss: 992.802368, consuming time:61.7433 s\n",
      "[epoch 6496]: training loss: 653.103760, consuming time:61.8452 s\n",
      "[epoch 6497]: training loss: 925.661011, consuming time:62.1084 s\n",
      "[epoch 6498]: training loss: 848.021240, consuming time:61.9643 s\n",
      "[epoch 6499]: training loss: 928.484497, consuming time:61.9389 s\n",
      "[epoch 6500]: training loss: 709.735107, consuming time:61.9990 s\n",
      "[epoch 6501]: training loss: 747.565430, consuming time:61.9079 s\n",
      "[epoch 6502]: training loss: 1157.531006, consuming time:61.9975 s\n",
      "[epoch 6503]: training loss: 795.762939, consuming time:62.0674 s\n",
      "[epoch 6504]: training loss: 789.988525, consuming time:61.7901 s\n",
      "[epoch 6505]: training loss: 916.849121, consuming time:62.1040 s\n",
      "[epoch 6506]: training loss: 945.693604, consuming time:61.8107 s\n",
      "[epoch 6507]: training loss: 844.379822, consuming time:61.8087 s\n",
      "[epoch 6508]: training loss: 1137.838135, consuming time:62.2237 s\n",
      "[epoch 6509]: training loss: 957.002686, consuming time:61.9864 s\n",
      "[epoch 6510]: training loss: 950.873718, consuming time:62.0325 s\n",
      "[epoch 6511]: training loss: 836.213867, consuming time:61.9340 s\n",
      "[epoch 6512]: training loss: 746.033813, consuming time:61.9497 s\n",
      "[epoch 6513]: training loss: 1181.146973, consuming time:62.2079 s\n",
      "[epoch 6514]: training loss: 1000.512390, consuming time:62.0829 s\n",
      "[epoch 6515]: training loss: 956.692871, consuming time:61.9366 s\n",
      "[epoch 6516]: training loss: 1021.177002, consuming time:61.8905 s\n",
      "[epoch 6517]: training loss: 881.313110, consuming time:61.9138 s\n",
      "[epoch 6518]: training loss: 929.729614, consuming time:62.0580 s\n",
      "[epoch 6519]: training loss: 912.510376, consuming time:62.0386 s\n",
      "[epoch 6520]: training loss: 802.598145, consuming time:61.9865 s\n",
      "[epoch 6521]: training loss: 1064.419189, consuming time:61.9398 s\n",
      "[epoch 6522]: training loss: 905.015015, consuming time:61.8822 s\n",
      "[epoch 6523]: training loss: 884.536133, consuming time:61.9606 s\n",
      "[epoch 6524]: training loss: 855.255859, consuming time:61.9986 s\n",
      "[epoch 6525]: training loss: 1053.027588, consuming time:61.7608 s\n",
      "[epoch 6526]: training loss: 1022.125061, consuming time:61.9342 s\n",
      "[epoch 6527]: training loss: 838.743469, consuming time:61.8254 s\n",
      "[epoch 6528]: training loss: 847.832031, consuming time:61.8801 s\n",
      "[epoch 6529]: training loss: 739.375977, consuming time:66.0892 s\n",
      "[epoch 6530]: training loss: 957.115845, consuming time:61.9195 s\n",
      "[epoch 6531]: training loss: 1069.183594, consuming time:62.0347 s\n",
      "[epoch 6532]: training loss: 1157.314697, consuming time:62.1600 s\n",
      "[epoch 6533]: training loss: 999.339233, consuming time:61.9819 s\n",
      "[epoch 6534]: training loss: 741.094482, consuming time:61.8053 s\n",
      "[epoch 6535]: training loss: 765.925415, consuming time:61.9071 s\n",
      "[epoch 6536]: training loss: 1131.265137, consuming time:61.9756 s\n",
      "[epoch 6537]: training loss: 752.330017, consuming time:61.9420 s\n",
      "[epoch 6538]: training loss: 903.668335, consuming time:61.9869 s\n",
      "[epoch 6539]: training loss: 1130.903564, consuming time:61.8976 s\n",
      "[epoch 6540]: training loss: 1036.891846, consuming time:62.0860 s\n",
      "[epoch 6541]: training loss: 980.355347, consuming time:61.9674 s\n",
      "[epoch 6542]: training loss: 702.563110, consuming time:61.9651 s\n",
      "[epoch 6543]: training loss: 721.720520, consuming time:62.0512 s\n",
      "[epoch 6544]: training loss: 762.923706, consuming time:62.0835 s\n",
      "[epoch 6545]: training loss: 781.283325, consuming time:62.0284 s\n",
      "[epoch 6546]: training loss: 1182.435059, consuming time:61.8747 s\n",
      "[epoch 6547]: training loss: 831.672119, consuming time:61.9559 s\n",
      "[epoch 6548]: training loss: 978.498108, consuming time:62.0500 s\n",
      "[epoch 6549]: training loss: 1206.990234, consuming time:62.0474 s\n",
      "[epoch 6550]: training loss: 892.863647, consuming time:61.9850 s\n",
      "[epoch 6551]: training loss: 916.764771, consuming time:61.9695 s\n",
      "[epoch 6552]: training loss: 962.380615, consuming time:62.1162 s\n",
      "[epoch 6553]: training loss: 949.977234, consuming time:62.0009 s\n",
      "[epoch 6554]: training loss: 1246.992920, consuming time:62.0727 s\n",
      "[epoch 6555]: training loss: 837.146729, consuming time:62.1712 s\n",
      "[epoch 6556]: training loss: 1082.187378, consuming time:61.8333 s\n",
      "[epoch 6557]: training loss: 1056.248901, consuming time:62.1370 s\n",
      "[epoch 6558]: training loss: 767.755127, consuming time:62.0944 s\n",
      "[epoch 6559]: training loss: 891.353455, consuming time:61.9000 s\n",
      "[epoch 6560]: training loss: 1012.356445, consuming time:61.9865 s\n",
      "[epoch 6561]: training loss: 831.129639, consuming time:62.1653 s\n",
      "[epoch 6562]: training loss: 916.878052, consuming time:61.8868 s\n",
      "[epoch 6563]: training loss: 888.562500, consuming time:62.1129 s\n",
      "[epoch 6564]: training loss: 902.811951, consuming time:62.2431 s\n",
      "[epoch 6565]: training loss: 1064.707397, consuming time:62.1085 s\n",
      "[epoch 6566]: training loss: 1069.492188, consuming time:62.2543 s\n",
      "[epoch 6567]: training loss: 828.125183, consuming time:62.1399 s\n",
      "[epoch 6568]: training loss: 1139.319336, consuming time:62.2091 s\n",
      "[epoch 6569]: training loss: 1016.936890, consuming time:62.0145 s\n",
      "[epoch 6570]: training loss: 1117.437134, consuming time:62.0731 s\n",
      "[epoch 6571]: training loss: 1099.687012, consuming time:62.3620 s\n",
      "[epoch 6572]: training loss: 864.882202, consuming time:62.1919 s\n",
      "[epoch 6573]: training loss: 962.320679, consuming time:61.9763 s\n",
      "[epoch 6574]: training loss: 1052.005615, consuming time:61.9221 s\n",
      "[epoch 6575]: training loss: 1302.361206, consuming time:61.9652 s\n",
      "[epoch 6576]: training loss: 845.070068, consuming time:61.9119 s\n",
      "[epoch 6577]: training loss: 1087.775757, consuming time:61.9619 s\n",
      "[epoch 6578]: training loss: 1078.435913, consuming time:62.0554 s\n",
      "[epoch 6579]: training loss: 1175.952637, consuming time:62.0517 s\n",
      "[epoch 6580]: training loss: 1073.467529, consuming time:61.9296 s\n",
      "[epoch 6581]: training loss: 1252.560669, consuming time:62.0642 s\n",
      "[epoch 6582]: training loss: 844.337097, consuming time:61.9490 s\n",
      "[epoch 6583]: training loss: 939.655640, consuming time:61.9431 s\n",
      "[epoch 6584]: training loss: 853.694885, consuming time:62.1435 s\n",
      "[epoch 6585]: training loss: 879.534119, consuming time:61.9918 s\n",
      "[epoch 6586]: training loss: 1100.702637, consuming time:61.8659 s\n",
      "[epoch 6587]: training loss: 772.121887, consuming time:62.0892 s\n",
      "[epoch 6588]: training loss: 887.211609, consuming time:62.0326 s\n",
      "[epoch 6589]: training loss: 751.485291, consuming time:61.8759 s\n",
      "[epoch 6590]: training loss: 1090.315796, consuming time:62.0191 s\n",
      "[epoch 6591]: training loss: 1147.046265, consuming time:61.9664 s\n",
      "[epoch 6592]: training loss: 756.887207, consuming time:61.9437 s\n",
      "[epoch 6593]: training loss: 750.712891, consuming time:61.9110 s\n",
      "[epoch 6594]: training loss: 916.898315, consuming time:62.0607 s\n",
      "[epoch 6595]: training loss: 793.608643, consuming time:62.0116 s\n",
      "[epoch 6596]: training loss: 1046.538086, consuming time:62.0811 s\n",
      "[epoch 6597]: training loss: 909.374756, consuming time:61.8452 s\n",
      "[epoch 6598]: training loss: 886.801880, consuming time:61.9186 s\n",
      "[epoch 6599]: training loss: 772.702026, consuming time:62.0133 s\n",
      "[epoch 6600]: training loss: 1019.050903, consuming time:61.9708 s\n",
      "[epoch 6601]: training loss: 783.489441, consuming time:62.1505 s\n",
      "[epoch 6602]: training loss: 773.234253, consuming time:61.9776 s\n",
      "[epoch 6603]: training loss: 996.495972, consuming time:62.0839 s\n",
      "[epoch 6604]: training loss: 750.017151, consuming time:61.8531 s\n",
      "[epoch 6605]: training loss: 997.675537, consuming time:62.0377 s\n",
      "[epoch 6606]: training loss: 812.154602, consuming time:61.9558 s\n",
      "[epoch 6607]: training loss: 891.595337, consuming time:62.0602 s\n",
      "[epoch 6608]: training loss: 935.138855, consuming time:62.0092 s\n",
      "[epoch 6609]: training loss: 823.807495, consuming time:62.1876 s\n",
      "[epoch 6610]: training loss: 1070.143311, consuming time:61.8476 s\n",
      "[epoch 6611]: training loss: 938.159058, consuming time:61.9977 s\n",
      "[epoch 6612]: training loss: 852.393555, consuming time:61.9348 s\n",
      "[epoch 6613]: training loss: 1045.101318, consuming time:62.0941 s\n",
      "[epoch 6614]: training loss: 736.270691, consuming time:61.9360 s\n",
      "[epoch 6615]: training loss: 1119.987793, consuming time:61.9717 s\n",
      "[epoch 6616]: training loss: 1059.074951, consuming time:61.8705 s\n",
      "[epoch 6617]: training loss: 1316.375488, consuming time:62.0020 s\n",
      "[epoch 6618]: training loss: 1125.787598, consuming time:62.1248 s\n",
      "[epoch 6619]: training loss: 1204.142334, consuming time:62.0610 s\n",
      "[epoch 6620]: training loss: 1123.343018, consuming time:62.0114 s\n",
      "[epoch 6621]: training loss: 903.669922, consuming time:62.0169 s\n",
      "[epoch 6622]: training loss: 995.993530, consuming time:62.0820 s\n",
      "[epoch 6623]: training loss: 906.480103, consuming time:62.0776 s\n",
      "[epoch 6624]: training loss: 874.881958, consuming time:62.0036 s\n",
      "[epoch 6625]: training loss: 836.512573, consuming time:61.9112 s\n",
      "[epoch 6626]: training loss: 1181.176270, consuming time:61.9080 s\n",
      "[epoch 6627]: training loss: 974.880127, consuming time:61.9305 s\n",
      "[epoch 6628]: training loss: 910.532043, consuming time:61.8386 s\n",
      "[epoch 6629]: training loss: 809.892090, consuming time:61.9869 s\n",
      "[epoch 6630]: training loss: 920.871643, consuming time:62.1234 s\n",
      "[epoch 6631]: training loss: 853.803833, consuming time:61.9655 s\n",
      "[epoch 6632]: training loss: 820.668701, consuming time:61.9751 s\n",
      "[epoch 6633]: training loss: 902.464478, consuming time:61.9597 s\n",
      "[epoch 6634]: training loss: 850.690430, consuming time:61.9997 s\n",
      "[epoch 6635]: training loss: 709.626587, consuming time:61.8667 s\n",
      "[epoch 6636]: training loss: 1011.505859, consuming time:62.0021 s\n",
      "[epoch 6637]: training loss: 897.309998, consuming time:62.0967 s\n",
      "[epoch 6638]: training loss: 878.981201, consuming time:62.0765 s\n",
      "[epoch 6639]: training loss: 809.555298, consuming time:61.9582 s\n",
      "[epoch 6640]: training loss: 962.976562, consuming time:62.0132 s\n",
      "[epoch 6641]: training loss: 804.975952, consuming time:61.9234 s\n",
      "[epoch 6642]: training loss: 924.866272, consuming time:62.0305 s\n",
      "[epoch 6643]: training loss: 879.790466, consuming time:61.9600 s\n",
      "[epoch 6644]: training loss: 824.486206, consuming time:61.8603 s\n",
      "[epoch 6645]: training loss: 1145.614014, consuming time:61.9244 s\n",
      "[epoch 6646]: training loss: 892.894775, consuming time:61.9330 s\n",
      "[epoch 6647]: training loss: 825.822021, consuming time:62.0016 s\n",
      "[epoch 6648]: training loss: 933.654968, consuming time:61.9132 s\n",
      "[epoch 6649]: training loss: 1028.935303, consuming time:62.0418 s\n",
      "[epoch 6650]: training loss: 950.505615, consuming time:61.9618 s\n",
      "[epoch 6651]: training loss: 935.887085, consuming time:62.0234 s\n",
      "[epoch 6652]: training loss: 860.345154, consuming time:61.8640 s\n",
      "[epoch 6653]: training loss: 820.960327, consuming time:62.0201 s\n",
      "[epoch 6654]: training loss: 906.223022, consuming time:61.9984 s\n",
      "[epoch 6655]: training loss: 904.767334, consuming time:62.0376 s\n",
      "[epoch 6656]: training loss: 1271.197510, consuming time:61.7967 s\n",
      "[epoch 6657]: training loss: 1115.556274, consuming time:61.8942 s\n",
      "[epoch 6658]: training loss: 874.297729, consuming time:61.8933 s\n",
      "[epoch 6659]: training loss: 856.080933, consuming time:62.0136 s\n",
      "[epoch 6660]: training loss: 1069.676514, consuming time:61.9856 s\n",
      "[epoch 6661]: training loss: 909.075867, consuming time:62.0712 s\n",
      "[epoch 6662]: training loss: 904.260132, consuming time:61.8954 s\n",
      "[epoch 6663]: training loss: 976.955566, consuming time:61.9630 s\n",
      "[epoch 6664]: training loss: 774.703247, consuming time:61.9925 s\n",
      "[epoch 6665]: training loss: 1098.801514, consuming time:62.0768 s\n",
      "[epoch 6666]: training loss: 794.850586, consuming time:62.1212 s\n",
      "[epoch 6667]: training loss: 908.520874, consuming time:61.8781 s\n",
      "[epoch 6668]: training loss: 643.023865, consuming time:61.9275 s\n",
      "[epoch 6669]: training loss: 703.722900, consuming time:62.1003 s\n",
      "[epoch 6670]: training loss: 972.829346, consuming time:62.2282 s\n",
      "[epoch 6671]: training loss: 891.732056, consuming time:61.9802 s\n",
      "[epoch 6672]: training loss: 1066.570679, consuming time:61.9663 s\n",
      "[epoch 6673]: training loss: 853.326355, consuming time:61.9652 s\n",
      "[epoch 6674]: training loss: 756.088379, consuming time:61.9390 s\n",
      "[epoch 6675]: training loss: 865.122681, consuming time:62.5394 s\n",
      "[epoch 6676]: training loss: 1142.226929, consuming time:62.4341 s\n",
      "[epoch 6677]: training loss: 871.237915, consuming time:62.6545 s\n",
      "[epoch 6678]: training loss: 836.936279, consuming time:61.6531 s\n",
      "[epoch 6679]: training loss: 1005.020020, consuming time:61.8844 s\n",
      "[epoch 6680]: training loss: 766.895569, consuming time:64.9221 s\n",
      "[epoch 6681]: training loss: 979.777527, consuming time:64.0604 s\n",
      "[epoch 6682]: training loss: 794.716187, consuming time:66.3474 s\n",
      "[epoch 6683]: training loss: 859.752930, consuming time:68.8783 s\n",
      "[epoch 6684]: training loss: 1066.363281, consuming time:73.1777 s\n",
      "[epoch 6685]: training loss: 846.911865, consuming time:63.5988 s\n",
      "[epoch 6686]: training loss: 829.125793, consuming time:61.9593 s\n",
      "[epoch 6687]: training loss: 937.240356, consuming time:60.4813 s\n",
      "[epoch 6688]: training loss: 875.513062, consuming time:61.6480 s\n",
      "[epoch 6689]: training loss: 895.132629, consuming time:64.8824 s\n",
      "[epoch 6690]: training loss: 808.672913, consuming time:62.3657 s\n",
      "[epoch 6691]: training loss: 944.341431, consuming time:70.0286 s\n",
      "[epoch 6692]: training loss: 800.391724, consuming time:67.8309 s\n",
      "[epoch 6693]: training loss: 1008.223999, consuming time:63.7738 s\n",
      "[epoch 6694]: training loss: 811.142822, consuming time:65.1322 s\n",
      "[epoch 6695]: training loss: 938.023926, consuming time:64.3114 s\n",
      "[epoch 6696]: training loss: 916.230530, consuming time:63.8338 s\n",
      "[epoch 6697]: training loss: 1012.317627, consuming time:65.5234 s\n",
      "[epoch 6698]: training loss: 1061.755127, consuming time:65.7882 s\n",
      "[epoch 6699]: training loss: 986.243347, consuming time:63.8294 s\n",
      "[epoch 6700]: training loss: 778.030212, consuming time:65.1273 s\n",
      "[epoch 6701]: training loss: 962.937805, consuming time:67.2591 s\n",
      "[epoch 6702]: training loss: 844.278809, consuming time:64.7072 s\n",
      "[epoch 6703]: training loss: 871.653870, consuming time:64.0455 s\n",
      "[epoch 6704]: training loss: 782.072571, consuming time:64.2138 s\n",
      "[epoch 6705]: training loss: 1251.023926, consuming time:70.8257 s\n",
      "[epoch 6706]: training loss: 1057.329590, consuming time:71.7537 s\n",
      "[epoch 6707]: training loss: 934.336182, consuming time:70.7403 s\n",
      "[epoch 6708]: training loss: 952.375610, consuming time:60.8514 s\n",
      "[epoch 6709]: training loss: 1035.311523, consuming time:61.1008 s\n",
      "[epoch 6710]: training loss: 919.822266, consuming time:60.7199 s\n",
      "[epoch 6711]: training loss: 1066.469727, consuming time:60.3841 s\n",
      "[epoch 6712]: training loss: 975.921021, consuming time:61.0751 s\n",
      "[epoch 6713]: training loss: 857.329834, consuming time:61.0651 s\n",
      "[epoch 6714]: training loss: 1037.651855, consuming time:65.6566 s\n",
      "[epoch 6715]: training loss: 815.138367, consuming time:60.9488 s\n",
      "[epoch 6716]: training loss: 1142.869629, consuming time:61.0311 s\n",
      "[epoch 6717]: training loss: 986.635315, consuming time:61.1598 s\n",
      "[epoch 6718]: training loss: 972.889221, consuming time:60.8989 s\n",
      "[epoch 6719]: training loss: 851.009338, consuming time:61.8358 s\n",
      "[epoch 6720]: training loss: 756.555359, consuming time:63.4678 s\n",
      "[epoch 6721]: training loss: 842.451721, consuming time:61.0971 s\n",
      "[epoch 6722]: training loss: 992.919312, consuming time:65.1539 s\n",
      "[epoch 6723]: training loss: 795.352051, consuming time:65.9030 s\n",
      "[epoch 6724]: training loss: 1053.262939, consuming time:66.1182 s\n",
      "[epoch 6725]: training loss: 1294.125000, consuming time:69.0142 s\n",
      "[epoch 6726]: training loss: 1253.367676, consuming time:65.6815 s\n",
      "[epoch 6727]: training loss: 965.704956, consuming time:68.8684 s\n",
      "[epoch 6728]: training loss: 881.311401, consuming time:66.2803 s\n",
      "[epoch 6729]: training loss: 905.790955, consuming time:63.1443 s\n",
      "[epoch 6730]: training loss: 1037.843018, consuming time:61.3603 s\n",
      "[epoch 6731]: training loss: 704.354614, consuming time:61.0577 s\n",
      "[epoch 6732]: training loss: 826.897705, consuming time:60.3918 s\n",
      "[epoch 6733]: training loss: 820.588623, consuming time:60.7747 s\n",
      "[epoch 6734]: training loss: 1076.939819, consuming time:60.0991 s\n",
      "[epoch 6735]: training loss: 1220.888428, consuming time:60.3903 s\n",
      "[epoch 6736]: training loss: 889.644531, consuming time:60.2533 s\n",
      "[epoch 6737]: training loss: 943.952209, consuming time:60.4336 s\n",
      "[epoch 6738]: training loss: 1053.517700, consuming time:60.5853 s\n",
      "[epoch 6739]: training loss: 981.116211, consuming time:60.1740 s\n",
      "[epoch 6740]: training loss: 871.994873, consuming time:60.4005 s\n",
      "[epoch 6741]: training loss: 964.888306, consuming time:60.5826 s\n",
      "[epoch 6742]: training loss: 908.401123, consuming time:60.3834 s\n",
      "[epoch 6743]: training loss: 597.022095, consuming time:60.3140 s\n",
      "[epoch 6744]: training loss: 998.029419, consuming time:60.7324 s\n",
      "[epoch 6745]: training loss: 1141.734131, consuming time:60.5256 s\n",
      "[epoch 6746]: training loss: 864.240967, consuming time:60.1701 s\n",
      "[epoch 6747]: training loss: 979.803528, consuming time:60.3876 s\n",
      "[epoch 6748]: training loss: 827.906860, consuming time:60.3491 s\n",
      "[epoch 6749]: training loss: 986.399048, consuming time:60.3768 s\n",
      "[epoch 6750]: training loss: 1054.269775, consuming time:60.7481 s\n",
      "[epoch 6751]: training loss: 738.147583, consuming time:60.3272 s\n",
      "[epoch 6752]: training loss: 1278.500732, consuming time:60.1244 s\n",
      "[epoch 6753]: training loss: 822.720947, consuming time:60.5179 s\n",
      "[epoch 6754]: training loss: 775.152100, consuming time:60.6398 s\n",
      "[epoch 6755]: training loss: 1050.417847, consuming time:60.2210 s\n",
      "[epoch 6756]: training loss: 856.484436, consuming time:60.5976 s\n",
      "[epoch 6757]: training loss: 928.741333, consuming time:60.4405 s\n",
      "[epoch 6758]: training loss: 738.673950, consuming time:60.2861 s\n",
      "[epoch 6759]: training loss: 708.262878, consuming time:60.3777 s\n",
      "[epoch 6760]: training loss: 1176.653931, consuming time:60.3457 s\n",
      "[epoch 6761]: training loss: 801.543640, consuming time:60.3362 s\n",
      "[epoch 6762]: training loss: 900.941345, consuming time:60.3709 s\n",
      "[epoch 6763]: training loss: 1084.424683, consuming time:60.4314 s\n",
      "[epoch 6764]: training loss: 886.248657, consuming time:60.4297 s\n",
      "[epoch 6765]: training loss: 953.277344, consuming time:60.4396 s\n",
      "[epoch 6766]: training loss: 740.453064, consuming time:60.5806 s\n",
      "[epoch 6767]: training loss: 1117.389648, consuming time:60.3136 s\n",
      "[epoch 6768]: training loss: 1232.436523, consuming time:60.5508 s\n",
      "[epoch 6769]: training loss: 1004.134644, consuming time:60.0827 s\n",
      "[epoch 6770]: training loss: 925.508057, consuming time:60.3231 s\n",
      "[epoch 6771]: training loss: 1036.013184, consuming time:60.2164 s\n",
      "[epoch 6772]: training loss: 1085.109863, consuming time:60.4947 s\n",
      "[epoch 6773]: training loss: 1156.801270, consuming time:60.2765 s\n",
      "[epoch 6774]: training loss: 773.618347, consuming time:60.5179 s\n",
      "[epoch 6775]: training loss: 911.026978, consuming time:60.3992 s\n",
      "[epoch 6776]: training loss: 665.890869, consuming time:60.0705 s\n",
      "[epoch 6777]: training loss: 892.996277, consuming time:60.4154 s\n",
      "[epoch 6778]: training loss: 1196.253540, consuming time:60.4142 s\n",
      "[epoch 6779]: training loss: 704.787781, consuming time:60.2466 s\n",
      "[epoch 6780]: training loss: 1142.851562, consuming time:60.5931 s\n",
      "[epoch 6781]: training loss: 1151.753662, consuming time:60.2665 s\n",
      "[epoch 6782]: training loss: 1066.520386, consuming time:60.5370 s\n",
      "[epoch 6783]: training loss: 1014.549927, consuming time:60.4293 s\n",
      "[epoch 6784]: training loss: 913.765015, consuming time:60.1987 s\n",
      "[epoch 6785]: training loss: 936.248535, consuming time:60.1626 s\n",
      "[epoch 6786]: training loss: 877.887329, consuming time:60.6005 s\n",
      "[epoch 6787]: training loss: 1243.666626, consuming time:60.3940 s\n",
      "[epoch 6788]: training loss: 818.927490, consuming time:60.2875 s\n",
      "[epoch 6789]: training loss: 1303.065674, consuming time:60.0968 s\n",
      "[epoch 6790]: training loss: 1026.921997, consuming time:60.4626 s\n",
      "[epoch 6791]: training loss: 960.488403, consuming time:60.2811 s\n",
      "[epoch 6792]: training loss: 1019.770508, consuming time:60.6911 s\n",
      "[epoch 6793]: training loss: 928.234680, consuming time:60.3151 s\n",
      "[epoch 6794]: training loss: 792.146484, consuming time:60.3533 s\n",
      "[epoch 6795]: training loss: 846.309509, consuming time:60.2598 s\n",
      "[epoch 6796]: training loss: 717.595215, consuming time:60.3243 s\n",
      "[epoch 6797]: training loss: 810.788147, consuming time:60.4691 s\n",
      "[epoch 6798]: training loss: 1113.862305, consuming time:60.5232 s\n",
      "[epoch 6799]: training loss: 738.826111, consuming time:60.1821 s\n",
      "[epoch 6800]: training loss: 801.310547, consuming time:59.9426 s\n",
      "[epoch 6801]: training loss: 925.080688, consuming time:60.3105 s\n",
      "[epoch 6802]: training loss: 936.911926, consuming time:60.4422 s\n",
      "[epoch 6803]: training loss: 950.010254, consuming time:60.3967 s\n",
      "[epoch 6804]: training loss: 775.542114, consuming time:60.3657 s\n",
      "[epoch 6805]: training loss: 1137.099121, consuming time:60.4076 s\n",
      "[epoch 6806]: training loss: 830.697937, consuming time:60.4788 s\n",
      "[epoch 6807]: training loss: 983.998535, consuming time:60.3310 s\n",
      "[epoch 6808]: training loss: 908.744568, consuming time:60.3572 s\n",
      "[epoch 6809]: training loss: 722.236328, consuming time:60.5471 s\n",
      "[epoch 6810]: training loss: 809.114014, consuming time:60.5335 s\n",
      "[epoch 6811]: training loss: 1087.574219, consuming time:60.7157 s\n",
      "[epoch 6812]: training loss: 787.204224, consuming time:60.4212 s\n",
      "[epoch 6813]: training loss: 1168.929321, consuming time:60.3797 s\n",
      "[epoch 6814]: training loss: 782.338989, consuming time:60.1220 s\n",
      "[epoch 6815]: training loss: 914.510986, consuming time:60.4781 s\n",
      "[epoch 6816]: training loss: 941.659058, consuming time:60.2796 s\n",
      "[epoch 6817]: training loss: 758.174561, consuming time:60.2578 s\n",
      "[epoch 6818]: training loss: 837.431213, consuming time:60.1342 s\n",
      "[epoch 6819]: training loss: 1068.881836, consuming time:60.6015 s\n",
      "[epoch 6820]: training loss: 1219.549927, consuming time:60.6645 s\n",
      "[epoch 6821]: training loss: 861.921509, consuming time:60.1750 s\n",
      "[epoch 6822]: training loss: 897.631104, consuming time:60.3535 s\n",
      "[epoch 6823]: training loss: 997.880920, consuming time:60.4038 s\n",
      "[epoch 6824]: training loss: 696.992554, consuming time:59.9803 s\n",
      "[epoch 6825]: training loss: 901.380859, consuming time:60.1728 s\n",
      "[epoch 6826]: training loss: 902.113159, consuming time:60.2991 s\n",
      "[epoch 6827]: training loss: 762.190002, consuming time:60.1801 s\n",
      "[epoch 6828]: training loss: 875.884277, consuming time:60.2590 s\n",
      "[epoch 6829]: training loss: 807.028564, consuming time:60.1456 s\n",
      "[epoch 6830]: training loss: 1085.956299, consuming time:60.3298 s\n",
      "[epoch 6831]: training loss: 1030.046509, consuming time:60.0894 s\n",
      "[epoch 6832]: training loss: 962.562866, consuming time:60.4608 s\n",
      "[epoch 6833]: training loss: 1099.106201, consuming time:60.4210 s\n",
      "[epoch 6834]: training loss: 777.675171, consuming time:60.5348 s\n",
      "[epoch 6835]: training loss: 1201.050781, consuming time:60.1716 s\n",
      "[epoch 6836]: training loss: 748.902710, consuming time:60.2829 s\n",
      "[epoch 6837]: training loss: 1040.276978, consuming time:60.4563 s\n",
      "[epoch 6838]: training loss: 1030.599365, consuming time:60.3787 s\n",
      "[epoch 6839]: training loss: 580.861084, consuming time:60.4871 s\n",
      "[epoch 6840]: training loss: 1088.651367, consuming time:60.5213 s\n",
      "[epoch 6841]: training loss: 893.207764, consuming time:60.1469 s\n",
      "[epoch 6842]: training loss: 938.252014, consuming time:59.9585 s\n",
      "[epoch 6843]: training loss: 1046.672119, consuming time:60.0602 s\n",
      "[epoch 6844]: training loss: 799.645996, consuming time:60.3135 s\n",
      "[epoch 6845]: training loss: 710.603455, consuming time:60.2207 s\n",
      "[epoch 6846]: training loss: 957.447632, consuming time:60.3464 s\n",
      "[epoch 6847]: training loss: 1169.112549, consuming time:60.3141 s\n",
      "[epoch 6848]: training loss: 984.003113, consuming time:60.2908 s\n",
      "[epoch 6849]: training loss: 974.438660, consuming time:60.3390 s\n",
      "[epoch 6850]: training loss: 970.525024, consuming time:60.2381 s\n",
      "[epoch 6851]: training loss: 1080.886719, consuming time:60.4482 s\n",
      "[epoch 6852]: training loss: 979.741882, consuming time:60.4030 s\n",
      "[epoch 6853]: training loss: 856.232788, consuming time:60.1877 s\n",
      "[epoch 6854]: training loss: 785.428955, consuming time:60.1138 s\n",
      "[epoch 6855]: training loss: 774.802917, consuming time:60.3097 s\n",
      "[epoch 6856]: training loss: 1080.258423, consuming time:60.2097 s\n",
      "[epoch 6857]: training loss: 742.329895, consuming time:60.3740 s\n",
      "[epoch 6858]: training loss: 797.155762, consuming time:60.4837 s\n",
      "[epoch 6859]: training loss: 836.554688, consuming time:60.3379 s\n",
      "[epoch 6860]: training loss: 997.487549, consuming time:59.9509 s\n",
      "[epoch 6861]: training loss: 776.359009, consuming time:60.8230 s\n",
      "[epoch 6862]: training loss: 1059.756836, consuming time:60.6220 s\n",
      "[epoch 6863]: training loss: 1011.124084, consuming time:60.4834 s\n",
      "[epoch 6864]: training loss: 737.583130, consuming time:60.6861 s\n",
      "[epoch 6865]: training loss: 1207.329224, consuming time:60.1816 s\n",
      "[epoch 6866]: training loss: 826.749573, consuming time:60.4106 s\n",
      "[epoch 6867]: training loss: 722.319336, consuming time:60.9139 s\n",
      "[epoch 6868]: training loss: 754.161255, consuming time:60.4429 s\n",
      "[epoch 6869]: training loss: 1033.956299, consuming time:60.6644 s\n",
      "[epoch 6870]: training loss: 1115.519653, consuming time:60.6310 s\n",
      "[epoch 6871]: training loss: 1137.678467, consuming time:60.4367 s\n",
      "[epoch 6872]: training loss: 817.799805, consuming time:60.4121 s\n",
      "[epoch 6873]: training loss: 989.736938, consuming time:60.5266 s\n",
      "[epoch 6874]: training loss: 911.967834, consuming time:60.2580 s\n",
      "[epoch 6875]: training loss: 953.081177, consuming time:60.4208 s\n",
      "[epoch 6876]: training loss: 1120.614746, consuming time:60.3275 s\n",
      "[epoch 6877]: training loss: 1315.238770, consuming time:60.3868 s\n",
      "[epoch 6878]: training loss: 904.427368, consuming time:60.6001 s\n",
      "[epoch 6879]: training loss: 995.575745, consuming time:60.3545 s\n",
      "[epoch 6880]: training loss: 1090.206299, consuming time:60.2959 s\n",
      "[epoch 6881]: training loss: 1088.251953, consuming time:60.5364 s\n",
      "[epoch 6882]: training loss: 1066.796265, consuming time:60.1593 s\n",
      "[epoch 6883]: training loss: 879.453552, consuming time:60.1894 s\n",
      "[epoch 6884]: training loss: 803.735474, consuming time:60.2919 s\n",
      "[epoch 6885]: training loss: 874.678589, consuming time:60.7394 s\n",
      "[epoch 6886]: training loss: 849.504822, consuming time:60.7139 s\n",
      "[epoch 6887]: training loss: 984.445618, consuming time:60.6523 s\n",
      "[epoch 6888]: training loss: 1081.775635, consuming time:60.5346 s\n",
      "[epoch 6889]: training loss: 835.978638, consuming time:60.0788 s\n",
      "[epoch 6890]: training loss: 1255.380981, consuming time:60.3943 s\n",
      "[epoch 6891]: training loss: 816.272827, consuming time:60.4747 s\n",
      "[epoch 6892]: training loss: 874.604919, consuming time:60.5024 s\n",
      "[epoch 6893]: training loss: 741.030029, consuming time:60.3891 s\n",
      "[epoch 6894]: training loss: 1085.571411, consuming time:60.1332 s\n",
      "[epoch 6895]: training loss: 1065.497314, consuming time:60.0568 s\n",
      "[epoch 6896]: training loss: 735.076294, consuming time:60.4237 s\n",
      "[epoch 6897]: training loss: 855.000183, consuming time:60.2781 s\n",
      "[epoch 6898]: training loss: 861.770996, consuming time:60.5756 s\n",
      "[epoch 6899]: training loss: 1078.788330, consuming time:60.2459 s\n",
      "[epoch 6900]: training loss: 904.767761, consuming time:60.3564 s\n",
      "[epoch 6901]: training loss: 712.868652, consuming time:60.2600 s\n",
      "[epoch 6902]: training loss: 1071.871948, consuming time:60.2994 s\n",
      "[epoch 6903]: training loss: 907.505798, consuming time:60.4629 s\n",
      "[epoch 6904]: training loss: 1118.034424, consuming time:60.6085 s\n",
      "[epoch 6905]: training loss: 867.730835, consuming time:60.7743 s\n",
      "[epoch 6906]: training loss: 915.025574, consuming time:60.6478 s\n",
      "[epoch 6907]: training loss: 901.265625, consuming time:60.1049 s\n",
      "[epoch 6908]: training loss: 1054.474487, consuming time:60.4857 s\n",
      "[epoch 6909]: training loss: 850.519653, consuming time:60.6973 s\n",
      "[epoch 6910]: training loss: 897.294617, consuming time:60.7299 s\n",
      "[epoch 6911]: training loss: 675.472168, consuming time:62.1749 s\n",
      "[epoch 6912]: training loss: 1069.797852, consuming time:66.7717 s\n",
      "[epoch 6913]: training loss: 985.126831, consuming time:70.0370 s\n",
      "[epoch 6914]: training loss: 710.050903, consuming time:65.7693 s\n",
      "[epoch 6915]: training loss: 777.481445, consuming time:62.3422 s\n",
      "[epoch 6916]: training loss: 1121.596191, consuming time:61.1855 s\n",
      "[epoch 6917]: training loss: 1040.303955, consuming time:60.5037 s\n",
      "[epoch 6918]: training loss: 994.344055, consuming time:60.9170 s\n",
      "[epoch 6919]: training loss: 800.692993, consuming time:60.7838 s\n",
      "[epoch 6920]: training loss: 806.588013, consuming time:60.8876 s\n",
      "[epoch 6921]: training loss: 907.723389, consuming time:60.3876 s\n",
      "[epoch 6922]: training loss: 483.051147, consuming time:61.9240 s\n",
      "[epoch 6923]: training loss: 1088.280273, consuming time:60.8119 s\n",
      "[epoch 6924]: training loss: 1018.983643, consuming time:61.1337 s\n",
      "[epoch 6925]: training loss: 1075.506958, consuming time:60.5373 s\n",
      "[epoch 6926]: training loss: 1057.925537, consuming time:60.8906 s\n",
      "[epoch 6927]: training loss: 1088.544189, consuming time:60.3425 s\n",
      "[epoch 6928]: training loss: 953.631226, consuming time:60.3624 s\n",
      "[epoch 6929]: training loss: 1112.031250, consuming time:60.4184 s\n",
      "[epoch 6930]: training loss: 1173.455078, consuming time:61.3677 s\n",
      "[epoch 6931]: training loss: 964.089233, consuming time:61.9179 s\n",
      "[epoch 6932]: training loss: 1043.222168, consuming time:61.4432 s\n",
      "[epoch 6933]: training loss: 651.546814, consuming time:60.4736 s\n",
      "[epoch 6934]: training loss: 837.643738, consuming time:60.9737 s\n",
      "[epoch 6935]: training loss: 1032.150146, consuming time:61.8988 s\n",
      "[epoch 6936]: training loss: 893.426270, consuming time:61.6662 s\n",
      "[epoch 6937]: training loss: 1007.217896, consuming time:61.7077 s\n",
      "[epoch 6938]: training loss: 658.919067, consuming time:61.6365 s\n",
      "[epoch 6939]: training loss: 887.210693, consuming time:60.6751 s\n",
      "[epoch 6940]: training loss: 986.118774, consuming time:61.0154 s\n",
      "[epoch 6941]: training loss: 821.721069, consuming time:62.2681 s\n",
      "[epoch 6942]: training loss: 1151.115845, consuming time:61.1206 s\n",
      "[epoch 6943]: training loss: 1051.620483, consuming time:60.5970 s\n",
      "[epoch 6944]: training loss: 900.885986, consuming time:60.1508 s\n",
      "[epoch 6945]: training loss: 722.691895, consuming time:60.0471 s\n",
      "[epoch 6946]: training loss: 880.631958, consuming time:60.2945 s\n",
      "[epoch 6947]: training loss: 1169.601074, consuming time:60.1247 s\n",
      "[epoch 6948]: training loss: 1101.462402, consuming time:59.7431 s\n",
      "[epoch 6949]: training loss: 832.148682, consuming time:60.1169 s\n",
      "[epoch 6950]: training loss: 1090.848389, consuming time:59.9447 s\n",
      "[epoch 6951]: training loss: 1144.861328, consuming time:60.1522 s\n",
      "[epoch 6952]: training loss: 900.735229, consuming time:60.4102 s\n",
      "[epoch 6953]: training loss: 827.120605, consuming time:60.1218 s\n",
      "[epoch 6954]: training loss: 943.290955, consuming time:59.7542 s\n",
      "[epoch 6955]: training loss: 966.998413, consuming time:60.2792 s\n",
      "[epoch 6956]: training loss: 945.009033, consuming time:60.1704 s\n",
      "[epoch 6957]: training loss: 698.079224, consuming time:60.2236 s\n",
      "[epoch 6958]: training loss: 985.617859, consuming time:60.4153 s\n",
      "[epoch 6959]: training loss: 912.401367, consuming time:60.0892 s\n",
      "[epoch 6960]: training loss: 1232.129639, consuming time:59.6974 s\n",
      "[epoch 6961]: training loss: 720.008301, consuming time:60.0820 s\n",
      "[epoch 6962]: training loss: 1156.950684, consuming time:60.9173 s\n",
      "[epoch 6963]: training loss: 876.795166, consuming time:60.9071 s\n",
      "[epoch 6964]: training loss: 919.633545, consuming time:61.2430 s\n",
      "[epoch 6965]: training loss: 929.820374, consuming time:60.3469 s\n",
      "[epoch 6966]: training loss: 945.492371, consuming time:60.5194 s\n",
      "[epoch 6967]: training loss: 936.236450, consuming time:60.2015 s\n",
      "[epoch 6968]: training loss: 875.854370, consuming time:60.4645 s\n",
      "[epoch 6969]: training loss: 936.349731, consuming time:60.7348 s\n",
      "[epoch 6970]: training loss: 819.817505, consuming time:60.8551 s\n",
      "[epoch 6971]: training loss: 921.028809, consuming time:60.6637 s\n",
      "[epoch 6972]: training loss: 1193.539795, consuming time:60.9524 s\n",
      "[epoch 6973]: training loss: 1008.787842, consuming time:60.6773 s\n",
      "[epoch 6974]: training loss: 802.926208, consuming time:62.1111 s\n",
      "[epoch 6975]: training loss: 833.738953, consuming time:61.5735 s\n",
      "[epoch 6976]: training loss: 762.416504, consuming time:60.9193 s\n",
      "[epoch 6977]: training loss: 930.982666, consuming time:60.7354 s\n",
      "[epoch 6978]: training loss: 956.181885, consuming time:61.0882 s\n",
      "[epoch 6979]: training loss: 848.612976, consuming time:61.0482 s\n",
      "[epoch 6980]: training loss: 806.843384, consuming time:60.8100 s\n",
      "[epoch 6981]: training loss: 1044.054932, consuming time:60.9851 s\n",
      "[epoch 6982]: training loss: 1006.366821, consuming time:62.0443 s\n",
      "[epoch 6983]: training loss: 965.218384, consuming time:65.5409 s\n",
      "[epoch 6984]: training loss: 764.633057, consuming time:62.8745 s\n",
      "[epoch 6985]: training loss: 940.081909, consuming time:62.4929 s\n",
      "[epoch 6986]: training loss: 975.040527, consuming time:62.2562 s\n",
      "[epoch 6987]: training loss: 870.118774, consuming time:63.3407 s\n",
      "[epoch 6988]: training loss: 1147.306396, consuming time:63.3736 s\n",
      "[epoch 6989]: training loss: 955.047607, consuming time:65.5596 s\n",
      "[epoch 6990]: training loss: 805.057373, consuming time:64.1801 s\n",
      "[epoch 6991]: training loss: 1197.981567, consuming time:63.0117 s\n",
      "[epoch 6992]: training loss: 1173.700684, consuming time:62.4521 s\n",
      "[epoch 6993]: training loss: 807.083679, consuming time:61.9988 s\n",
      "[epoch 6994]: training loss: 760.777588, consuming time:62.8668 s\n",
      "[epoch 6995]: training loss: 971.261841, consuming time:62.9788 s\n",
      "[epoch 6996]: training loss: 913.466125, consuming time:63.2771 s\n",
      "[epoch 6997]: training loss: 1111.934326, consuming time:62.3643 s\n",
      "[epoch 6998]: training loss: 1004.838928, consuming time:64.5179 s\n",
      "[epoch 6999]: training loss: 877.803955, consuming time:64.5370 s\n",
      "[epoch 7000]: training loss: 821.729675, consuming time:63.0333 s\n",
      "[epoch 7001]: training loss: 831.185913, consuming time:65.0360 s\n",
      "[epoch 7002]: training loss: 1067.116699, consuming time:65.4562 s\n",
      "[epoch 7003]: training loss: 1018.017822, consuming time:62.5786 s\n",
      "[epoch 7004]: training loss: 931.884094, consuming time:62.0040 s\n",
      "[epoch 7005]: training loss: 951.107422, consuming time:61.5077 s\n",
      "[epoch 7006]: training loss: 925.187988, consuming time:63.5558 s\n",
      "[epoch 7007]: training loss: 822.596680, consuming time:60.7568 s\n",
      "[epoch 7008]: training loss: 775.630737, consuming time:60.7009 s\n",
      "[epoch 7009]: training loss: 866.099731, consuming time:61.2934 s\n",
      "[epoch 7010]: training loss: 1108.859863, consuming time:60.7600 s\n",
      "[epoch 7011]: training loss: 913.420288, consuming time:61.1008 s\n",
      "[epoch 7012]: training loss: 800.874634, consuming time:61.2001 s\n",
      "[epoch 7013]: training loss: 949.275879, consuming time:61.0633 s\n",
      "[epoch 7014]: training loss: 1098.459229, consuming time:61.0468 s\n",
      "[epoch 7015]: training loss: 820.243896, consuming time:60.9297 s\n",
      "[epoch 7016]: training loss: 793.867554, consuming time:61.2632 s\n",
      "[epoch 7017]: training loss: 1190.708008, consuming time:61.3480 s\n",
      "[epoch 7018]: training loss: 1250.932129, consuming time:60.8167 s\n",
      "[epoch 7019]: training loss: 1415.863403, consuming time:61.0914 s\n",
      "[epoch 7020]: training loss: 888.302612, consuming time:64.6537 s\n",
      "[epoch 7021]: training loss: 1093.116821, consuming time:62.3809 s\n",
      "[epoch 7022]: training loss: 909.895996, consuming time:61.6881 s\n",
      "[epoch 7023]: training loss: 942.790283, consuming time:61.7792 s\n",
      "[epoch 7024]: training loss: 916.054016, consuming time:62.5622 s\n",
      "[epoch 7025]: training loss: 868.802612, consuming time:65.0507 s\n",
      "[epoch 7026]: training loss: 895.029968, consuming time:60.8953 s\n",
      "[epoch 7027]: training loss: 860.465942, consuming time:61.3066 s\n",
      "[epoch 7028]: training loss: 814.330139, consuming time:60.8716 s\n",
      "[epoch 7029]: training loss: 755.021484, consuming time:61.3030 s\n",
      "[epoch 7030]: training loss: 780.901611, consuming time:62.4320 s\n",
      "[epoch 7031]: training loss: 899.722778, consuming time:62.1483 s\n",
      "[epoch 7032]: training loss: 1239.473633, consuming time:61.5585 s\n",
      "[epoch 7033]: training loss: 591.129272, consuming time:60.9452 s\n",
      "[epoch 7034]: training loss: 1038.555664, consuming time:63.6167 s\n",
      "[epoch 7035]: training loss: 1072.570801, consuming time:64.7308 s\n",
      "[epoch 7036]: training loss: 735.223389, consuming time:64.3542 s\n",
      "[epoch 7037]: training loss: 813.621338, consuming time:64.1132 s\n",
      "[epoch 7038]: training loss: 660.114990, consuming time:62.3460 s\n",
      "[epoch 7039]: training loss: 755.976868, consuming time:61.3124 s\n",
      "[epoch 7040]: training loss: 599.565247, consuming time:60.9349 s\n",
      "[epoch 7041]: training loss: 823.547363, consuming time:63.0616 s\n",
      "[epoch 7042]: training loss: 795.925049, consuming time:62.3317 s\n",
      "[epoch 7043]: training loss: 1099.438232, consuming time:66.3565 s\n",
      "[epoch 7044]: training loss: 1138.030029, consuming time:66.4676 s\n",
      "[epoch 7045]: training loss: 980.367493, consuming time:64.7069 s\n",
      "[epoch 7046]: training loss: 971.651367, consuming time:64.7887 s\n",
      "[epoch 7047]: training loss: 895.081543, consuming time:61.2203 s\n",
      "[epoch 7048]: training loss: 989.943848, consuming time:64.6147 s\n",
      "[epoch 7049]: training loss: 913.595825, consuming time:64.2123 s\n",
      "[epoch 7050]: training loss: 895.198669, consuming time:63.8910 s\n",
      "[epoch 7051]: training loss: 805.122131, consuming time:62.0207 s\n",
      "[epoch 7052]: training loss: 1027.684448, consuming time:63.3778 s\n",
      "[epoch 7053]: training loss: 929.028076, consuming time:60.9511 s\n",
      "[epoch 7054]: training loss: 1205.999023, consuming time:60.5657 s\n",
      "[epoch 7055]: training loss: 1160.294678, consuming time:60.5975 s\n",
      "[epoch 7056]: training loss: 788.241577, consuming time:64.4208 s\n",
      "[epoch 7057]: training loss: 984.994629, consuming time:62.1894 s\n",
      "[epoch 7058]: training loss: 1184.555908, consuming time:60.8790 s\n",
      "[epoch 7059]: training loss: 950.215515, consuming time:60.7779 s\n",
      "[epoch 7060]: training loss: 1451.872559, consuming time:61.0430 s\n",
      "[epoch 7061]: training loss: 991.093140, consuming time:61.0138 s\n",
      "[epoch 7062]: training loss: 751.803101, consuming time:60.9517 s\n",
      "[epoch 7063]: training loss: 1155.918457, consuming time:60.8941 s\n",
      "[epoch 7064]: training loss: 846.492371, consuming time:60.3657 s\n",
      "[epoch 7065]: training loss: 1349.495361, consuming time:60.6812 s\n",
      "[epoch 7066]: training loss: 695.982849, consuming time:60.8339 s\n",
      "[epoch 7067]: training loss: 958.988525, consuming time:61.0750 s\n",
      "[epoch 7068]: training loss: 835.096558, consuming time:66.5259 s\n",
      "[epoch 7069]: training loss: 1128.999146, consuming time:60.7847 s\n",
      "[epoch 7070]: training loss: 1157.197998, consuming time:60.8640 s\n",
      "[epoch 7071]: training loss: 1025.152344, consuming time:60.9499 s\n",
      "[epoch 7072]: training loss: 881.139954, consuming time:60.9061 s\n",
      "[epoch 7073]: training loss: 1045.322144, consuming time:60.8189 s\n",
      "[epoch 7074]: training loss: 753.948242, consuming time:60.8106 s\n",
      "[epoch 7075]: training loss: 1144.492920, consuming time:60.8087 s\n",
      "[epoch 7076]: training loss: 1161.623535, consuming time:60.3627 s\n",
      "[epoch 7077]: training loss: 1274.626221, consuming time:60.6376 s\n",
      "[epoch 7078]: training loss: 974.138550, consuming time:60.6133 s\n",
      "[epoch 7079]: training loss: 927.195068, consuming time:60.7461 s\n",
      "[epoch 7080]: training loss: 1176.157227, consuming time:60.4871 s\n",
      "[epoch 7081]: training loss: 1044.546387, consuming time:60.8020 s\n",
      "[epoch 7082]: training loss: 1055.198975, consuming time:60.6697 s\n",
      "[epoch 7083]: training loss: 806.049133, consuming time:60.8268 s\n",
      "[epoch 7084]: training loss: 783.862000, consuming time:60.7527 s\n",
      "[epoch 7085]: training loss: 1002.396729, consuming time:60.5598 s\n",
      "[epoch 7086]: training loss: 1114.859375, consuming time:60.9079 s\n",
      "[epoch 7087]: training loss: 823.271729, consuming time:60.6233 s\n",
      "[epoch 7088]: training loss: 957.649902, consuming time:60.5564 s\n",
      "[epoch 7089]: training loss: 753.190002, consuming time:60.9605 s\n",
      "[epoch 7090]: training loss: 896.687866, consuming time:60.7441 s\n",
      "[epoch 7091]: training loss: 828.696350, consuming time:60.7683 s\n",
      "[epoch 7092]: training loss: 759.869507, consuming time:60.8621 s\n",
      "[epoch 7093]: training loss: 977.837891, consuming time:60.6114 s\n",
      "[epoch 7094]: training loss: 1123.445312, consuming time:60.7070 s\n",
      "[epoch 7095]: training loss: 915.441650, consuming time:61.0732 s\n",
      "[epoch 7096]: training loss: 806.878296, consuming time:61.0258 s\n",
      "[epoch 7097]: training loss: 703.649780, consuming time:60.8720 s\n",
      "[epoch 7098]: training loss: 790.616272, consuming time:61.0861 s\n",
      "[epoch 7099]: training loss: 1212.708618, consuming time:60.8375 s\n",
      "[epoch 7100]: training loss: 842.503418, consuming time:60.9084 s\n",
      "[epoch 7101]: training loss: 1004.280029, consuming time:60.7542 s\n",
      "[epoch 7102]: training loss: 932.590576, consuming time:60.8957 s\n",
      "[epoch 7103]: training loss: 913.581787, consuming time:60.8173 s\n",
      "[epoch 7104]: training loss: 1089.119385, consuming time:61.1252 s\n",
      "[epoch 7105]: training loss: 885.980469, consuming time:60.5528 s\n",
      "[epoch 7106]: training loss: 620.406006, consuming time:60.5797 s\n",
      "[epoch 7107]: training loss: 1079.958984, consuming time:60.5808 s\n",
      "[epoch 7108]: training loss: 910.991516, consuming time:61.3829 s\n",
      "[epoch 7109]: training loss: 1041.328125, consuming time:60.9293 s\n",
      "[epoch 7110]: training loss: 1118.355103, consuming time:60.8348 s\n",
      "[epoch 7111]: training loss: 808.997925, consuming time:60.4136 s\n",
      "[epoch 7112]: training loss: 790.581787, consuming time:60.7022 s\n",
      "[epoch 7113]: training loss: 908.807129, consuming time:60.6001 s\n",
      "[epoch 7114]: training loss: 1084.941162, consuming time:60.7308 s\n",
      "[epoch 7115]: training loss: 871.260986, consuming time:60.8142 s\n",
      "[epoch 7116]: training loss: 975.198303, consuming time:60.6628 s\n",
      "[epoch 7117]: training loss: 871.226807, consuming time:60.7487 s\n",
      "[epoch 7118]: training loss: 709.481934, consuming time:60.5831 s\n",
      "[epoch 7119]: training loss: 991.990234, consuming time:60.5926 s\n",
      "[epoch 7120]: training loss: 777.252808, consuming time:60.6357 s\n",
      "[epoch 7121]: training loss: 1189.059814, consuming time:60.5821 s\n",
      "[epoch 7122]: training loss: 881.156128, consuming time:60.3320 s\n",
      "[epoch 7123]: training loss: 738.186279, consuming time:60.5745 s\n",
      "[epoch 7124]: training loss: 849.992188, consuming time:60.8023 s\n",
      "[epoch 7125]: training loss: 888.233459, consuming time:60.6425 s\n",
      "[epoch 7126]: training loss: 759.084290, consuming time:60.8163 s\n",
      "[epoch 7127]: training loss: 958.227783, consuming time:60.7793 s\n",
      "[epoch 7128]: training loss: 764.310791, consuming time:60.5639 s\n",
      "[epoch 7129]: training loss: 1071.027100, consuming time:60.7973 s\n",
      "[epoch 7130]: training loss: 994.804199, consuming time:60.9505 s\n",
      "[epoch 7131]: training loss: 979.122314, consuming time:60.8063 s\n",
      "[epoch 7132]: training loss: 896.719360, consuming time:60.8664 s\n",
      "[epoch 7133]: training loss: 978.907837, consuming time:60.4844 s\n",
      "[epoch 7134]: training loss: 870.276001, consuming time:61.0217 s\n",
      "[epoch 7135]: training loss: 1070.321777, consuming time:60.7138 s\n",
      "[epoch 7136]: training loss: 1024.597168, consuming time:60.9159 s\n",
      "[epoch 7137]: training loss: 908.089600, consuming time:60.9460 s\n",
      "[epoch 7138]: training loss: 724.180847, consuming time:60.7733 s\n",
      "[epoch 7139]: training loss: 1207.340332, consuming time:60.8348 s\n",
      "[epoch 7140]: training loss: 1177.566895, consuming time:60.7973 s\n",
      "[epoch 7141]: training loss: 913.217285, consuming time:60.7353 s\n",
      "[epoch 7142]: training loss: 1063.389404, consuming time:60.7984 s\n",
      "[epoch 7143]: training loss: 716.350464, consuming time:61.0067 s\n",
      "[epoch 7144]: training loss: 1277.369385, consuming time:60.7480 s\n",
      "[epoch 7145]: training loss: 1070.496094, consuming time:60.9612 s\n",
      "[epoch 7146]: training loss: 1069.916016, consuming time:61.0370 s\n",
      "[epoch 7147]: training loss: 868.064697, consuming time:60.6845 s\n",
      "[epoch 7148]: training loss: 1124.275513, consuming time:60.8396 s\n",
      "[epoch 7149]: training loss: 1084.452393, consuming time:60.9702 s\n",
      "[epoch 7150]: training loss: 1147.576660, consuming time:61.1540 s\n",
      "[epoch 7151]: training loss: 856.606079, consuming time:60.6312 s\n",
      "[epoch 7152]: training loss: 1145.851562, consuming time:60.8255 s\n",
      "[epoch 7153]: training loss: 1159.636963, consuming time:60.5282 s\n",
      "[epoch 7154]: training loss: 1215.936523, consuming time:62.1418 s\n",
      "[epoch 7155]: training loss: 974.071899, consuming time:70.7549 s\n",
      "[epoch 7156]: training loss: 988.180664, consuming time:69.0027 s\n",
      "[epoch 7157]: training loss: 795.463623, consuming time:82.7521 s\n",
      "[epoch 7158]: training loss: 859.711548, consuming time:103.9327 s\n",
      "[epoch 7159]: training loss: 819.229858, consuming time:108.6525 s\n",
      "[epoch 7160]: training loss: 924.803223, consuming time:106.1460 s\n",
      "[epoch 7161]: training loss: 756.788086, consuming time:103.4976 s\n",
      "[epoch 7162]: training loss: 1101.837769, consuming time:102.6485 s\n",
      "[epoch 7163]: training loss: 798.347778, consuming time:105.2631 s\n",
      "[epoch 7164]: training loss: 799.592651, consuming time:103.4978 s\n",
      "[epoch 7165]: training loss: 800.163208, consuming time:104.7202 s\n",
      "[epoch 7166]: training loss: 745.764038, consuming time:100.8984 s\n",
      "[epoch 7167]: training loss: 908.788696, consuming time:102.5474 s\n",
      "[epoch 7168]: training loss: 1268.206909, consuming time:95.6265 s\n",
      "[epoch 7169]: training loss: 778.833496, consuming time:72.0065 s\n",
      "[epoch 7170]: training loss: 815.377319, consuming time:73.9129 s\n",
      "[epoch 7171]: training loss: 945.113403, consuming time:105.6151 s\n",
      "[epoch 7172]: training loss: 872.523865, consuming time:104.1402 s\n",
      "[epoch 7173]: training loss: 805.687744, consuming time:105.8612 s\n",
      "[epoch 7174]: training loss: 819.642578, consuming time:101.1766 s\n",
      "[epoch 7175]: training loss: 1011.283813, consuming time:104.6229 s\n",
      "[epoch 7176]: training loss: 800.186035, consuming time:103.6958 s\n",
      "[epoch 7177]: training loss: 1109.412354, consuming time:102.2575 s\n",
      "[epoch 7178]: training loss: 823.846313, consuming time:81.9534 s\n",
      "[epoch 7179]: training loss: 1169.335205, consuming time:70.3189 s\n",
      "[epoch 7180]: training loss: 1053.804688, consuming time:108.5099 s\n",
      "[epoch 7181]: training loss: 880.714478, consuming time:107.8173 s\n",
      "[epoch 7182]: training loss: 883.093018, consuming time:109.0431 s\n",
      "[epoch 7183]: training loss: 843.831116, consuming time:105.7820 s\n",
      "[epoch 7184]: training loss: 831.543152, consuming time:114.8936 s\n",
      "[epoch 7185]: training loss: 1060.701416, consuming time:108.4871 s\n",
      "[epoch 7186]: training loss: 944.792603, consuming time:104.9444 s\n",
      "[epoch 7187]: training loss: 980.491699, consuming time:109.1722 s\n",
      "[epoch 7188]: training loss: 1257.508911, consuming time:107.7481 s\n",
      "[epoch 7189]: training loss: 889.451294, consuming time:105.7874 s\n",
      "[epoch 7190]: training loss: 862.809570, consuming time:107.5410 s\n",
      "[epoch 7191]: training loss: 988.711487, consuming time:86.0641 s\n",
      "[epoch 7192]: training loss: 851.233276, consuming time:71.4220 s\n",
      "[epoch 7193]: training loss: 788.248291, consuming time:68.6532 s\n",
      "[epoch 7194]: training loss: 842.369568, consuming time:89.1113 s\n",
      "[epoch 7195]: training loss: 957.367432, consuming time:103.7896 s\n",
      "[epoch 7196]: training loss: 942.641479, consuming time:105.6375 s\n",
      "[epoch 7197]: training loss: 944.213013, consuming time:104.0820 s\n",
      "[epoch 7198]: training loss: 602.234497, consuming time:110.0813 s\n",
      "[epoch 7199]: training loss: 818.854248, consuming time:104.7054 s\n",
      "[epoch 7200]: training loss: 939.894226, consuming time:107.1437 s\n",
      "[epoch 7201]: training loss: 975.736938, consuming time:115.2462 s\n",
      "[epoch 7202]: training loss: 1093.433716, consuming time:102.9381 s\n",
      "[epoch 7203]: training loss: 979.689087, consuming time:90.9986 s\n",
      "[epoch 7204]: training loss: 855.062622, consuming time:73.0763 s\n",
      "[epoch 7205]: training loss: 1028.672119, consuming time:72.8617 s\n",
      "[epoch 7206]: training loss: 702.581055, consuming time:81.1811 s\n",
      "[epoch 7207]: training loss: 1100.653931, consuming time:126.3757 s\n",
      "[epoch 7208]: training loss: 780.468018, consuming time:124.0585 s\n",
      "[epoch 7209]: training loss: 886.860168, consuming time:131.6518 s\n",
      "[epoch 7210]: training loss: 944.575317, consuming time:124.9360 s\n",
      "[epoch 7211]: training loss: 828.523682, consuming time:129.5454 s\n",
      "[epoch 7212]: training loss: 940.186768, consuming time:136.7705 s\n",
      "[epoch 7213]: training loss: 1128.425415, consuming time:125.5978 s\n",
      "[epoch 7214]: training loss: 1286.825684, consuming time:130.3200 s\n",
      "[epoch 7215]: training loss: 1069.427490, consuming time:144.2134 s\n",
      "[epoch 7216]: training loss: 979.657410, consuming time:104.7195 s\n",
      "[epoch 7217]: training loss: 856.558411, consuming time:73.4729 s\n",
      "[epoch 7218]: training loss: 1109.182983, consuming time:71.2244 s\n",
      "[epoch 7219]: training loss: 1179.667725, consuming time:126.0610 s\n",
      "[epoch 7220]: training loss: 1027.187744, consuming time:122.8045 s\n",
      "[epoch 7221]: training loss: 627.515015, consuming time:124.4967 s\n",
      "[epoch 7222]: training loss: 872.389771, consuming time:124.0226 s\n",
      "[epoch 7223]: training loss: 1002.975891, consuming time:121.2909 s\n",
      "[epoch 7224]: training loss: 936.946899, consuming time:124.3308 s\n",
      "[epoch 7225]: training loss: 710.610168, consuming time:125.1983 s\n",
      "[epoch 7226]: training loss: 860.806641, consuming time:130.3923 s\n",
      "[epoch 7227]: training loss: 842.363770, consuming time:100.2822 s\n",
      "[epoch 7228]: training loss: 697.926025, consuming time:72.8644 s\n",
      "[epoch 7229]: training loss: 813.732910, consuming time:89.0046 s\n",
      "[epoch 7230]: training loss: 956.624268, consuming time:121.8477 s\n",
      "[epoch 7231]: training loss: 932.741821, consuming time:125.5143 s\n",
      "[epoch 7232]: training loss: 902.916138, consuming time:126.2838 s\n",
      "[epoch 7233]: training loss: 1000.191650, consuming time:115.2975 s\n",
      "[epoch 7234]: training loss: 1100.909180, consuming time:125.1023 s\n",
      "[epoch 7235]: training loss: 1069.383789, consuming time:119.9113 s\n",
      "[epoch 7236]: training loss: 1030.787109, consuming time:74.0761 s\n",
      "[epoch 7237]: training loss: 1145.463989, consuming time:70.4383 s\n",
      "[epoch 7238]: training loss: 1080.208130, consuming time:66.5105 s\n",
      "[epoch 7239]: training loss: 1065.000732, consuming time:66.6151 s\n",
      "[epoch 7240]: training loss: 859.405640, consuming time:130.1533 s\n",
      "[epoch 7241]: training loss: 953.688721, consuming time:125.8507 s\n",
      "[epoch 7242]: training loss: 858.540527, consuming time:125.7762 s\n",
      "[epoch 7243]: training loss: 1018.549622, consuming time:121.2564 s\n",
      "[epoch 7244]: training loss: 1049.320435, consuming time:127.3390 s\n",
      "[epoch 7245]: training loss: 915.458496, consuming time:126.0697 s\n",
      "[epoch 7246]: training loss: 821.945923, consuming time:73.6478 s\n",
      "[epoch 7247]: training loss: 1032.106812, consuming time:72.1898 s\n",
      "[epoch 7248]: training loss: 851.036011, consuming time:70.0917 s\n",
      "[epoch 7249]: training loss: 802.929565, consuming time:66.7141 s\n",
      "[epoch 7250]: training loss: 885.450867, consuming time:127.5783 s\n",
      "[epoch 7251]: training loss: 787.143677, consuming time:122.8304 s\n",
      "[epoch 7252]: training loss: 972.246704, consuming time:118.4814 s\n",
      "[epoch 7253]: training loss: 1103.761108, consuming time:122.8060 s\n",
      "[epoch 7254]: training loss: 645.232422, consuming time:119.5237 s\n",
      "[epoch 7255]: training loss: 836.958252, consuming time:125.5787 s\n",
      "[epoch 7256]: training loss: 883.951111, consuming time:107.8457 s\n",
      "[epoch 7257]: training loss: 939.901428, consuming time:68.4793 s\n",
      "[epoch 7258]: training loss: 909.994019, consuming time:68.8708 s\n",
      "[epoch 7259]: training loss: 815.490479, consuming time:68.3761 s\n",
      "[epoch 7260]: training loss: 1022.988708, consuming time:69.4608 s\n",
      "[epoch 7261]: training loss: 981.715698, consuming time:69.3019 s\n",
      "[epoch 7262]: training loss: 1022.891968, consuming time:69.3873 s\n",
      "[epoch 7263]: training loss: 1053.466309, consuming time:69.4776 s\n",
      "[epoch 7264]: training loss: 710.499390, consuming time:69.1184 s\n",
      "[epoch 7265]: training loss: 1146.948486, consuming time:69.0541 s\n",
      "[epoch 7266]: training loss: 1010.730042, consuming time:69.0947 s\n",
      "[epoch 7267]: training loss: 773.211792, consuming time:69.2189 s\n",
      "[epoch 7268]: training loss: 883.305298, consuming time:69.1648 s\n",
      "[epoch 7269]: training loss: 1044.472168, consuming time:69.1509 s\n",
      "[epoch 7270]: training loss: 852.694336, consuming time:69.2611 s\n",
      "[epoch 7271]: training loss: 764.705627, consuming time:69.1357 s\n",
      "[epoch 7272]: training loss: 1060.183350, consuming time:65.6628 s\n",
      "[epoch 7273]: training loss: 839.914795, consuming time:62.4294 s\n",
      "[epoch 7274]: training loss: 807.174316, consuming time:62.3632 s\n",
      "[epoch 7275]: training loss: 1037.300171, consuming time:60.7735 s\n",
      "[epoch 7276]: training loss: 822.914551, consuming time:60.7810 s\n",
      "[epoch 7277]: training loss: 988.564209, consuming time:60.8109 s\n",
      "[epoch 7278]: training loss: 851.537170, consuming time:60.6682 s\n",
      "[epoch 7279]: training loss: 1176.089478, consuming time:60.4245 s\n",
      "[epoch 7280]: training loss: 743.515747, consuming time:60.9146 s\n",
      "[epoch 7281]: training loss: 798.808960, consuming time:60.8496 s\n",
      "[epoch 7282]: training loss: 984.920898, consuming time:60.9054 s\n",
      "[epoch 7283]: training loss: 1030.615479, consuming time:61.2183 s\n",
      "[epoch 7284]: training loss: 1281.823975, consuming time:60.6925 s\n",
      "[epoch 7285]: training loss: 968.824646, consuming time:60.9071 s\n",
      "[epoch 7286]: training loss: 1031.198486, consuming time:61.1086 s\n",
      "[epoch 7287]: training loss: 569.166382, consuming time:60.8708 s\n",
      "[epoch 7288]: training loss: 944.835938, consuming time:61.0596 s\n",
      "[epoch 7289]: training loss: 813.092163, consuming time:60.5366 s\n",
      "[epoch 7290]: training loss: 705.376343, consuming time:61.3735 s\n",
      "[epoch 7291]: training loss: 707.926514, consuming time:61.0323 s\n",
      "[epoch 7292]: training loss: 831.220642, consuming time:63.4926 s\n",
      "[epoch 7293]: training loss: 872.070068, consuming time:61.0774 s\n",
      "[epoch 7294]: training loss: 700.187134, consuming time:61.5505 s\n",
      "[epoch 7295]: training loss: 1076.697266, consuming time:60.7419 s\n",
      "[epoch 7296]: training loss: 769.823975, consuming time:61.2046 s\n",
      "[epoch 7297]: training loss: 921.225098, consuming time:61.0260 s\n",
      "[epoch 7298]: training loss: 911.393127, consuming time:61.1500 s\n",
      "[epoch 7299]: training loss: 801.303223, consuming time:61.6624 s\n",
      "[epoch 7300]: training loss: 1002.060913, consuming time:61.5048 s\n",
      "[epoch 7301]: training loss: 1037.438477, consuming time:61.2497 s\n",
      "[epoch 7302]: training loss: 915.757019, consuming time:61.5273 s\n",
      "[epoch 7303]: training loss: 1000.158569, consuming time:61.2397 s\n",
      "[epoch 7304]: training loss: 831.173889, consuming time:61.1912 s\n",
      "[epoch 7305]: training loss: 846.813599, consuming time:61.1818 s\n",
      "[epoch 7306]: training loss: 1035.939087, consuming time:61.0551 s\n",
      "[epoch 7307]: training loss: 771.461914, consuming time:60.8092 s\n",
      "[epoch 7308]: training loss: 1186.208130, consuming time:60.9520 s\n",
      "[epoch 7309]: training loss: 649.902161, consuming time:60.5129 s\n",
      "[epoch 7310]: training loss: 741.168884, consuming time:60.9034 s\n",
      "[epoch 7311]: training loss: 778.591431, consuming time:61.0780 s\n",
      "[epoch 7312]: training loss: 621.232239, consuming time:60.8287 s\n",
      "[epoch 7313]: training loss: 759.939636, consuming time:61.1398 s\n",
      "[epoch 7314]: training loss: 1085.671265, consuming time:60.7820 s\n",
      "[epoch 7315]: training loss: 788.472168, consuming time:60.7701 s\n",
      "[epoch 7316]: training loss: 906.736816, consuming time:60.7704 s\n",
      "[epoch 7317]: training loss: 900.380798, consuming time:61.3418 s\n",
      "[epoch 7318]: training loss: 747.168823, consuming time:61.5530 s\n",
      "[epoch 7319]: training loss: 873.934448, consuming time:61.2162 s\n",
      "[epoch 7320]: training loss: 707.964600, consuming time:61.5730 s\n",
      "[epoch 7321]: training loss: 950.867981, consuming time:61.7870 s\n",
      "[epoch 7322]: training loss: 815.835815, consuming time:61.7235 s\n",
      "[epoch 7323]: training loss: 1126.515625, consuming time:61.4829 s\n",
      "[epoch 7324]: training loss: 894.954590, consuming time:61.8579 s\n",
      "[epoch 7325]: training loss: 713.236694, consuming time:61.8037 s\n",
      "[epoch 7326]: training loss: 878.533203, consuming time:62.3049 s\n",
      "[epoch 7327]: training loss: 1089.671387, consuming time:66.2591 s\n",
      "[epoch 7328]: training loss: 798.478760, consuming time:65.6239 s\n",
      "[epoch 7329]: training loss: 1038.529053, consuming time:62.0071 s\n",
      "[epoch 7330]: training loss: 1045.614380, consuming time:61.4679 s\n",
      "[epoch 7331]: training loss: 708.468384, consuming time:61.4361 s\n",
      "[epoch 7332]: training loss: 916.748779, consuming time:61.3984 s\n",
      "[epoch 7333]: training loss: 1035.178711, consuming time:61.1948 s\n",
      "[epoch 7334]: training loss: 1128.700684, consuming time:61.0504 s\n",
      "[epoch 7335]: training loss: 717.607910, consuming time:61.0236 s\n",
      "[epoch 7336]: training loss: 804.500977, consuming time:64.4965 s\n",
      "[epoch 7337]: training loss: 935.330872, consuming time:62.6869 s\n",
      "[epoch 7338]: training loss: 961.789673, consuming time:62.2300 s\n",
      "[epoch 7339]: training loss: 913.861206, consuming time:62.0822 s\n",
      "[epoch 7340]: training loss: 1008.495300, consuming time:61.4989 s\n",
      "[epoch 7341]: training loss: 1248.361816, consuming time:61.4974 s\n",
      "[epoch 7342]: training loss: 899.094727, consuming time:61.3468 s\n",
      "[epoch 7343]: training loss: 1058.225098, consuming time:64.2656 s\n",
      "[epoch 7344]: training loss: 916.613220, consuming time:63.5877 s\n",
      "[epoch 7345]: training loss: 960.340881, consuming time:61.8980 s\n",
      "[epoch 7346]: training loss: 1017.821167, consuming time:61.0899 s\n",
      "[epoch 7347]: training loss: 882.933960, consuming time:60.9087 s\n",
      "[epoch 7348]: training loss: 835.102783, consuming time:61.5341 s\n",
      "[epoch 7349]: training loss: 902.330627, consuming time:60.5511 s\n",
      "[epoch 7350]: training loss: 811.762146, consuming time:60.9520 s\n",
      "[epoch 7351]: training loss: 869.173340, consuming time:61.3620 s\n",
      "[epoch 7352]: training loss: 1248.525879, consuming time:61.2190 s\n",
      "[epoch 7353]: training loss: 861.703308, consuming time:61.5372 s\n",
      "[epoch 7354]: training loss: 1183.532959, consuming time:60.9801 s\n",
      "[epoch 7355]: training loss: 974.656982, consuming time:61.0720 s\n",
      "[epoch 7356]: training loss: 730.247131, consuming time:61.0484 s\n",
      "[epoch 7357]: training loss: 635.649536, consuming time:61.0229 s\n",
      "[epoch 7358]: training loss: 938.511902, consuming time:60.8889 s\n",
      "[epoch 7359]: training loss: 969.587158, consuming time:61.1595 s\n",
      "[epoch 7360]: training loss: 1156.415527, consuming time:60.4712 s\n",
      "[epoch 7361]: training loss: 1031.359375, consuming time:60.9574 s\n",
      "[epoch 7362]: training loss: 1165.885254, consuming time:60.9288 s\n",
      "[epoch 7363]: training loss: 958.954346, consuming time:61.8840 s\n",
      "[epoch 7364]: training loss: 1041.416260, consuming time:61.3085 s\n",
      "[epoch 7365]: training loss: 922.863403, consuming time:60.7216 s\n",
      "[epoch 7366]: training loss: 808.976318, consuming time:60.9372 s\n",
      "[epoch 7367]: training loss: 922.762451, consuming time:60.4973 s\n",
      "[epoch 7368]: training loss: 828.789490, consuming time:60.2774 s\n",
      "[epoch 7369]: training loss: 915.062500, consuming time:63.0257 s\n",
      "[epoch 7370]: training loss: 1020.867310, consuming time:61.4377 s\n",
      "[epoch 7371]: training loss: 825.731445, consuming time:61.4621 s\n",
      "[epoch 7372]: training loss: 986.222534, consuming time:61.3498 s\n",
      "[epoch 7373]: training loss: 949.626160, consuming time:63.7709 s\n",
      "[epoch 7374]: training loss: 994.065613, consuming time:62.3060 s\n",
      "[epoch 7375]: training loss: 832.519531, consuming time:63.5562 s\n",
      "[epoch 7376]: training loss: 1187.953491, consuming time:61.2293 s\n",
      "[epoch 7377]: training loss: 1088.695679, consuming time:61.0477 s\n",
      "[epoch 7378]: training loss: 1012.067566, consuming time:60.0268 s\n",
      "[epoch 7379]: training loss: 932.289429, consuming time:59.7643 s\n",
      "[epoch 7380]: training loss: 927.802612, consuming time:60.3617 s\n",
      "[epoch 7381]: training loss: 1168.658203, consuming time:60.4531 s\n",
      "[epoch 7382]: training loss: 772.955566, consuming time:60.4456 s\n",
      "[epoch 7383]: training loss: 1081.327881, consuming time:60.8585 s\n",
      "[epoch 7384]: training loss: 1033.800781, consuming time:59.9180 s\n",
      "[epoch 7385]: training loss: 1727.063232, consuming time:59.9111 s\n",
      "[epoch 7386]: training loss: 863.028931, consuming time:60.2021 s\n",
      "[epoch 7387]: training loss: 1071.791016, consuming time:60.2905 s\n",
      "[epoch 7388]: training loss: 744.558472, consuming time:60.3917 s\n",
      "[epoch 7389]: training loss: 878.298889, consuming time:59.9636 s\n",
      "[epoch 7390]: training loss: 840.805298, consuming time:59.8552 s\n",
      "[epoch 7391]: training loss: 898.045166, consuming time:59.8087 s\n",
      "[epoch 7392]: training loss: 1099.178345, consuming time:60.2442 s\n",
      "[epoch 7393]: training loss: 967.821777, consuming time:60.5007 s\n",
      "[epoch 7394]: training loss: 785.182617, consuming time:60.3272 s\n",
      "[epoch 7395]: training loss: 797.334595, consuming time:60.2424 s\n",
      "[epoch 7396]: training loss: 998.852295, consuming time:59.7898 s\n",
      "[epoch 7397]: training loss: 834.869019, consuming time:59.7336 s\n",
      "[epoch 7398]: training loss: 924.600586, consuming time:60.1437 s\n",
      "[epoch 7399]: training loss: 1277.972168, consuming time:60.2855 s\n",
      "[epoch 7400]: training loss: 537.697021, consuming time:60.3486 s\n",
      "[epoch 7401]: training loss: 1142.840820, consuming time:60.5061 s\n",
      "[epoch 7402]: training loss: 796.329895, consuming time:59.8401 s\n",
      "[epoch 7403]: training loss: 1130.362305, consuming time:59.8602 s\n",
      "[epoch 7404]: training loss: 1040.491089, consuming time:60.1918 s\n",
      "[epoch 7405]: training loss: 869.184570, consuming time:60.2012 s\n",
      "[epoch 7406]: training loss: 1188.093750, consuming time:60.3586 s\n",
      "[epoch 7407]: training loss: 997.150085, consuming time:60.3413 s\n",
      "[epoch 7408]: training loss: 926.165771, consuming time:59.8751 s\n",
      "[epoch 7409]: training loss: 843.064453, consuming time:60.0034 s\n",
      "[epoch 7410]: training loss: 754.581055, consuming time:60.1302 s\n",
      "[epoch 7411]: training loss: 859.730164, consuming time:60.7698 s\n",
      "[epoch 7412]: training loss: 915.601379, consuming time:60.4438 s\n",
      "[epoch 7413]: training loss: 645.440430, consuming time:60.3193 s\n",
      "[epoch 7414]: training loss: 774.849976, consuming time:59.9791 s\n",
      "[epoch 7415]: training loss: 662.934753, consuming time:59.9706 s\n",
      "[epoch 7416]: training loss: 807.523987, consuming time:60.3051 s\n",
      "[epoch 7417]: training loss: 613.256348, consuming time:60.4676 s\n",
      "[epoch 7418]: training loss: 989.133179, consuming time:60.2799 s\n",
      "[epoch 7419]: training loss: 1132.106934, consuming time:60.4006 s\n",
      "[epoch 7420]: training loss: 872.225342, consuming time:59.8438 s\n",
      "[epoch 7421]: training loss: 801.945312, consuming time:60.3044 s\n",
      "[epoch 7422]: training loss: 819.457764, consuming time:60.4395 s\n",
      "[epoch 7423]: training loss: 1032.763184, consuming time:60.4791 s\n",
      "[epoch 7424]: training loss: 1282.702759, consuming time:65.1764 s\n",
      "[epoch 7425]: training loss: 840.607056, consuming time:61.7877 s\n",
      "[epoch 7426]: training loss: 775.781982, consuming time:62.1552 s\n",
      "[epoch 7427]: training loss: 1000.655518, consuming time:64.7950 s\n",
      "[epoch 7428]: training loss: 925.027222, consuming time:61.9230 s\n",
      "[epoch 7429]: training loss: 1243.785278, consuming time:65.7689 s\n",
      "[epoch 7430]: training loss: 888.454102, consuming time:63.0607 s\n",
      "[epoch 7431]: training loss: 959.325684, consuming time:62.0555 s\n",
      "[epoch 7432]: training loss: 1295.866211, consuming time:62.0795 s\n",
      "[epoch 7433]: training loss: 1003.428955, consuming time:61.9568 s\n",
      "[epoch 7434]: training loss: 735.715698, consuming time:62.1315 s\n",
      "[epoch 7435]: training loss: 895.091553, consuming time:62.2069 s\n",
      "[epoch 7436]: training loss: 1208.613037, consuming time:62.0261 s\n",
      "[epoch 7437]: training loss: 801.912720, consuming time:62.1186 s\n",
      "[epoch 7438]: training loss: 765.935059, consuming time:61.9567 s\n",
      "[epoch 7439]: training loss: 983.859375, consuming time:61.6509 s\n",
      "[epoch 7440]: training loss: 955.145264, consuming time:62.0603 s\n",
      "[epoch 7441]: training loss: 1012.296875, consuming time:62.2725 s\n",
      "[epoch 7442]: training loss: 921.308960, consuming time:61.6850 s\n",
      "[epoch 7443]: training loss: 1076.227051, consuming time:62.3863 s\n",
      "[epoch 7444]: training loss: 699.803223, consuming time:61.8483 s\n",
      "[epoch 7445]: training loss: 906.382690, consuming time:62.1486 s\n",
      "[epoch 7446]: training loss: 881.573181, consuming time:61.8501 s\n",
      "[epoch 7447]: training loss: 855.154541, consuming time:62.1994 s\n",
      "[epoch 7448]: training loss: 995.942627, consuming time:62.0433 s\n",
      "[epoch 7449]: training loss: 1014.784485, consuming time:62.1585 s\n",
      "[epoch 7450]: training loss: 1120.203735, consuming time:62.3764 s\n",
      "[epoch 7451]: training loss: 991.626587, consuming time:62.0382 s\n",
      "[epoch 7452]: training loss: 544.256104, consuming time:62.1420 s\n",
      "[epoch 7453]: training loss: 902.435791, consuming time:62.2499 s\n",
      "[epoch 7454]: training loss: 1077.325195, consuming time:62.0705 s\n",
      "[epoch 7455]: training loss: 959.625488, consuming time:61.6598 s\n",
      "[epoch 7456]: training loss: 871.437744, consuming time:61.8244 s\n",
      "[epoch 7457]: training loss: 1021.634277, consuming time:62.0101 s\n",
      "[epoch 7458]: training loss: 1068.824585, consuming time:62.0044 s\n",
      "[epoch 7459]: training loss: 1058.980469, consuming time:62.2724 s\n",
      "[epoch 7460]: training loss: 1001.528748, consuming time:61.9947 s\n",
      "[epoch 7461]: training loss: 879.866150, consuming time:61.7530 s\n",
      "[epoch 7462]: training loss: 890.011841, consuming time:61.8952 s\n",
      "[epoch 7463]: training loss: 747.702148, consuming time:61.8996 s\n",
      "[epoch 7464]: training loss: 922.237671, consuming time:62.1173 s\n",
      "[epoch 7465]: training loss: 938.819641, consuming time:62.2050 s\n",
      "[epoch 7466]: training loss: 1270.684204, consuming time:61.7786 s\n",
      "[epoch 7467]: training loss: 949.918091, consuming time:61.9219 s\n",
      "[epoch 7468]: training loss: 902.455139, consuming time:61.9310 s\n",
      "[epoch 7469]: training loss: 797.948730, consuming time:62.3775 s\n",
      "[epoch 7470]: training loss: 983.490662, consuming time:62.0965 s\n",
      "[epoch 7471]: training loss: 950.924194, consuming time:61.9405 s\n",
      "[epoch 7472]: training loss: 887.122864, consuming time:61.8705 s\n",
      "[epoch 7473]: training loss: 929.656372, consuming time:62.0182 s\n",
      "[epoch 7474]: training loss: 1012.045105, consuming time:61.8126 s\n",
      "[epoch 7475]: training loss: 1028.931274, consuming time:62.0835 s\n",
      "[epoch 7476]: training loss: 663.672241, consuming time:62.1409 s\n",
      "[epoch 7477]: training loss: 1050.270996, consuming time:62.0263 s\n",
      "[epoch 7478]: training loss: 858.884521, consuming time:62.0171 s\n",
      "[epoch 7479]: training loss: 1053.764893, consuming time:61.6153 s\n",
      "[epoch 7480]: training loss: 808.571777, consuming time:62.0690 s\n",
      "[epoch 7481]: training loss: 796.852234, consuming time:62.0925 s\n",
      "[epoch 7482]: training loss: 874.143005, consuming time:61.9904 s\n",
      "[epoch 7483]: training loss: 1026.689697, consuming time:61.8261 s\n",
      "[epoch 7484]: training loss: 837.422607, consuming time:61.7450 s\n",
      "[epoch 7485]: training loss: 857.624268, consuming time:61.6533 s\n",
      "[epoch 7486]: training loss: 1105.901611, consuming time:62.3323 s\n",
      "[epoch 7487]: training loss: 1197.308350, consuming time:62.0201 s\n",
      "[epoch 7488]: training loss: 840.265625, consuming time:61.9788 s\n",
      "[epoch 7489]: training loss: 784.262024, consuming time:61.9305 s\n",
      "[epoch 7490]: training loss: 919.380615, consuming time:61.8572 s\n",
      "[epoch 7491]: training loss: 843.568726, consuming time:62.1250 s\n",
      "[epoch 7492]: training loss: 1102.167847, consuming time:62.4103 s\n",
      "[epoch 7493]: training loss: 970.258728, consuming time:62.5651 s\n",
      "[epoch 7494]: training loss: 1136.039795, consuming time:62.0273 s\n",
      "[epoch 7495]: training loss: 959.179932, consuming time:61.6270 s\n",
      "[epoch 7496]: training loss: 985.932129, consuming time:62.0848 s\n",
      "[epoch 7497]: training loss: 692.068237, consuming time:62.0813 s\n",
      "[epoch 7498]: training loss: 896.513428, consuming time:62.3412 s\n",
      "[epoch 7499]: training loss: 1079.123047, consuming time:62.1885 s\n",
      "[epoch 7500]: training loss: 1211.473877, consuming time:61.8801 s\n",
      "[epoch 7501]: training loss: 966.959961, consuming time:61.8498 s\n",
      "[epoch 7502]: training loss: 1009.656250, consuming time:61.6353 s\n",
      "[epoch 7503]: training loss: 1081.660889, consuming time:61.9188 s\n",
      "[epoch 7504]: training loss: 745.922241, consuming time:62.1401 s\n",
      "[epoch 7505]: training loss: 790.193787, consuming time:62.3342 s\n",
      "[epoch 7506]: training loss: 1003.921997, consuming time:62.1289 s\n",
      "[epoch 7507]: training loss: 949.785950, consuming time:62.2574 s\n",
      "[epoch 7508]: training loss: 951.715637, consuming time:61.8186 s\n",
      "[epoch 7509]: training loss: 1143.922241, consuming time:62.1303 s\n",
      "[epoch 7510]: training loss: 1250.635742, consuming time:62.1882 s\n",
      "[epoch 7511]: training loss: 805.375366, consuming time:62.0140 s\n",
      "[epoch 7512]: training loss: 799.181519, consuming time:61.7677 s\n",
      "[epoch 7513]: training loss: 701.721924, consuming time:61.9188 s\n",
      "[epoch 7514]: training loss: 855.209229, consuming time:62.1308 s\n",
      "[epoch 7515]: training loss: 722.956482, consuming time:62.1038 s\n",
      "[epoch 7516]: training loss: 666.743713, consuming time:62.1841 s\n",
      "[epoch 7517]: training loss: 1158.171753, consuming time:61.8421 s\n",
      "[epoch 7518]: training loss: 908.811707, consuming time:61.7577 s\n",
      "[epoch 7519]: training loss: 847.561157, consuming time:61.8460 s\n",
      "[epoch 7520]: training loss: 1235.860596, consuming time:62.1339 s\n",
      "[epoch 7521]: training loss: 825.839600, consuming time:62.0709 s\n",
      "[epoch 7522]: training loss: 1027.453613, consuming time:62.1487 s\n",
      "[epoch 7523]: training loss: 943.831543, consuming time:62.1209 s\n",
      "[epoch 7524]: training loss: 1105.323486, consuming time:61.7342 s\n",
      "[epoch 7525]: training loss: 922.724792, consuming time:62.0311 s\n",
      "[epoch 7526]: training loss: 769.405579, consuming time:62.1239 s\n",
      "[epoch 7527]: training loss: 1034.435059, consuming time:62.0949 s\n",
      "[epoch 7528]: training loss: 885.362732, consuming time:61.7490 s\n",
      "[epoch 7529]: training loss: 819.932617, consuming time:61.7102 s\n",
      "[epoch 7530]: training loss: 962.074646, consuming time:61.8584 s\n",
      "[epoch 7531]: training loss: 993.523804, consuming time:62.0063 s\n",
      "[epoch 7532]: training loss: 1200.204346, consuming time:62.2422 s\n",
      "[epoch 7533]: training loss: 814.247742, consuming time:61.7734 s\n",
      "[epoch 7534]: training loss: 1097.211792, consuming time:61.8628 s\n",
      "[epoch 7535]: training loss: 1147.183105, consuming time:61.9665 s\n",
      "[epoch 7536]: training loss: 840.945312, consuming time:61.9196 s\n",
      "[epoch 7537]: training loss: 793.440674, consuming time:61.8874 s\n",
      "[epoch 7538]: training loss: 1032.722168, consuming time:61.9354 s\n",
      "[epoch 7539]: training loss: 914.036011, consuming time:61.9413 s\n",
      "[epoch 7540]: training loss: 885.173096, consuming time:61.8084 s\n",
      "[epoch 7541]: training loss: 992.431824, consuming time:61.8638 s\n",
      "[epoch 7542]: training loss: 978.963745, consuming time:61.8090 s\n",
      "[epoch 7543]: training loss: 905.704834, consuming time:61.8264 s\n",
      "[epoch 7544]: training loss: 724.611206, consuming time:62.1872 s\n",
      "[epoch 7545]: training loss: 868.974304, consuming time:61.7930 s\n",
      "[epoch 7546]: training loss: 1057.634155, consuming time:61.6995 s\n",
      "[epoch 7547]: training loss: 939.855225, consuming time:61.7871 s\n",
      "[epoch 7548]: training loss: 1122.202637, consuming time:62.0483 s\n",
      "[epoch 7549]: training loss: 1019.418823, consuming time:62.1588 s\n",
      "[epoch 7550]: training loss: 703.168823, consuming time:62.0893 s\n",
      "[epoch 7551]: training loss: 1103.428955, consuming time:61.9021 s\n",
      "[epoch 7552]: training loss: 877.192749, consuming time:61.8053 s\n",
      "[epoch 7553]: training loss: 853.329285, consuming time:61.6437 s\n",
      "[epoch 7554]: training loss: 886.218750, consuming time:62.2400 s\n",
      "[epoch 7555]: training loss: 1265.061279, consuming time:62.0712 s\n",
      "[epoch 7556]: training loss: 857.826599, consuming time:62.1480 s\n",
      "[epoch 7557]: training loss: 858.876099, consuming time:61.5874 s\n",
      "[epoch 7558]: training loss: 1426.548340, consuming time:61.7188 s\n",
      "[epoch 7559]: training loss: 889.711060, consuming time:62.0895 s\n",
      "[epoch 7560]: training loss: 936.085876, consuming time:62.0330 s\n",
      "[epoch 7561]: training loss: 899.194824, consuming time:61.8666 s\n",
      "[epoch 7562]: training loss: 903.213074, consuming time:61.9318 s\n",
      "[epoch 7563]: training loss: 817.273926, consuming time:61.9010 s\n",
      "[epoch 7564]: training loss: 754.485596, consuming time:61.7759 s\n",
      "[epoch 7565]: training loss: 1252.939453, consuming time:62.6233 s\n",
      "[epoch 7566]: training loss: 943.258423, consuming time:62.1460 s\n",
      "[epoch 7567]: training loss: 923.264221, consuming time:62.0767 s\n",
      "[epoch 7568]: training loss: 1198.385132, consuming time:61.7465 s\n",
      "[epoch 7569]: training loss: 855.643372, consuming time:61.7758 s\n",
      "[epoch 7570]: training loss: 784.753601, consuming time:61.6861 s\n",
      "[epoch 7571]: training loss: 867.579468, consuming time:62.2027 s\n",
      "[epoch 7572]: training loss: 921.041504, consuming time:61.8487 s\n",
      "[epoch 7573]: training loss: 881.053894, consuming time:61.6719 s\n",
      "[epoch 7574]: training loss: 858.154663, consuming time:61.6047 s\n",
      "[epoch 7575]: training loss: 1538.056396, consuming time:61.6738 s\n",
      "[epoch 7576]: training loss: 987.443970, consuming time:61.9310 s\n",
      "[epoch 7577]: training loss: 900.298157, consuming time:62.0956 s\n",
      "[epoch 7578]: training loss: 1007.276123, consuming time:62.1830 s\n",
      "[epoch 7579]: training loss: 1056.082153, consuming time:62.1421 s\n",
      "[epoch 7580]: training loss: 737.292480, consuming time:61.7051 s\n",
      "[epoch 7581]: training loss: 1019.287598, consuming time:61.9573 s\n",
      "[epoch 7582]: training loss: 948.095337, consuming time:62.0606 s\n",
      "[epoch 7583]: training loss: 853.859985, consuming time:62.0320 s\n",
      "[epoch 7584]: training loss: 1098.170410, consuming time:61.9214 s\n",
      "[epoch 7585]: training loss: 846.526001, consuming time:61.8381 s\n",
      "[epoch 7586]: training loss: 1031.391113, consuming time:61.8618 s\n",
      "[epoch 7587]: training loss: 947.167175, consuming time:61.9710 s\n",
      "[epoch 7588]: training loss: 954.342896, consuming time:61.9467 s\n",
      "[epoch 7589]: training loss: 1081.275635, consuming time:62.2339 s\n",
      "[epoch 7590]: training loss: 833.042480, consuming time:61.8144 s\n",
      "[epoch 7591]: training loss: 736.438965, consuming time:61.8523 s\n",
      "[epoch 7592]: training loss: 748.801270, consuming time:61.6966 s\n",
      "[epoch 7593]: training loss: 866.140869, consuming time:62.0681 s\n",
      "[epoch 7594]: training loss: 998.813782, consuming time:61.7496 s\n",
      "[epoch 7595]: training loss: 768.330933, consuming time:62.0293 s\n",
      "[epoch 7596]: training loss: 1028.978638, consuming time:61.7540 s\n",
      "[epoch 7597]: training loss: 832.934143, consuming time:61.8347 s\n",
      "[epoch 7598]: training loss: 776.563049, consuming time:62.0544 s\n",
      "[epoch 7599]: training loss: 985.721680, consuming time:61.9239 s\n",
      "[epoch 7600]: training loss: 869.528931, consuming time:62.2106 s\n",
      "[epoch 7601]: training loss: 767.138794, consuming time:61.9830 s\n",
      "[epoch 7602]: training loss: 1021.249146, consuming time:61.7956 s\n",
      "[epoch 7603]: training loss: 748.759399, consuming time:61.8728 s\n",
      "[epoch 7604]: training loss: 822.293884, consuming time:62.0230 s\n",
      "[epoch 7605]: training loss: 834.020874, consuming time:62.0712 s\n",
      "[epoch 7606]: training loss: 1007.087280, consuming time:61.9791 s\n",
      "[epoch 7607]: training loss: 824.824951, consuming time:61.7233 s\n",
      "[epoch 7608]: training loss: 1146.250610, consuming time:61.9598 s\n",
      "[epoch 7609]: training loss: 1230.683960, consuming time:61.8321 s\n",
      "[epoch 7610]: training loss: 907.629639, consuming time:61.7085 s\n",
      "[epoch 7611]: training loss: 1204.948242, consuming time:62.1590 s\n",
      "[epoch 7612]: training loss: 888.074280, consuming time:61.8994 s\n",
      "[epoch 7613]: training loss: 1044.638672, consuming time:61.6512 s\n",
      "[epoch 7614]: training loss: 945.793152, consuming time:62.0847 s\n",
      "[epoch 7615]: training loss: 895.106873, consuming time:61.6011 s\n",
      "[epoch 7616]: training loss: 780.632629, consuming time:62.1681 s\n",
      "[epoch 7617]: training loss: 760.571411, consuming time:61.9177 s\n",
      "[epoch 7618]: training loss: 860.771606, consuming time:62.0589 s\n",
      "[epoch 7619]: training loss: 1297.293457, consuming time:61.8224 s\n",
      "[epoch 7620]: training loss: 1067.897339, consuming time:61.6766 s\n",
      "[epoch 7621]: training loss: 740.653076, consuming time:61.7968 s\n",
      "[epoch 7622]: training loss: 947.459412, consuming time:62.1375 s\n",
      "[epoch 7623]: training loss: 1111.932617, consuming time:62.3262 s\n",
      "[epoch 7624]: training loss: 832.886230, consuming time:61.7762 s\n",
      "[epoch 7625]: training loss: 787.504272, consuming time:61.6170 s\n",
      "[epoch 7626]: training loss: 817.419067, consuming time:61.8598 s\n",
      "[epoch 7627]: training loss: 934.226318, consuming time:61.9705 s\n",
      "[epoch 7628]: training loss: 1331.172852, consuming time:62.3117 s\n",
      "[epoch 7629]: training loss: 869.115662, consuming time:61.9607 s\n",
      "[epoch 7630]: training loss: 1017.703064, consuming time:61.7061 s\n",
      "[epoch 7631]: training loss: 1078.270264, consuming time:61.7985 s\n",
      "[epoch 7632]: training loss: 1054.192383, consuming time:62.0714 s\n",
      "[epoch 7633]: training loss: 1074.074219, consuming time:62.0500 s\n",
      "[epoch 7634]: training loss: 946.907104, consuming time:62.1891 s\n",
      "[epoch 7635]: training loss: 817.135132, consuming time:62.1565 s\n",
      "[epoch 7636]: training loss: 859.867371, consuming time:62.0916 s\n",
      "[epoch 7637]: training loss: 791.515015, consuming time:61.8908 s\n",
      "[epoch 7638]: training loss: 1025.507080, consuming time:62.0693 s\n",
      "[epoch 7639]: training loss: 1181.813599, consuming time:62.0711 s\n",
      "[epoch 7640]: training loss: 910.716187, consuming time:62.1661 s\n",
      "[epoch 7641]: training loss: 863.462585, consuming time:61.7362 s\n",
      "[epoch 7642]: training loss: 913.220581, consuming time:61.6290 s\n",
      "[epoch 7643]: training loss: 982.836548, consuming time:61.7321 s\n",
      "[epoch 7644]: training loss: 895.985596, consuming time:61.9555 s\n",
      "[epoch 7645]: training loss: 887.126221, consuming time:62.1023 s\n",
      "[epoch 7646]: training loss: 971.899780, consuming time:61.9880 s\n",
      "[epoch 7647]: training loss: 926.161621, consuming time:61.8857 s\n",
      "[epoch 7648]: training loss: 990.669312, consuming time:61.6616 s\n",
      "[epoch 7649]: training loss: 939.634705, consuming time:61.9595 s\n",
      "[epoch 7650]: training loss: 875.556885, consuming time:61.8383 s\n",
      "[epoch 7651]: training loss: 999.935913, consuming time:62.0134 s\n",
      "[epoch 7652]: training loss: 617.314209, consuming time:61.8542 s\n",
      "[epoch 7653]: training loss: 884.300293, consuming time:61.5512 s\n",
      "[epoch 7654]: training loss: 1060.900879, consuming time:62.0349 s\n",
      "[epoch 7655]: training loss: 1011.025513, consuming time:61.7321 s\n",
      "[epoch 7656]: training loss: 874.641785, consuming time:61.9531 s\n",
      "[epoch 7657]: training loss: 775.852539, consuming time:62.1299 s\n",
      "[epoch 7658]: training loss: 899.944275, consuming time:61.8007 s\n",
      "[epoch 7659]: training loss: 1106.940063, consuming time:61.7024 s\n",
      "[epoch 7660]: training loss: 1011.792480, consuming time:61.8184 s\n",
      "[epoch 7661]: training loss: 1003.094727, consuming time:62.0632 s\n",
      "[epoch 7662]: training loss: 924.774353, consuming time:62.2633 s\n",
      "[epoch 7663]: training loss: 1126.227173, consuming time:61.9024 s\n",
      "[epoch 7664]: training loss: 762.987122, consuming time:61.6295 s\n",
      "[epoch 7665]: training loss: 766.506104, consuming time:62.0453 s\n",
      "[epoch 7666]: training loss: 1352.869385, consuming time:62.0182 s\n",
      "[epoch 7667]: training loss: 1095.068848, consuming time:62.2175 s\n",
      "[epoch 7668]: training loss: 1004.143005, consuming time:62.1225 s\n",
      "[epoch 7669]: training loss: 877.908203, consuming time:61.9919 s\n",
      "[epoch 7670]: training loss: 969.459106, consuming time:62.1522 s\n",
      "[epoch 7671]: training loss: 832.663879, consuming time:61.8138 s\n",
      "[epoch 7672]: training loss: 849.968140, consuming time:61.8737 s\n",
      "[epoch 7673]: training loss: 926.577454, consuming time:62.4564 s\n",
      "[epoch 7674]: training loss: 1041.416260, consuming time:62.2264 s\n",
      "[epoch 7675]: training loss: 1014.132568, consuming time:61.9370 s\n",
      "[epoch 7676]: training loss: 920.518494, consuming time:61.6031 s\n",
      "[epoch 7677]: training loss: 878.662903, consuming time:61.6680 s\n",
      "[epoch 7678]: training loss: 952.921875, consuming time:62.1496 s\n",
      "[epoch 7679]: training loss: 1046.924072, consuming time:62.1135 s\n",
      "[epoch 7680]: training loss: 854.136108, consuming time:61.7921 s\n",
      "[epoch 7681]: training loss: 1079.926392, consuming time:62.1432 s\n",
      "[epoch 7682]: training loss: 769.592346, consuming time:61.9173 s\n",
      "[epoch 7683]: training loss: 1328.579712, consuming time:61.8312 s\n",
      "[epoch 7684]: training loss: 1126.903564, consuming time:62.2732 s\n",
      "[epoch 7685]: training loss: 952.109375, consuming time:62.0049 s\n",
      "[epoch 7686]: training loss: 753.416870, consuming time:61.8051 s\n",
      "[epoch 7687]: training loss: 1022.377441, consuming time:61.7987 s\n",
      "[epoch 7688]: training loss: 896.413818, consuming time:61.6862 s\n",
      "[epoch 7689]: training loss: 911.907349, consuming time:61.9202 s\n",
      "[epoch 7690]: training loss: 880.900879, consuming time:62.1729 s\n",
      "[epoch 7691]: training loss: 1406.638550, consuming time:61.8537 s\n",
      "[epoch 7692]: training loss: 1118.370361, consuming time:61.5238 s\n",
      "[epoch 7693]: training loss: 1049.193848, consuming time:61.9557 s\n",
      "[epoch 7694]: training loss: 888.965759, consuming time:61.9893 s\n",
      "[epoch 7695]: training loss: 928.929199, consuming time:62.1621 s\n",
      "[epoch 7696]: training loss: 735.410950, consuming time:62.2065 s\n",
      "[epoch 7697]: training loss: 947.779663, consuming time:61.9722 s\n",
      "[epoch 7698]: training loss: 881.924866, consuming time:61.9346 s\n",
      "[epoch 7699]: training loss: 904.537476, consuming time:61.9388 s\n",
      "[epoch 7700]: training loss: 915.064514, consuming time:61.7440 s\n",
      "[epoch 7701]: training loss: 772.100464, consuming time:61.9695 s\n",
      "[epoch 7702]: training loss: 907.029663, consuming time:61.9433 s\n",
      "[epoch 7703]: training loss: 918.742065, consuming time:61.8839 s\n",
      "[epoch 7704]: training loss: 1045.003052, consuming time:62.0856 s\n",
      "[epoch 7705]: training loss: 1001.338623, consuming time:61.7452 s\n",
      "[epoch 7706]: training loss: 1065.879639, consuming time:62.0222 s\n",
      "[epoch 7707]: training loss: 865.694885, consuming time:61.8588 s\n",
      "[epoch 7708]: training loss: 913.815552, consuming time:62.0172 s\n",
      "[epoch 7709]: training loss: 974.307190, consuming time:62.1015 s\n",
      "[epoch 7710]: training loss: 910.331909, consuming time:61.5935 s\n",
      "[epoch 7711]: training loss: 926.512939, consuming time:62.1343 s\n",
      "[epoch 7712]: training loss: 897.170410, consuming time:62.2779 s\n",
      "[epoch 7713]: training loss: 1012.486755, consuming time:62.1708 s\n",
      "[epoch 7714]: training loss: 1025.364258, consuming time:61.8626 s\n",
      "[epoch 7715]: training loss: 882.779297, consuming time:61.7224 s\n",
      "[epoch 7716]: training loss: 912.786987, consuming time:61.8982 s\n",
      "[epoch 7717]: training loss: 823.676270, consuming time:61.8825 s\n",
      "[epoch 7718]: training loss: 915.345276, consuming time:62.0430 s\n",
      "[epoch 7719]: training loss: 743.038696, consuming time:62.0862 s\n",
      "[epoch 7720]: training loss: 1266.331299, consuming time:61.8501 s\n",
      "[epoch 7721]: training loss: 929.613342, consuming time:61.6494 s\n",
      "[epoch 7722]: training loss: 931.142700, consuming time:61.7804 s\n",
      "[epoch 7723]: training loss: 888.572876, consuming time:62.3223 s\n",
      "[epoch 7724]: training loss: 965.716919, consuming time:61.9741 s\n",
      "[epoch 7725]: training loss: 1089.211304, consuming time:62.1091 s\n",
      "[epoch 7726]: training loss: 1268.235229, consuming time:62.0734 s\n",
      "[epoch 7727]: training loss: 1072.726929, consuming time:61.8358 s\n",
      "[epoch 7728]: training loss: 996.042725, consuming time:61.7300 s\n",
      "[epoch 7729]: training loss: 905.475830, consuming time:61.9742 s\n",
      "[epoch 7730]: training loss: 809.479065, consuming time:62.1599 s\n",
      "[epoch 7731]: training loss: 966.853149, consuming time:62.7144 s\n",
      "[epoch 7732]: training loss: 708.236816, consuming time:61.5158 s\n",
      "[epoch 7733]: training loss: 805.258545, consuming time:61.6132 s\n",
      "[epoch 7734]: training loss: 1012.135010, consuming time:61.7414 s\n",
      "[epoch 7735]: training loss: 994.404236, consuming time:61.9747 s\n",
      "[epoch 7736]: training loss: 941.580017, consuming time:62.1916 s\n",
      "[epoch 7737]: training loss: 963.749573, consuming time:61.6645 s\n",
      "[epoch 7738]: training loss: 798.073059, consuming time:61.7175 s\n",
      "[epoch 7739]: training loss: 834.963745, consuming time:62.2005 s\n",
      "[epoch 7740]: training loss: 831.501648, consuming time:62.2957 s\n",
      "[epoch 7741]: training loss: 921.468750, consuming time:62.2906 s\n",
      "[epoch 7742]: training loss: 747.643677, consuming time:62.1929 s\n",
      "[epoch 7743]: training loss: 847.231262, consuming time:61.7271 s\n",
      "[epoch 7744]: training loss: 1002.434692, consuming time:61.9503 s\n",
      "[epoch 7745]: training loss: 922.452759, consuming time:61.9155 s\n",
      "[epoch 7746]: training loss: 873.986694, consuming time:62.0778 s\n",
      "[epoch 7747]: training loss: 981.476685, consuming time:62.0242 s\n",
      "[epoch 7748]: training loss: 747.375610, consuming time:62.0937 s\n",
      "[epoch 7749]: training loss: 963.315430, consuming time:61.6993 s\n",
      "[epoch 7750]: training loss: 803.615662, consuming time:61.7774 s\n",
      "[epoch 7751]: training loss: 711.491333, consuming time:61.9969 s\n",
      "[epoch 7752]: training loss: 1113.994141, consuming time:62.0988 s\n",
      "[epoch 7753]: training loss: 1235.866089, consuming time:62.1788 s\n",
      "[epoch 7754]: training loss: 1102.969482, consuming time:61.7876 s\n",
      "[epoch 7755]: training loss: 769.506714, consuming time:61.7130 s\n",
      "[epoch 7756]: training loss: 959.705322, consuming time:61.7853 s\n",
      "[epoch 7757]: training loss: 1092.106323, consuming time:61.6998 s\n",
      "[epoch 7758]: training loss: 868.942444, consuming time:62.0635 s\n",
      "[epoch 7759]: training loss: 807.589417, consuming time:61.9061 s\n",
      "[epoch 7760]: training loss: 860.353882, consuming time:61.8390 s\n",
      "[epoch 7761]: training loss: 774.978333, consuming time:62.0795 s\n",
      "[epoch 7762]: training loss: 1141.895142, consuming time:61.8246 s\n",
      "[epoch 7763]: training loss: 855.398560, consuming time:62.0538 s\n",
      "[epoch 7764]: training loss: 819.635559, consuming time:62.2428 s\n",
      "[epoch 7765]: training loss: 1026.281494, consuming time:62.0566 s\n",
      "[epoch 7766]: training loss: 814.700073, consuming time:61.8473 s\n",
      "[epoch 7767]: training loss: 808.186890, consuming time:62.0632 s\n",
      "[epoch 7768]: training loss: 827.764771, consuming time:62.1525 s\n",
      "[epoch 7769]: training loss: 819.238037, consuming time:62.5489 s\n",
      "[epoch 7770]: training loss: 969.521240, consuming time:62.2916 s\n",
      "[epoch 7771]: training loss: 1312.840454, consuming time:62.2141 s\n",
      "[epoch 7772]: training loss: 859.471130, consuming time:62.1015 s\n",
      "[epoch 7773]: training loss: 996.107666, consuming time:61.8002 s\n",
      "[epoch 7774]: training loss: 905.135498, consuming time:61.9513 s\n",
      "[epoch 7775]: training loss: 990.878784, consuming time:61.8712 s\n",
      "[epoch 7776]: training loss: 1028.620361, consuming time:62.1861 s\n",
      "[epoch 7777]: training loss: 1003.917114, consuming time:62.1927 s\n",
      "[epoch 7778]: training loss: 1099.439453, consuming time:61.8236 s\n",
      "[epoch 7779]: training loss: 854.885315, consuming time:61.6605 s\n",
      "[epoch 7780]: training loss: 952.221436, consuming time:61.9373 s\n",
      "[epoch 7781]: training loss: 823.848816, consuming time:62.4225 s\n",
      "[epoch 7782]: training loss: 997.169128, consuming time:62.0369 s\n",
      "[epoch 7783]: training loss: 1032.861816, consuming time:61.8621 s\n",
      "[epoch 7784]: training loss: 826.857910, consuming time:61.5531 s\n",
      "[epoch 7785]: training loss: 926.751953, consuming time:61.9857 s\n",
      "[epoch 7786]: training loss: 895.146057, consuming time:62.1049 s\n",
      "[epoch 7787]: training loss: 1003.228760, consuming time:61.9658 s\n",
      "[epoch 7788]: training loss: 741.483643, consuming time:61.9876 s\n",
      "[epoch 7789]: training loss: 965.761597, consuming time:62.0194 s\n",
      "[epoch 7790]: training loss: 989.662842, consuming time:61.8046 s\n",
      "[epoch 7791]: training loss: 807.431763, consuming time:61.8224 s\n",
      "[epoch 7792]: training loss: 1017.798401, consuming time:62.1140 s\n",
      "[epoch 7793]: training loss: 768.383728, consuming time:65.7490 s\n",
      "[epoch 7794]: training loss: 676.416626, consuming time:62.0447 s\n",
      "[epoch 7795]: training loss: 1027.520264, consuming time:62.1548 s\n",
      "[epoch 7796]: training loss: 1024.484497, consuming time:61.7715 s\n",
      "[epoch 7797]: training loss: 990.453491, consuming time:62.5187 s\n",
      "[epoch 7798]: training loss: 1155.762939, consuming time:62.5783 s\n",
      "[epoch 7799]: training loss: 836.915588, consuming time:61.9550 s\n",
      "[epoch 7800]: training loss: 904.507080, consuming time:61.9952 s\n",
      "[epoch 7801]: training loss: 763.033203, consuming time:61.9360 s\n",
      "[epoch 7802]: training loss: 782.118713, consuming time:61.8584 s\n",
      "[epoch 7803]: training loss: 908.383057, consuming time:62.3274 s\n",
      "[epoch 7804]: training loss: 914.450806, consuming time:62.2285 s\n",
      "[epoch 7805]: training loss: 953.759644, consuming time:61.8900 s\n",
      "[epoch 7806]: training loss: 798.921875, consuming time:61.6645 s\n",
      "[epoch 7807]: training loss: 1017.203247, consuming time:61.7977 s\n",
      "[epoch 7808]: training loss: 732.170227, consuming time:62.1615 s\n",
      "[epoch 7809]: training loss: 1156.889404, consuming time:62.0023 s\n",
      "[epoch 7810]: training loss: 740.625610, consuming time:62.2494 s\n",
      "[epoch 7811]: training loss: 1297.196655, consuming time:61.9030 s\n",
      "[epoch 7812]: training loss: 958.058350, consuming time:62.0955 s\n",
      "[epoch 7813]: training loss: 688.583984, consuming time:61.9184 s\n",
      "[epoch 7814]: training loss: 1165.986328, consuming time:61.8076 s\n",
      "[epoch 7815]: training loss: 1130.327148, consuming time:61.9463 s\n",
      "[epoch 7816]: training loss: 837.801514, consuming time:61.9288 s\n",
      "[epoch 7817]: training loss: 974.796631, consuming time:62.0037 s\n",
      "[epoch 7818]: training loss: 814.220215, consuming time:61.9804 s\n",
      "[epoch 7819]: training loss: 961.194092, consuming time:61.9968 s\n",
      "[epoch 7820]: training loss: 915.197876, consuming time:61.9227 s\n",
      "[epoch 7821]: training loss: 829.195007, consuming time:62.1496 s\n",
      "[epoch 7822]: training loss: 1089.575562, consuming time:61.7891 s\n",
      "[epoch 7823]: training loss: 1120.515381, consuming time:62.0121 s\n",
      "[epoch 7824]: training loss: 819.840637, consuming time:61.7214 s\n",
      "[epoch 7825]: training loss: 1096.286865, consuming time:62.0884 s\n",
      "[epoch 7826]: training loss: 978.743591, consuming time:62.1946 s\n",
      "[epoch 7827]: training loss: 1283.501953, consuming time:62.2088 s\n",
      "[epoch 7828]: training loss: 1225.736328, consuming time:61.8304 s\n",
      "[epoch 7829]: training loss: 1027.339844, consuming time:61.7445 s\n",
      "[epoch 7830]: training loss: 588.917114, consuming time:61.9967 s\n",
      "[epoch 7831]: training loss: 718.710083, consuming time:62.2876 s\n",
      "[epoch 7832]: training loss: 1090.424316, consuming time:62.1774 s\n",
      "[epoch 7833]: training loss: 975.759155, consuming time:61.8289 s\n",
      "[epoch 7834]: training loss: 956.873901, consuming time:61.9737 s\n",
      "[epoch 7835]: training loss: 938.923218, consuming time:62.0211 s\n",
      "[epoch 7836]: training loss: 837.114502, consuming time:62.0949 s\n",
      "[epoch 7837]: training loss: 829.050293, consuming time:62.1774 s\n",
      "[epoch 7838]: training loss: 1023.115112, consuming time:62.1358 s\n",
      "[epoch 7839]: training loss: 1015.221680, consuming time:61.6957 s\n",
      "[epoch 7840]: training loss: 832.020630, consuming time:61.6813 s\n",
      "[epoch 7841]: training loss: 1119.704468, consuming time:62.1321 s\n",
      "[epoch 7842]: training loss: 1195.122803, consuming time:62.0115 s\n",
      "[epoch 7843]: training loss: 1033.508667, consuming time:62.1528 s\n",
      "[epoch 7844]: training loss: 694.291504, consuming time:61.9093 s\n",
      "[epoch 7845]: training loss: 1012.520630, consuming time:61.7277 s\n",
      "[epoch 7846]: training loss: 928.792969, consuming time:61.5747 s\n",
      "[epoch 7847]: training loss: 846.251343, consuming time:61.9257 s\n",
      "[epoch 7848]: training loss: 906.357300, consuming time:62.2383 s\n",
      "[epoch 7849]: training loss: 1115.869995, consuming time:62.0740 s\n",
      "[epoch 7850]: training loss: 768.518799, consuming time:61.9289 s\n",
      "[epoch 7851]: training loss: 639.437378, consuming time:62.0058 s\n",
      "[epoch 7852]: training loss: 1063.739990, consuming time:61.9327 s\n",
      "[epoch 7853]: training loss: 1006.153137, consuming time:62.0852 s\n",
      "[epoch 7854]: training loss: 907.173889, consuming time:62.3578 s\n",
      "[epoch 7855]: training loss: 841.407715, consuming time:62.3989 s\n",
      "[epoch 7856]: training loss: 1093.742065, consuming time:62.0360 s\n",
      "[epoch 7857]: training loss: 866.221985, consuming time:62.0469 s\n",
      "[epoch 7858]: training loss: 784.194458, consuming time:61.8061 s\n",
      "[epoch 7859]: training loss: 743.268250, consuming time:62.3908 s\n",
      "[epoch 7860]: training loss: 808.850586, consuming time:61.8121 s\n",
      "[epoch 7861]: training loss: 834.858093, consuming time:62.4054 s\n",
      "[epoch 7862]: training loss: 700.419922, consuming time:61.7803 s\n",
      "[epoch 7863]: training loss: 1292.732422, consuming time:61.4715 s\n",
      "[epoch 7864]: training loss: 892.664917, consuming time:62.0024 s\n",
      "[epoch 7865]: training loss: 985.588196, consuming time:62.0734 s\n",
      "[epoch 7866]: training loss: 760.457764, consuming time:62.2811 s\n",
      "[epoch 7867]: training loss: 769.582153, consuming time:61.8216 s\n",
      "[epoch 7868]: training loss: 782.726929, consuming time:61.8506 s\n",
      "[epoch 7869]: training loss: 839.580811, consuming time:62.0753 s\n",
      "[epoch 7870]: training loss: 904.322021, consuming time:62.1278 s\n",
      "[epoch 7871]: training loss: 888.917725, consuming time:62.0385 s\n",
      "[epoch 7872]: training loss: 994.730835, consuming time:62.3064 s\n",
      "[epoch 7873]: training loss: 766.870483, consuming time:61.6733 s\n",
      "[epoch 7874]: training loss: 600.654053, consuming time:62.0129 s\n",
      "[epoch 7875]: training loss: 806.761536, consuming time:61.8411 s\n",
      "[epoch 7876]: training loss: 788.788330, consuming time:62.0117 s\n",
      "[epoch 7877]: training loss: 838.922485, consuming time:62.1542 s\n",
      "[epoch 7878]: training loss: 889.403198, consuming time:62.2782 s\n",
      "[epoch 7879]: training loss: 804.642334, consuming time:61.5916 s\n",
      "[epoch 7880]: training loss: 991.754028, consuming time:61.9528 s\n",
      "[epoch 7881]: training loss: 867.600464, consuming time:62.8090 s\n",
      "[epoch 7882]: training loss: 891.812439, consuming time:62.0891 s\n",
      "[epoch 7883]: training loss: 1119.736938, consuming time:62.0650 s\n",
      "[epoch 7884]: training loss: 962.749390, consuming time:62.2841 s\n",
      "[epoch 7885]: training loss: 898.034546, consuming time:61.6094 s\n",
      "[epoch 7886]: training loss: 938.492126, consuming time:62.0991 s\n",
      "[epoch 7887]: training loss: 917.856201, consuming time:62.1719 s\n",
      "[epoch 7888]: training loss: 1063.019897, consuming time:62.1272 s\n",
      "[epoch 7889]: training loss: 1037.825928, consuming time:62.0129 s\n",
      "[epoch 7890]: training loss: 760.596313, consuming time:62.1518 s\n",
      "[epoch 7891]: training loss: 946.922607, consuming time:61.9673 s\n",
      "[epoch 7892]: training loss: 999.439331, consuming time:62.0072 s\n",
      "[epoch 7893]: training loss: 797.470398, consuming time:62.3522 s\n",
      "[epoch 7894]: training loss: 900.628174, consuming time:62.6470 s\n",
      "[epoch 7895]: training loss: 1001.218628, consuming time:62.1271 s\n",
      "[epoch 7896]: training loss: 920.790405, consuming time:61.8256 s\n",
      "[epoch 7897]: training loss: 827.560547, consuming time:62.0873 s\n",
      "[epoch 7898]: training loss: 786.002502, consuming time:61.7738 s\n",
      "[epoch 7899]: training loss: 982.828613, consuming time:62.3495 s\n",
      "[epoch 7900]: training loss: 750.993958, consuming time:62.4705 s\n",
      "[epoch 7901]: training loss: 775.537415, consuming time:61.9792 s\n",
      "[epoch 7902]: training loss: 771.172119, consuming time:62.1631 s\n",
      "[epoch 7903]: training loss: 790.481995, consuming time:61.8235 s\n",
      "[epoch 7904]: training loss: 831.424438, consuming time:61.9636 s\n",
      "[epoch 7905]: training loss: 1094.357056, consuming time:62.2214 s\n",
      "[epoch 7906]: training loss: 858.436523, consuming time:62.1161 s\n",
      "[epoch 7907]: training loss: 1183.928101, consuming time:62.1889 s\n",
      "[epoch 7908]: training loss: 812.754761, consuming time:61.8793 s\n",
      "[epoch 7909]: training loss: 1047.041260, consuming time:62.0443 s\n",
      "[epoch 7910]: training loss: 1001.884766, consuming time:62.2723 s\n",
      "[epoch 7911]: training loss: 1038.409180, consuming time:62.2687 s\n",
      "[epoch 7912]: training loss: 1052.656372, consuming time:62.0762 s\n",
      "[epoch 7913]: training loss: 1015.486145, consuming time:62.2096 s\n",
      "[epoch 7914]: training loss: 806.808777, consuming time:61.7144 s\n",
      "[epoch 7915]: training loss: 931.580444, consuming time:62.2419 s\n",
      "[epoch 7916]: training loss: 836.403625, consuming time:62.4570 s\n",
      "[epoch 7917]: training loss: 848.158447, consuming time:61.9812 s\n",
      "[epoch 7918]: training loss: 1116.413330, consuming time:62.1286 s\n",
      "[epoch 7919]: training loss: 1219.191650, consuming time:61.9022 s\n",
      "[epoch 7920]: training loss: 1111.643066, consuming time:61.8430 s\n",
      "[epoch 7921]: training loss: 971.917358, consuming time:62.1480 s\n",
      "[epoch 7922]: training loss: 939.829773, consuming time:62.3513 s\n",
      "[epoch 7923]: training loss: 1089.653320, consuming time:62.0253 s\n",
      "[epoch 7924]: training loss: 1074.763550, consuming time:62.0782 s\n",
      "[epoch 7925]: training loss: 943.736694, consuming time:62.0666 s\n",
      "[epoch 7926]: training loss: 869.767883, consuming time:61.6918 s\n",
      "[epoch 7927]: training loss: 836.386963, consuming time:62.1449 s\n",
      "[epoch 7928]: training loss: 958.668701, consuming time:62.2905 s\n",
      "[epoch 7929]: training loss: 1070.755615, consuming time:62.1172 s\n",
      "[epoch 7930]: training loss: 860.914429, consuming time:62.1898 s\n",
      "[epoch 7931]: training loss: 1309.773315, consuming time:61.8492 s\n",
      "[epoch 7932]: training loss: 866.239868, consuming time:62.0178 s\n",
      "[epoch 7933]: training loss: 1273.156250, consuming time:62.2967 s\n",
      "[epoch 7934]: training loss: 832.929565, consuming time:62.3674 s\n",
      "[epoch 7935]: training loss: 995.165894, consuming time:62.4300 s\n",
      "[epoch 7936]: training loss: 808.555969, consuming time:62.1537 s\n",
      "[epoch 7937]: training loss: 586.017395, consuming time:61.8467 s\n",
      "[epoch 7938]: training loss: 906.728333, consuming time:62.2188 s\n",
      "[epoch 7939]: training loss: 979.236389, consuming time:62.2322 s\n",
      "[epoch 7940]: training loss: 727.903198, consuming time:62.0117 s\n",
      "[epoch 7941]: training loss: 1083.246582, consuming time:62.0584 s\n",
      "[epoch 7942]: training loss: 735.071655, consuming time:61.9957 s\n",
      "[epoch 7943]: training loss: 1389.893555, consuming time:61.9344 s\n",
      "[epoch 7944]: training loss: 931.660156, consuming time:62.3290 s\n",
      "[epoch 7945]: training loss: 877.051331, consuming time:62.4714 s\n",
      "[epoch 7946]: training loss: 586.842041, consuming time:62.2191 s\n",
      "[epoch 7947]: training loss: 798.886353, consuming time:62.0042 s\n",
      "[epoch 7948]: training loss: 856.643555, consuming time:62.1325 s\n",
      "[epoch 7949]: training loss: 1181.059814, consuming time:61.8309 s\n",
      "[epoch 7950]: training loss: 884.199036, consuming time:62.3342 s\n",
      "[epoch 7951]: training loss: 915.237915, consuming time:62.4309 s\n",
      "[epoch 7952]: training loss: 896.438599, consuming time:62.3104 s\n",
      "[epoch 7953]: training loss: 830.278320, consuming time:61.8233 s\n",
      "[epoch 7954]: training loss: 980.677429, consuming time:61.9756 s\n",
      "[epoch 7955]: training loss: 1141.531128, consuming time:62.1487 s\n",
      "[epoch 7956]: training loss: 1037.528076, consuming time:62.3662 s\n",
      "[epoch 7957]: training loss: 1005.891235, consuming time:62.2777 s\n",
      "[epoch 7958]: training loss: 881.016602, consuming time:61.7939 s\n",
      "[epoch 7959]: training loss: 1125.719849, consuming time:62.0583 s\n",
      "[epoch 7960]: training loss: 891.118164, consuming time:62.0729 s\n",
      "[epoch 7961]: training loss: 1080.307983, consuming time:62.4003 s\n",
      "[epoch 7962]: training loss: 1255.406982, consuming time:61.9504 s\n",
      "[epoch 7963]: training loss: 1294.114502, consuming time:62.2188 s\n",
      "[epoch 7964]: training loss: 964.780273, consuming time:61.8458 s\n",
      "[epoch 7965]: training loss: 885.145264, consuming time:61.7018 s\n",
      "[epoch 7966]: training loss: 1068.071289, consuming time:62.1471 s\n",
      "[epoch 7967]: training loss: 1214.379639, consuming time:62.0972 s\n",
      "[epoch 7968]: training loss: 1017.280090, consuming time:62.2369 s\n",
      "[epoch 7969]: training loss: 965.678284, consuming time:62.2084 s\n",
      "[epoch 7970]: training loss: 1020.298767, consuming time:62.0115 s\n",
      "[epoch 7971]: training loss: 998.376587, consuming time:62.3771 s\n",
      "[epoch 7972]: training loss: 1093.114014, consuming time:61.9104 s\n",
      "[epoch 7973]: training loss: 1165.309326, consuming time:62.2343 s\n",
      "[epoch 7974]: training loss: 931.785522, consuming time:62.1578 s\n",
      "[epoch 7975]: training loss: 663.855835, consuming time:61.8973 s\n",
      "[epoch 7976]: training loss: 1002.701050, consuming time:62.0907 s\n",
      "[epoch 7977]: training loss: 783.613647, consuming time:61.9866 s\n",
      "[epoch 7978]: training loss: 929.794312, consuming time:62.4378 s\n",
      "[epoch 7979]: training loss: 1006.786133, consuming time:62.1955 s\n",
      "[epoch 7980]: training loss: 908.723022, consuming time:62.1827 s\n",
      "[epoch 7981]: training loss: 889.451416, consuming time:62.2034 s\n",
      "[epoch 7982]: training loss: 909.298828, consuming time:62.0621 s\n",
      "[epoch 7983]: training loss: 962.127869, consuming time:61.9921 s\n",
      "[epoch 7984]: training loss: 718.182434, consuming time:62.4140 s\n",
      "[epoch 7985]: training loss: 1129.785400, consuming time:62.1279 s\n",
      "[epoch 7986]: training loss: 1144.792603, consuming time:62.0981 s\n",
      "[epoch 7987]: training loss: 788.562134, consuming time:62.0048 s\n",
      "[epoch 7988]: training loss: 965.437500, consuming time:61.9979 s\n",
      "[epoch 7989]: training loss: 978.071960, consuming time:61.9726 s\n",
      "[epoch 7990]: training loss: 1028.509033, consuming time:62.1101 s\n",
      "[epoch 7991]: training loss: 1090.240479, consuming time:62.4886 s\n",
      "[epoch 7992]: training loss: 800.611755, consuming time:61.9091 s\n",
      "[epoch 7993]: training loss: 884.234619, consuming time:61.4260 s\n",
      "[epoch 7994]: training loss: 706.124878, consuming time:60.4298 s\n",
      "[epoch 7995]: training loss: 981.488953, consuming time:60.4075 s\n",
      "[epoch 7996]: training loss: 987.964661, consuming time:64.7196 s\n",
      "[epoch 7997]: training loss: 858.491272, consuming time:70.2837 s\n",
      "[epoch 7998]: training loss: 823.154053, consuming time:67.1793 s\n",
      "[epoch 7999]: training loss: 961.133789, consuming time:67.8875 s\n",
      "[epoch 8000]: training loss: 858.566467, consuming time:66.2089 s\n",
      "[epoch 8001]: training loss: 1162.159302, consuming time:67.4982 s\n",
      "[epoch 8002]: training loss: 1075.536133, consuming time:67.0878 s\n",
      "[epoch 8003]: training loss: 954.870300, consuming time:66.1054 s\n",
      "[epoch 8004]: training loss: 1129.550293, consuming time:61.3183 s\n",
      "[epoch 8005]: training loss: 1017.837341, consuming time:60.9280 s\n",
      "[epoch 8006]: training loss: 961.051147, consuming time:61.1409 s\n",
      "[epoch 8007]: training loss: 765.068604, consuming time:61.3082 s\n",
      "[epoch 8008]: training loss: 876.722778, consuming time:61.1846 s\n",
      "[epoch 8009]: training loss: 749.133972, consuming time:61.6170 s\n",
      "[epoch 8010]: training loss: 955.144287, consuming time:60.8925 s\n",
      "[epoch 8011]: training loss: 938.309570, consuming time:61.1328 s\n",
      "[epoch 8012]: training loss: 660.188232, consuming time:61.5591 s\n",
      "[epoch 8013]: training loss: 1083.799072, consuming time:61.1737 s\n",
      "[epoch 8014]: training loss: 930.091919, consuming time:61.1219 s\n",
      "[epoch 8015]: training loss: 1146.979370, consuming time:60.5674 s\n",
      "[epoch 8016]: training loss: 984.084595, consuming time:61.0057 s\n",
      "[epoch 8017]: training loss: 864.829102, consuming time:61.0660 s\n",
      "[epoch 8018]: training loss: 835.146179, consuming time:61.6157 s\n",
      "[epoch 8019]: training loss: 1004.690063, consuming time:61.4500 s\n",
      "[epoch 8020]: training loss: 928.477539, consuming time:60.9775 s\n",
      "[epoch 8021]: training loss: 940.804260, consuming time:61.7725 s\n",
      "[epoch 8022]: training loss: 876.325439, consuming time:61.7851 s\n",
      "[epoch 8023]: training loss: 899.181335, consuming time:61.1678 s\n",
      "[epoch 8024]: training loss: 1281.698242, consuming time:63.4930 s\n",
      "[epoch 8025]: training loss: 652.816345, consuming time:60.9963 s\n",
      "[epoch 8026]: training loss: 1046.713135, consuming time:64.2245 s\n",
      "[epoch 8027]: training loss: 896.442749, consuming time:63.4837 s\n",
      "[epoch 8028]: training loss: 936.907959, consuming time:63.8030 s\n",
      "[epoch 8029]: training loss: 798.329468, consuming time:63.2504 s\n",
      "[epoch 8030]: training loss: 1089.175049, consuming time:60.6217 s\n",
      "[epoch 8031]: training loss: 1003.848267, consuming time:61.1442 s\n",
      "[epoch 8032]: training loss: 814.261353, consuming time:64.3579 s\n",
      "[epoch 8033]: training loss: 925.411926, consuming time:62.3630 s\n",
      "[epoch 8034]: training loss: 945.274780, consuming time:65.8321 s\n",
      "[epoch 8035]: training loss: 1132.946167, consuming time:61.4962 s\n",
      "[epoch 8036]: training loss: 973.141846, consuming time:60.5707 s\n",
      "[epoch 8037]: training loss: 858.373535, consuming time:60.6669 s\n",
      "[epoch 8038]: training loss: 817.867676, consuming time:60.9604 s\n",
      "[epoch 8039]: training loss: 910.459717, consuming time:60.7891 s\n",
      "[epoch 8040]: training loss: 957.740173, consuming time:61.2929 s\n",
      "[epoch 8041]: training loss: 896.269775, consuming time:60.2883 s\n",
      "[epoch 8042]: training loss: 884.167236, consuming time:60.5865 s\n",
      "[epoch 8043]: training loss: 878.537964, consuming time:61.3867 s\n",
      "[epoch 8044]: training loss: 1178.826050, consuming time:61.8270 s\n",
      "[epoch 8045]: training loss: 834.657349, consuming time:64.9929 s\n",
      "[epoch 8046]: training loss: 727.615784, consuming time:60.8844 s\n",
      "[epoch 8047]: training loss: 872.924500, consuming time:61.0240 s\n",
      "[epoch 8048]: training loss: 972.741394, consuming time:60.9258 s\n",
      "[epoch 8049]: training loss: 713.959656, consuming time:61.0681 s\n",
      "[epoch 8050]: training loss: 758.755127, consuming time:61.1084 s\n",
      "[epoch 8051]: training loss: 1018.656921, consuming time:60.8207 s\n",
      "[epoch 8052]: training loss: 722.644409, consuming time:61.3977 s\n",
      "[epoch 8053]: training loss: 998.602966, consuming time:60.8573 s\n",
      "[epoch 8054]: training loss: 894.824463, consuming time:60.9593 s\n",
      "[epoch 8055]: training loss: 857.877686, consuming time:61.1430 s\n",
      "[epoch 8056]: training loss: 697.254822, consuming time:61.8321 s\n",
      "[epoch 8057]: training loss: 935.580566, consuming time:60.3756 s\n",
      "[epoch 8058]: training loss: 862.903503, consuming time:60.9410 s\n",
      "[epoch 8059]: training loss: 975.398315, consuming time:60.6171 s\n",
      "[epoch 8060]: training loss: 871.408203, consuming time:60.6802 s\n",
      "[epoch 8061]: training loss: 904.510254, consuming time:61.6921 s\n",
      "[epoch 8062]: training loss: 1087.482910, consuming time:61.4009 s\n",
      "[epoch 8063]: training loss: 824.420227, consuming time:61.5648 s\n",
      "[epoch 8064]: training loss: 904.084106, consuming time:61.0585 s\n",
      "[epoch 8065]: training loss: 1172.841064, consuming time:61.0385 s\n",
      "[epoch 8066]: training loss: 850.384705, consuming time:61.4808 s\n",
      "[epoch 8067]: training loss: 1131.105469, consuming time:61.1896 s\n",
      "[epoch 8068]: training loss: 897.808105, consuming time:61.1817 s\n",
      "[epoch 8069]: training loss: 1083.174438, consuming time:61.2184 s\n",
      "[epoch 8070]: training loss: 1088.717773, consuming time:61.7583 s\n",
      "[epoch 8071]: training loss: 838.887939, consuming time:61.3264 s\n",
      "[epoch 8072]: training loss: 783.430664, consuming time:60.9807 s\n",
      "[epoch 8073]: training loss: 1224.778076, consuming time:61.0294 s\n",
      "[epoch 8074]: training loss: 986.847778, consuming time:60.7707 s\n",
      "[epoch 8075]: training loss: 800.314026, consuming time:61.2971 s\n",
      "[epoch 8076]: training loss: 935.931030, consuming time:60.8978 s\n",
      "[epoch 8077]: training loss: 1101.757202, consuming time:60.7073 s\n",
      "[epoch 8078]: training loss: 914.196960, consuming time:60.5438 s\n",
      "[epoch 8079]: training loss: 800.100708, consuming time:60.4463 s\n",
      "[epoch 8080]: training loss: 825.767090, consuming time:60.7985 s\n",
      "[epoch 8081]: training loss: 1054.120972, consuming time:60.2730 s\n",
      "[epoch 8082]: training loss: 741.060242, consuming time:60.4294 s\n",
      "[epoch 8083]: training loss: 917.140747, consuming time:60.1989 s\n",
      "[epoch 8084]: training loss: 1202.993652, consuming time:60.3999 s\n",
      "[epoch 8085]: training loss: 1051.378296, consuming time:60.7152 s\n",
      "[epoch 8086]: training loss: 888.251465, consuming time:60.7636 s\n",
      "[epoch 8087]: training loss: 954.399353, consuming time:60.2108 s\n",
      "[epoch 8088]: training loss: 842.378174, consuming time:60.8246 s\n",
      "[epoch 8089]: training loss: 913.867615, consuming time:60.9694 s\n",
      "[epoch 8090]: training loss: 1076.665039, consuming time:60.6524 s\n",
      "[epoch 8091]: training loss: 1169.017090, consuming time:60.6025 s\n",
      "[epoch 8092]: training loss: 1013.167664, consuming time:60.9320 s\n",
      "[epoch 8093]: training loss: 907.359985, consuming time:60.5502 s\n",
      "[epoch 8094]: training loss: 726.926025, consuming time:60.5758 s\n",
      "[epoch 8095]: training loss: 1118.810303, consuming time:60.6527 s\n",
      "[epoch 8096]: training loss: 961.036499, consuming time:60.4003 s\n",
      "[epoch 8097]: training loss: 929.546082, consuming time:60.7331 s\n",
      "[epoch 8098]: training loss: 1061.201416, consuming time:60.6028 s\n",
      "[epoch 8099]: training loss: 1097.533813, consuming time:60.5145 s\n",
      "[epoch 8100]: training loss: 907.396301, consuming time:60.1604 s\n",
      "[epoch 8101]: training loss: 1029.777710, consuming time:60.4960 s\n",
      "[epoch 8102]: training loss: 749.705933, consuming time:60.4123 s\n",
      "[epoch 8103]: training loss: 813.996460, consuming time:61.0879 s\n",
      "[epoch 8104]: training loss: 1064.443115, consuming time:60.7283 s\n",
      "[epoch 8105]: training loss: 1091.170654, consuming time:60.3973 s\n",
      "[epoch 8106]: training loss: 846.019165, consuming time:60.3870 s\n",
      "[epoch 8107]: training loss: 1000.343262, consuming time:60.5908 s\n",
      "[epoch 8108]: training loss: 937.868164, consuming time:60.6650 s\n",
      "[epoch 8109]: training loss: 924.266846, consuming time:60.2728 s\n",
      "[epoch 8110]: training loss: 900.228638, consuming time:60.6112 s\n",
      "[epoch 8111]: training loss: 868.792114, consuming time:60.1525 s\n",
      "[epoch 8112]: training loss: 741.205505, consuming time:60.5728 s\n",
      "[epoch 8113]: training loss: 974.501282, consuming time:65.7635 s\n",
      "[epoch 8114]: training loss: 1092.439453, consuming time:63.1546 s\n",
      "[epoch 8115]: training loss: 743.344360, consuming time:62.4661 s\n",
      "[epoch 8116]: training loss: 1086.417358, consuming time:63.8610 s\n",
      "[epoch 8117]: training loss: 790.111694, consuming time:63.4004 s\n",
      "[epoch 8118]: training loss: 1025.918579, consuming time:62.3887 s\n",
      "[epoch 8119]: training loss: 735.002930, consuming time:62.1808 s\n",
      "[epoch 8120]: training loss: 858.340149, consuming time:64.6786 s\n",
      "[epoch 8121]: training loss: 1103.692871, consuming time:60.9053 s\n",
      "[epoch 8122]: training loss: 797.658569, consuming time:60.8897 s\n",
      "[epoch 8123]: training loss: 843.629456, consuming time:60.5966 s\n",
      "[epoch 8124]: training loss: 827.315186, consuming time:61.0435 s\n",
      "[epoch 8125]: training loss: 912.189453, consuming time:64.0950 s\n",
      "[epoch 8126]: training loss: 967.567566, consuming time:62.9717 s\n",
      "[epoch 8127]: training loss: 915.919678, consuming time:61.5028 s\n",
      "[epoch 8128]: training loss: 852.036865, consuming time:60.5761 s\n",
      "[epoch 8129]: training loss: 990.099915, consuming time:60.5389 s\n",
      "[epoch 8130]: training loss: 872.091858, consuming time:61.0315 s\n",
      "[epoch 8131]: training loss: 1135.642578, consuming time:61.5090 s\n",
      "[epoch 8132]: training loss: 801.941650, consuming time:61.3700 s\n",
      "[epoch 8133]: training loss: 1176.824219, consuming time:61.0179 s\n",
      "[epoch 8134]: training loss: 848.794983, consuming time:68.6573 s\n",
      "[epoch 8135]: training loss: 720.577759, consuming time:62.1894 s\n",
      "[epoch 8136]: training loss: 807.387573, consuming time:65.0543 s\n",
      "[epoch 8137]: training loss: 686.157104, consuming time:62.1301 s\n",
      "[epoch 8138]: training loss: 727.432495, consuming time:66.8898 s\n",
      "[epoch 8139]: training loss: 712.848755, consuming time:63.2581 s\n",
      "[epoch 8140]: training loss: 896.235107, consuming time:60.3446 s\n",
      "[epoch 8141]: training loss: 991.454346, consuming time:59.5625 s\n",
      "[epoch 8142]: training loss: 1156.247559, consuming time:60.6356 s\n",
      "[epoch 8143]: training loss: 791.593750, consuming time:60.2110 s\n",
      "[epoch 8144]: training loss: 887.096558, consuming time:60.8491 s\n",
      "[epoch 8145]: training loss: 1006.981445, consuming time:60.3280 s\n",
      "[epoch 8146]: training loss: 1080.133545, consuming time:59.9446 s\n",
      "[epoch 8147]: training loss: 997.563232, consuming time:59.9182 s\n",
      "[epoch 8148]: training loss: 1032.688721, consuming time:60.1831 s\n",
      "[epoch 8149]: training loss: 731.954041, consuming time:60.9272 s\n",
      "[epoch 8150]: training loss: 800.597229, consuming time:60.4068 s\n",
      "[epoch 8151]: training loss: 939.665649, consuming time:60.1493 s\n",
      "[epoch 8152]: training loss: 987.669678, consuming time:60.3402 s\n",
      "[epoch 8153]: training loss: 860.689087, consuming time:60.7566 s\n",
      "[epoch 8154]: training loss: 1279.808716, consuming time:60.0533 s\n",
      "[epoch 8155]: training loss: 972.195496, consuming time:60.1126 s\n",
      "[epoch 8156]: training loss: 1143.912964, consuming time:60.2611 s\n",
      "[epoch 8157]: training loss: 925.737793, consuming time:59.8746 s\n",
      "[epoch 8158]: training loss: 656.295166, consuming time:60.4151 s\n",
      "[epoch 8159]: training loss: 1130.229248, consuming time:59.8954 s\n",
      "[epoch 8160]: training loss: 821.261475, consuming time:60.6915 s\n",
      "[epoch 8161]: training loss: 1216.812256, consuming time:60.8607 s\n",
      "[epoch 8162]: training loss: 941.466919, consuming time:61.2912 s\n",
      "[epoch 8163]: training loss: 859.716492, consuming time:61.8782 s\n",
      "[epoch 8164]: training loss: 1010.283020, consuming time:61.6373 s\n",
      "[epoch 8165]: training loss: 1086.092529, consuming time:61.1532 s\n",
      "[epoch 8166]: training loss: 1044.466064, consuming time:61.2210 s\n",
      "[epoch 8167]: training loss: 778.564941, consuming time:61.4731 s\n",
      "[epoch 8168]: training loss: 1021.476929, consuming time:62.2291 s\n",
      "[epoch 8169]: training loss: 600.162720, consuming time:61.4515 s\n",
      "[epoch 8170]: training loss: 1038.652832, consuming time:63.6951 s\n",
      "[epoch 8171]: training loss: 848.009521, consuming time:63.6630 s\n",
      "[epoch 8172]: training loss: 943.359619, consuming time:64.2644 s\n",
      "[epoch 8173]: training loss: 969.127808, consuming time:63.5645 s\n",
      "[epoch 8174]: training loss: 770.680054, consuming time:60.9374 s\n",
      "[epoch 8175]: training loss: 973.667114, consuming time:61.3430 s\n",
      "[epoch 8176]: training loss: 999.828247, consuming time:61.1429 s\n",
      "[epoch 8177]: training loss: 913.500061, consuming time:61.1293 s\n",
      "[epoch 8178]: training loss: 875.655273, consuming time:60.6205 s\n",
      "[epoch 8179]: training loss: 988.763672, consuming time:61.6390 s\n",
      "[epoch 8180]: training loss: 974.296387, consuming time:62.7904 s\n",
      "[epoch 8181]: training loss: 857.494690, consuming time:61.3316 s\n",
      "[epoch 8182]: training loss: 931.343323, consuming time:60.8513 s\n",
      "[epoch 8183]: training loss: 1080.617432, consuming time:61.8795 s\n",
      "[epoch 8184]: training loss: 890.832275, consuming time:60.4767 s\n",
      "[epoch 8185]: training loss: 939.981812, consuming time:60.6237 s\n",
      "[epoch 8186]: training loss: 865.079224, consuming time:60.6056 s\n",
      "[epoch 8187]: training loss: 652.812317, consuming time:60.5428 s\n",
      "[epoch 8188]: training loss: 936.770386, consuming time:60.0934 s\n",
      "[epoch 8189]: training loss: 859.151733, consuming time:60.0512 s\n",
      "[epoch 8190]: training loss: 1031.137207, consuming time:64.0827 s\n",
      "[epoch 8191]: training loss: 941.103271, consuming time:60.7434 s\n",
      "[epoch 8192]: training loss: 991.341248, consuming time:60.4726 s\n",
      "[epoch 8193]: training loss: 866.108826, consuming time:61.0299 s\n",
      "[epoch 8194]: training loss: 930.767761, consuming time:60.1021 s\n",
      "[epoch 8195]: training loss: 1043.064453, consuming time:60.7127 s\n",
      "[epoch 8196]: training loss: 939.567261, consuming time:60.4946 s\n",
      "[epoch 8197]: training loss: 796.846680, consuming time:60.6246 s\n",
      "[epoch 8198]: training loss: 832.305847, consuming time:59.9767 s\n",
      "[epoch 8199]: training loss: 1282.838501, consuming time:60.1895 s\n",
      "[epoch 8200]: training loss: 1031.341064, consuming time:60.5976 s\n",
      "[epoch 8201]: training loss: 942.678345, consuming time:64.1577 s\n",
      "[epoch 8202]: training loss: 1070.561768, consuming time:63.3624 s\n",
      "[epoch 8203]: training loss: 1048.074951, consuming time:62.3191 s\n",
      "[epoch 8204]: training loss: 813.370178, consuming time:62.3018 s\n",
      "[epoch 8205]: training loss: 896.240234, consuming time:62.8923 s\n",
      "[epoch 8206]: training loss: 814.842285, consuming time:61.4593 s\n",
      "[epoch 8207]: training loss: 1068.250366, consuming time:61.1169 s\n",
      "[epoch 8208]: training loss: 967.664917, consuming time:61.2757 s\n",
      "[epoch 8209]: training loss: 832.595581, consuming time:61.3794 s\n",
      "[epoch 8210]: training loss: 1019.238770, consuming time:61.7163 s\n",
      "[epoch 8211]: training loss: 1380.790649, consuming time:61.5730 s\n",
      "[epoch 8212]: training loss: 833.529968, consuming time:61.1507 s\n",
      "[epoch 8213]: training loss: 844.655518, consuming time:61.0169 s\n",
      "[epoch 8214]: training loss: 1312.749512, consuming time:61.7461 s\n",
      "[epoch 8215]: training loss: 703.959656, consuming time:65.1653 s\n",
      "[epoch 8216]: training loss: 1032.014160, consuming time:61.7202 s\n",
      "[epoch 8217]: training loss: 905.284668, consuming time:61.5675 s\n",
      "[epoch 8218]: training loss: 740.248291, consuming time:62.1115 s\n",
      "[epoch 8219]: training loss: 1168.490112, consuming time:61.6578 s\n",
      "[epoch 8220]: training loss: 1248.294189, consuming time:62.3815 s\n",
      "[epoch 8221]: training loss: 805.649048, consuming time:61.3248 s\n",
      "[epoch 8222]: training loss: 1018.853455, consuming time:60.6052 s\n",
      "[epoch 8223]: training loss: 1070.970947, consuming time:60.5307 s\n",
      "[epoch 8224]: training loss: 1016.869385, consuming time:60.4832 s\n",
      "[epoch 8225]: training loss: 960.522583, consuming time:60.8162 s\n",
      "[epoch 8226]: training loss: 814.218994, consuming time:62.9461 s\n",
      "[epoch 8227]: training loss: 1041.710938, consuming time:65.8503 s\n",
      "[epoch 8228]: training loss: 1101.434082, consuming time:65.3052 s\n",
      "[epoch 8229]: training loss: 983.848267, consuming time:63.2264 s\n",
      "[epoch 8230]: training loss: 998.924561, consuming time:65.7506 s\n",
      "[epoch 8231]: training loss: 989.842651, consuming time:63.9500 s\n",
      "[epoch 8232]: training loss: 1003.522156, consuming time:63.9481 s\n",
      "[epoch 8233]: training loss: 1127.314209, consuming time:65.3381 s\n",
      "[epoch 8234]: training loss: 983.366699, consuming time:61.7999 s\n",
      "[epoch 8235]: training loss: 874.992859, consuming time:63.8227 s\n",
      "[epoch 8236]: training loss: 876.331177, consuming time:62.7092 s\n",
      "[epoch 8237]: training loss: 1228.663574, consuming time:62.4713 s\n",
      "[epoch 8238]: training loss: 541.709473, consuming time:64.0237 s\n",
      "[epoch 8239]: training loss: 735.664795, consuming time:65.7651 s\n",
      "[epoch 8240]: training loss: 1228.781494, consuming time:66.2514 s\n",
      "[epoch 8241]: training loss: 666.373169, consuming time:61.2137 s\n",
      "[epoch 8242]: training loss: 1000.686584, consuming time:63.7968 s\n",
      "[epoch 8243]: training loss: 803.296448, consuming time:65.7792 s\n",
      "[epoch 8244]: training loss: 775.516541, consuming time:60.8394 s\n",
      "[epoch 8245]: training loss: 994.288940, consuming time:60.5642 s\n",
      "[epoch 8246]: training loss: 798.004883, consuming time:60.1039 s\n",
      "[epoch 8247]: training loss: 898.078125, consuming time:60.5757 s\n",
      "[epoch 8248]: training loss: 928.454346, consuming time:60.3025 s\n",
      "[epoch 8249]: training loss: 1055.988770, consuming time:60.1048 s\n",
      "[epoch 8250]: training loss: 912.516602, consuming time:60.3572 s\n",
      "[epoch 8251]: training loss: 1020.296570, consuming time:60.1856 s\n",
      "[epoch 8252]: training loss: 971.580994, consuming time:59.8789 s\n",
      "[epoch 8253]: training loss: 941.506470, consuming time:60.2429 s\n",
      "[epoch 8254]: training loss: 1200.229126, consuming time:60.1467 s\n",
      "[epoch 8255]: training loss: 824.889160, consuming time:60.4689 s\n",
      "[epoch 8256]: training loss: 926.902649, consuming time:60.1959 s\n",
      "[epoch 8257]: training loss: 823.101318, consuming time:60.0380 s\n",
      "[epoch 8258]: training loss: 1039.852051, consuming time:60.1341 s\n",
      "[epoch 8259]: training loss: 736.095032, consuming time:60.3447 s\n",
      "[epoch 8260]: training loss: 869.857178, consuming time:60.6303 s\n",
      "[epoch 8261]: training loss: 1164.880493, consuming time:60.3808 s\n",
      "[epoch 8262]: training loss: 737.471558, consuming time:60.5169 s\n",
      "[epoch 8263]: training loss: 914.051819, consuming time:60.5490 s\n",
      "[epoch 8264]: training loss: 788.940491, consuming time:60.1594 s\n",
      "[epoch 8265]: training loss: 866.642578, consuming time:60.2137 s\n",
      "[epoch 8266]: training loss: 1048.025879, consuming time:60.4631 s\n",
      "[epoch 8267]: training loss: 949.008301, consuming time:60.4215 s\n",
      "[epoch 8268]: training loss: 694.202759, consuming time:60.6318 s\n",
      "[epoch 8269]: training loss: 937.827759, consuming time:60.7164 s\n",
      "[epoch 8270]: training loss: 914.325745, consuming time:60.2806 s\n",
      "[epoch 8271]: training loss: 978.454651, consuming time:60.4238 s\n",
      "[epoch 8272]: training loss: 1049.640259, consuming time:60.3672 s\n",
      "[epoch 8273]: training loss: 1124.343384, consuming time:60.3888 s\n",
      "[epoch 8274]: training loss: 904.874146, consuming time:60.2158 s\n",
      "[epoch 8275]: training loss: 1238.199829, consuming time:60.3368 s\n",
      "[epoch 8276]: training loss: 822.425171, consuming time:60.1839 s\n",
      "[epoch 8277]: training loss: 849.754761, consuming time:60.3681 s\n",
      "[epoch 8278]: training loss: 1009.104187, consuming time:60.3298 s\n",
      "[epoch 8279]: training loss: 1026.708984, consuming time:60.6737 s\n",
      "[epoch 8280]: training loss: 806.959839, consuming time:60.9155 s\n",
      "[epoch 8281]: training loss: 950.789429, consuming time:61.0887 s\n",
      "[epoch 8282]: training loss: 889.301575, consuming time:63.0287 s\n",
      "[epoch 8283]: training loss: 1045.958130, consuming time:61.6008 s\n",
      "[epoch 8284]: training loss: 812.889709, consuming time:62.1628 s\n",
      "[epoch 8285]: training loss: 863.227234, consuming time:60.8551 s\n",
      "[epoch 8286]: training loss: 1023.191040, consuming time:60.6650 s\n",
      "[epoch 8287]: training loss: 880.837646, consuming time:61.1519 s\n",
      "[epoch 8288]: training loss: 897.508667, consuming time:60.2815 s\n",
      "[epoch 8289]: training loss: 967.831909, consuming time:60.9371 s\n",
      "[epoch 8290]: training loss: 1092.826172, consuming time:61.1102 s\n",
      "[epoch 8291]: training loss: 813.833740, consuming time:61.5012 s\n",
      "[epoch 8292]: training loss: 862.239746, consuming time:60.7242 s\n",
      "[epoch 8293]: training loss: 1053.543457, consuming time:60.5795 s\n",
      "[epoch 8294]: training loss: 866.889526, consuming time:60.8161 s\n",
      "[epoch 8295]: training loss: 801.905396, consuming time:61.1752 s\n",
      "[epoch 8296]: training loss: 536.626587, consuming time:60.6614 s\n",
      "[epoch 8297]: training loss: 859.986755, consuming time:61.1506 s\n",
      "[epoch 8298]: training loss: 1098.446045, consuming time:60.4374 s\n",
      "[epoch 8299]: training loss: 859.973755, consuming time:60.8019 s\n",
      "[epoch 8300]: training loss: 869.590942, consuming time:60.9176 s\n",
      "[epoch 8301]: training loss: 1188.411621, consuming time:60.6403 s\n",
      "[epoch 8302]: training loss: 970.038696, consuming time:60.4036 s\n",
      "[epoch 8303]: training loss: 1169.395020, consuming time:60.2189 s\n",
      "[epoch 8304]: training loss: 1216.575073, consuming time:60.3809 s\n",
      "[epoch 8305]: training loss: 832.551514, consuming time:60.5342 s\n",
      "[epoch 8306]: training loss: 702.168640, consuming time:60.7789 s\n",
      "[epoch 8307]: training loss: 947.965698, consuming time:60.9610 s\n",
      "[epoch 8308]: training loss: 718.301147, consuming time:60.4801 s\n",
      "[epoch 8309]: training loss: 1105.487793, consuming time:60.7044 s\n",
      "[epoch 8310]: training loss: 1125.992188, consuming time:60.4795 s\n",
      "[epoch 8311]: training loss: 952.644043, consuming time:60.5981 s\n",
      "[epoch 8312]: training loss: 845.657471, consuming time:60.4918 s\n",
      "[epoch 8313]: training loss: 789.665222, consuming time:61.0712 s\n",
      "[epoch 8314]: training loss: 895.018860, consuming time:62.0076 s\n",
      "[epoch 8315]: training loss: 873.701294, consuming time:60.9457 s\n",
      "[epoch 8316]: training loss: 1272.766113, consuming time:60.4566 s\n",
      "[epoch 8317]: training loss: 1088.806519, consuming time:61.9996 s\n",
      "[epoch 8318]: training loss: 954.847900, consuming time:61.1896 s\n",
      "[epoch 8319]: training loss: 1109.264526, consuming time:60.6605 s\n",
      "[epoch 8320]: training loss: 925.737488, consuming time:61.2357 s\n",
      "[epoch 8321]: training loss: 786.604492, consuming time:60.4850 s\n",
      "[epoch 8322]: training loss: 1184.779175, consuming time:60.7996 s\n",
      "[epoch 8323]: training loss: 990.206726, consuming time:60.4969 s\n",
      "[epoch 8324]: training loss: 983.425903, consuming time:60.8034 s\n",
      "[epoch 8325]: training loss: 853.000793, consuming time:61.1517 s\n",
      "[epoch 8326]: training loss: 903.585144, consuming time:61.0807 s\n",
      "[epoch 8327]: training loss: 1171.813721, consuming time:60.7538 s\n",
      "[epoch 8328]: training loss: 716.842163, consuming time:60.8466 s\n",
      "[epoch 8329]: training loss: 767.291809, consuming time:60.5138 s\n",
      "[epoch 8330]: training loss: 768.805969, consuming time:60.7809 s\n",
      "[epoch 8331]: training loss: 1049.209229, consuming time:61.2153 s\n",
      "[epoch 8332]: training loss: 805.533569, consuming time:61.3844 s\n",
      "[epoch 8333]: training loss: 1160.809204, consuming time:60.8586 s\n",
      "[epoch 8334]: training loss: 756.288696, consuming time:61.9257 s\n",
      "[epoch 8335]: training loss: 953.368774, consuming time:61.0091 s\n",
      "[epoch 8336]: training loss: 1047.312744, consuming time:61.3462 s\n",
      "[epoch 8337]: training loss: 652.783020, consuming time:61.2441 s\n",
      "[epoch 8338]: training loss: 1062.195557, consuming time:61.1295 s\n",
      "[epoch 8339]: training loss: 1060.341431, consuming time:60.5625 s\n",
      "[epoch 8340]: training loss: 793.071655, consuming time:60.0892 s\n",
      "[epoch 8341]: training loss: 1030.564697, consuming time:60.3061 s\n",
      "[epoch 8342]: training loss: 827.535095, consuming time:60.7159 s\n",
      "[epoch 8343]: training loss: 783.423950, consuming time:61.6007 s\n",
      "[epoch 8344]: training loss: 835.594971, consuming time:60.6866 s\n",
      "[epoch 8345]: training loss: 1024.592529, consuming time:60.3602 s\n",
      "[epoch 8346]: training loss: 918.509766, consuming time:61.3848 s\n",
      "[epoch 8347]: training loss: 1233.971436, consuming time:60.9271 s\n",
      "[epoch 8348]: training loss: 913.206543, consuming time:60.7217 s\n",
      "[epoch 8349]: training loss: 734.678162, consuming time:60.1261 s\n",
      "[epoch 8350]: training loss: 1016.618408, consuming time:60.6133 s\n",
      "[epoch 8351]: training loss: 822.808228, consuming time:60.9509 s\n",
      "[epoch 8352]: training loss: 1195.318237, consuming time:65.0851 s\n",
      "[epoch 8353]: training loss: 780.799805, consuming time:64.2555 s\n",
      "[epoch 8354]: training loss: 954.451782, consuming time:64.6880 s\n",
      "[epoch 8355]: training loss: 776.830872, consuming time:65.5372 s\n",
      "[epoch 8356]: training loss: 1304.862305, consuming time:63.0762 s\n",
      "[epoch 8357]: training loss: 906.162109, consuming time:60.9541 s\n",
      "[epoch 8358]: training loss: 717.419922, consuming time:60.9393 s\n",
      "[epoch 8359]: training loss: 1084.153687, consuming time:60.6139 s\n",
      "[epoch 8360]: training loss: 725.438477, consuming time:61.1892 s\n",
      "[epoch 8361]: training loss: 820.469055, consuming time:61.3119 s\n",
      "[epoch 8362]: training loss: 1129.342529, consuming time:60.7828 s\n",
      "[epoch 8363]: training loss: 718.760071, consuming time:60.7003 s\n",
      "[epoch 8364]: training loss: 960.809204, consuming time:60.2105 s\n",
      "[epoch 8365]: training loss: 876.322144, consuming time:60.8546 s\n",
      "[epoch 8366]: training loss: 859.353455, consuming time:64.5879 s\n",
      "[epoch 8367]: training loss: 872.063354, consuming time:65.3849 s\n",
      "[epoch 8368]: training loss: 815.092896, consuming time:61.4584 s\n",
      "[epoch 8369]: training loss: 933.205933, consuming time:61.5813 s\n",
      "[epoch 8370]: training loss: 887.705872, consuming time:60.9526 s\n",
      "[epoch 8371]: training loss: 899.216187, consuming time:61.1607 s\n",
      "[epoch 8372]: training loss: 1105.203857, consuming time:61.2112 s\n",
      "[epoch 8373]: training loss: 815.547607, consuming time:61.2122 s\n",
      "[epoch 8374]: training loss: 924.502319, consuming time:61.3321 s\n",
      "[epoch 8375]: training loss: 748.667419, consuming time:61.1160 s\n",
      "[epoch 8376]: training loss: 1040.158936, consuming time:61.4671 s\n",
      "[epoch 8377]: training loss: 812.260986, consuming time:61.2022 s\n",
      "[epoch 8378]: training loss: 872.182983, consuming time:61.3795 s\n",
      "[epoch 8379]: training loss: 1030.469116, consuming time:63.4989 s\n",
      "[epoch 8380]: training loss: 892.212769, consuming time:63.2112 s\n",
      "[epoch 8381]: training loss: 790.148804, consuming time:64.3575 s\n",
      "[epoch 8382]: training loss: 865.413025, consuming time:60.4979 s\n",
      "[epoch 8383]: training loss: 702.551697, consuming time:60.3564 s\n",
      "[epoch 8384]: training loss: 797.727295, consuming time:60.7717 s\n",
      "[epoch 8385]: training loss: 928.690002, consuming time:62.1723 s\n",
      "[epoch 8386]: training loss: 1117.576172, consuming time:62.0109 s\n",
      "[epoch 8387]: training loss: 947.311523, consuming time:60.7510 s\n",
      "[epoch 8388]: training loss: 794.852966, consuming time:60.8107 s\n",
      "[epoch 8389]: training loss: 967.203979, consuming time:60.7049 s\n",
      "[epoch 8390]: training loss: 926.223877, consuming time:60.8742 s\n",
      "[epoch 8391]: training loss: 925.097900, consuming time:60.9528 s\n",
      "[epoch 8392]: training loss: 935.782227, consuming time:60.7660 s\n",
      "[epoch 8393]: training loss: 704.467285, consuming time:64.3485 s\n",
      "[epoch 8394]: training loss: 1139.659912, consuming time:66.4198 s\n",
      "[epoch 8395]: training loss: 1061.842529, consuming time:66.0128 s\n",
      "[epoch 8396]: training loss: 707.707520, consuming time:62.7275 s\n",
      "[epoch 8397]: training loss: 911.528687, consuming time:63.9726 s\n",
      "[epoch 8398]: training loss: 1207.251953, consuming time:62.1848 s\n",
      "[epoch 8399]: training loss: 993.879150, consuming time:59.5216 s\n",
      "[epoch 8400]: training loss: 876.732910, consuming time:60.4412 s\n",
      "[epoch 8401]: training loss: 878.985535, consuming time:60.6736 s\n",
      "[epoch 8402]: training loss: 1372.345215, consuming time:60.1522 s\n",
      "[epoch 8403]: training loss: 1078.766968, consuming time:64.2975 s\n",
      "[epoch 8404]: training loss: 1021.664795, consuming time:62.4579 s\n",
      "[epoch 8405]: training loss: 754.194153, consuming time:62.7765 s\n",
      "[epoch 8406]: training loss: 823.289062, consuming time:61.8750 s\n",
      "[epoch 8407]: training loss: 1097.267212, consuming time:62.0339 s\n",
      "[epoch 8408]: training loss: 782.421875, consuming time:63.1728 s\n",
      "[epoch 8409]: training loss: 880.282898, consuming time:63.7743 s\n",
      "[epoch 8410]: training loss: 824.603271, consuming time:61.2304 s\n",
      "[epoch 8411]: training loss: 1044.363647, consuming time:61.3363 s\n",
      "[epoch 8412]: training loss: 899.632324, consuming time:63.4831 s\n",
      "[epoch 8413]: training loss: 1020.227051, consuming time:62.0849 s\n",
      "[epoch 8414]: training loss: 891.620605, consuming time:61.0778 s\n",
      "[epoch 8415]: training loss: 1058.372559, consuming time:61.0939 s\n",
      "[epoch 8416]: training loss: 898.575989, consuming time:60.5586 s\n",
      "[epoch 8417]: training loss: 798.543091, consuming time:60.8448 s\n",
      "[epoch 8418]: training loss: 864.150146, consuming time:61.3990 s\n",
      "[epoch 8419]: training loss: 919.347778, consuming time:61.6229 s\n",
      "[epoch 8420]: training loss: 896.050049, consuming time:62.4329 s\n",
      "[epoch 8421]: training loss: 898.936646, consuming time:61.7703 s\n",
      "[epoch 8422]: training loss: 1043.434448, consuming time:61.8480 s\n",
      "[epoch 8423]: training loss: 1014.132080, consuming time:61.7916 s\n",
      "[epoch 8424]: training loss: 767.140015, consuming time:61.9098 s\n",
      "[epoch 8425]: training loss: 1056.728271, consuming time:61.1175 s\n",
      "[epoch 8426]: training loss: 929.443115, consuming time:61.0164 s\n",
      "[epoch 8427]: training loss: 1063.913696, consuming time:61.0831 s\n",
      "[epoch 8428]: training loss: 689.680908, consuming time:60.6068 s\n",
      "[epoch 8429]: training loss: 1285.284424, consuming time:60.6989 s\n",
      "[epoch 8430]: training loss: 969.793518, consuming time:61.1824 s\n",
      "[epoch 8431]: training loss: 1156.537598, consuming time:61.1210 s\n",
      "[epoch 8432]: training loss: 929.917480, consuming time:60.8529 s\n",
      "[epoch 8433]: training loss: 1008.171875, consuming time:60.8671 s\n",
      "[epoch 8434]: training loss: 988.251831, consuming time:60.5264 s\n",
      "[epoch 8435]: training loss: 848.274048, consuming time:60.8168 s\n",
      "[epoch 8436]: training loss: 1064.628174, consuming time:61.0463 s\n",
      "[epoch 8437]: training loss: 959.468628, consuming time:60.7822 s\n",
      "[epoch 8438]: training loss: 968.162720, consuming time:60.9680 s\n",
      "[epoch 8439]: training loss: 711.526855, consuming time:61.1438 s\n",
      "[epoch 8440]: training loss: 1000.135803, consuming time:60.6207 s\n",
      "[epoch 8441]: training loss: 825.221680, consuming time:60.5149 s\n",
      "[epoch 8442]: training loss: 877.380737, consuming time:61.2311 s\n",
      "[epoch 8443]: training loss: 793.414612, consuming time:60.7837 s\n",
      "[epoch 8444]: training loss: 1149.375488, consuming time:60.6577 s\n",
      "[epoch 8445]: training loss: 953.607666, consuming time:60.8023 s\n",
      "[epoch 8446]: training loss: 881.970947, consuming time:60.8976 s\n",
      "[epoch 8447]: training loss: 1010.523560, consuming time:60.8052 s\n",
      "[epoch 8448]: training loss: 804.949341, consuming time:60.7441 s\n",
      "[epoch 8449]: training loss: 1106.347168, consuming time:60.6212 s\n",
      "[epoch 8450]: training loss: 798.764038, consuming time:63.2054 s\n",
      "[epoch 8451]: training loss: 742.944641, consuming time:61.6345 s\n",
      "[epoch 8452]: training loss: 815.897949, consuming time:61.3714 s\n",
      "[epoch 8453]: training loss: 1073.203857, consuming time:61.3210 s\n",
      "[epoch 8454]: training loss: 771.300537, consuming time:62.1862 s\n",
      "[epoch 8455]: training loss: 1037.695312, consuming time:60.3610 s\n",
      "[epoch 8456]: training loss: 864.217163, consuming time:60.6474 s\n",
      "[epoch 8457]: training loss: 757.737183, consuming time:60.3162 s\n",
      "[epoch 8458]: training loss: 829.407410, consuming time:60.2848 s\n",
      "[epoch 8459]: training loss: 690.112122, consuming time:65.4590 s\n",
      "[epoch 8460]: training loss: 999.224243, consuming time:60.7008 s\n",
      "[epoch 8461]: training loss: 962.259827, consuming time:61.2113 s\n",
      "[epoch 8462]: training loss: 662.630005, consuming time:60.8368 s\n",
      "[epoch 8463]: training loss: 1059.896729, consuming time:60.5923 s\n",
      "[epoch 8464]: training loss: 1139.866943, consuming time:61.5310 s\n",
      "[epoch 8465]: training loss: 874.281128, consuming time:61.5362 s\n",
      "[epoch 8466]: training loss: 935.325500, consuming time:60.8548 s\n",
      "[epoch 8467]: training loss: 1546.553833, consuming time:60.6998 s\n",
      "[epoch 8468]: training loss: 1060.754028, consuming time:61.4084 s\n",
      "[epoch 8469]: training loss: 1146.902466, consuming time:63.3610 s\n",
      "[epoch 8470]: training loss: 950.969482, consuming time:62.1448 s\n",
      "[epoch 8471]: training loss: 1019.477356, consuming time:62.0034 s\n",
      "[epoch 8472]: training loss: 951.513611, consuming time:65.7360 s\n",
      "[epoch 8473]: training loss: 1099.515015, consuming time:64.6819 s\n",
      "[epoch 8474]: training loss: 699.401123, consuming time:61.3370 s\n",
      "[epoch 8475]: training loss: 962.302612, consuming time:60.9403 s\n",
      "[epoch 8476]: training loss: 841.580261, consuming time:61.2902 s\n",
      "[epoch 8477]: training loss: 927.164917, consuming time:61.2225 s\n",
      "[epoch 8478]: training loss: 666.181519, consuming time:61.4049 s\n",
      "[epoch 8479]: training loss: 635.786011, consuming time:63.3335 s\n",
      "[epoch 8480]: training loss: 879.490784, consuming time:61.3742 s\n",
      "[epoch 8481]: training loss: 894.079712, consuming time:61.3457 s\n",
      "[epoch 8482]: training loss: 1062.283325, consuming time:66.1698 s\n",
      "[epoch 8483]: training loss: 867.841980, consuming time:66.5409 s\n",
      "[epoch 8484]: training loss: 829.964294, consuming time:62.1266 s\n",
      "[epoch 8485]: training loss: 842.538330, consuming time:60.5305 s\n",
      "[epoch 8486]: training loss: 791.706116, consuming time:61.5311 s\n",
      "[epoch 8487]: training loss: 853.043579, consuming time:61.4935 s\n",
      "[epoch 8488]: training loss: 769.191528, consuming time:63.2478 s\n",
      "[epoch 8489]: training loss: 863.229126, consuming time:62.6962 s\n",
      "[epoch 8490]: training loss: 999.789795, consuming time:67.5313 s\n",
      "[epoch 8491]: training loss: 1073.618042, consuming time:64.3209 s\n",
      "[epoch 8492]: training loss: 874.789124, consuming time:65.5671 s\n",
      "[epoch 8493]: training loss: 982.347534, consuming time:62.8562 s\n",
      "[epoch 8494]: training loss: 837.230591, consuming time:61.8758 s\n",
      "[epoch 8495]: training loss: 936.463745, consuming time:60.2687 s\n",
      "[epoch 8496]: training loss: 1021.919800, consuming time:60.2962 s\n",
      "[epoch 8497]: training loss: 913.510864, consuming time:60.8204 s\n",
      "[epoch 8498]: training loss: 944.509766, consuming time:61.1125 s\n",
      "[epoch 8499]: training loss: 796.942200, consuming time:61.0556 s\n",
      "[epoch 8500]: training loss: 666.917664, consuming time:60.6666 s\n",
      "[epoch 8501]: training loss: 792.150085, consuming time:60.1033 s\n",
      "[epoch 8502]: training loss: 815.384888, consuming time:61.0067 s\n",
      "[epoch 8503]: training loss: 993.459473, consuming time:60.8114 s\n",
      "[epoch 8504]: training loss: 836.136353, consuming time:60.9978 s\n",
      "[epoch 8505]: training loss: 1190.372925, consuming time:62.8059 s\n",
      "[epoch 8506]: training loss: 732.715332, consuming time:64.0754 s\n",
      "[epoch 8507]: training loss: 723.507996, consuming time:61.6295 s\n",
      "[epoch 8508]: training loss: 954.229980, consuming time:64.0599 s\n",
      "[epoch 8509]: training loss: 866.802124, consuming time:61.1585 s\n",
      "[epoch 8510]: training loss: 933.795166, consuming time:62.2365 s\n",
      "[epoch 8511]: training loss: 804.343323, consuming time:63.1763 s\n",
      "[epoch 8512]: training loss: 868.566895, consuming time:60.3690 s\n",
      "[epoch 8513]: training loss: 1112.721924, consuming time:60.7466 s\n",
      "[epoch 8514]: training loss: 1104.226562, consuming time:62.9236 s\n",
      "[epoch 8515]: training loss: 1077.723755, consuming time:61.1230 s\n",
      "[epoch 8516]: training loss: 885.382080, consuming time:64.7252 s\n",
      "[epoch 8517]: training loss: 858.504089, consuming time:65.8477 s\n",
      "[epoch 8518]: training loss: 1078.455322, consuming time:64.2229 s\n",
      "[epoch 8519]: training loss: 1077.658936, consuming time:63.2766 s\n",
      "[epoch 8520]: training loss: 1128.014648, consuming time:61.6821 s\n",
      "[epoch 8521]: training loss: 963.081299, consuming time:60.8013 s\n",
      "[epoch 8522]: training loss: 941.535889, consuming time:61.4457 s\n",
      "[epoch 8523]: training loss: 995.380554, consuming time:60.3002 s\n",
      "[epoch 8524]: training loss: 778.788818, consuming time:60.1941 s\n",
      "[epoch 8525]: training loss: 982.889221, consuming time:60.6793 s\n",
      "[epoch 8526]: training loss: 1078.391113, consuming time:61.3357 s\n",
      "[epoch 8527]: training loss: 753.365601, consuming time:60.8838 s\n",
      "[epoch 8528]: training loss: 870.048706, consuming time:61.1588 s\n",
      "[epoch 8529]: training loss: 1137.227295, consuming time:60.8529 s\n",
      "[epoch 8530]: training loss: 811.912659, consuming time:60.8407 s\n",
      "[epoch 8531]: training loss: 603.171753, consuming time:60.6337 s\n",
      "[epoch 8532]: training loss: 834.047852, consuming time:60.6453 s\n",
      "[epoch 8533]: training loss: 782.166504, consuming time:60.8530 s\n",
      "[epoch 8534]: training loss: 1052.302246, consuming time:60.8059 s\n",
      "[epoch 8535]: training loss: 981.160645, consuming time:60.6379 s\n",
      "[epoch 8536]: training loss: 749.615906, consuming time:60.5364 s\n",
      "[epoch 8537]: training loss: 1026.535278, consuming time:60.7362 s\n",
      "[epoch 8538]: training loss: 1060.387207, consuming time:61.0140 s\n",
      "[epoch 8539]: training loss: 1204.097046, consuming time:61.0383 s\n",
      "[epoch 8540]: training loss: 836.828918, consuming time:60.9911 s\n",
      "[epoch 8541]: training loss: 781.136475, consuming time:62.7026 s\n",
      "[epoch 8542]: training loss: 923.769165, consuming time:64.7393 s\n",
      "[epoch 8543]: training loss: 699.655457, consuming time:65.2582 s\n",
      "[epoch 8544]: training loss: 777.032959, consuming time:65.0435 s\n",
      "[epoch 8545]: training loss: 723.255981, consuming time:63.3821 s\n",
      "[epoch 8546]: training loss: 828.860229, consuming time:64.8368 s\n",
      "[epoch 8547]: training loss: 1059.974854, consuming time:61.4931 s\n",
      "[epoch 8548]: training loss: 913.675781, consuming time:61.6415 s\n",
      "[epoch 8549]: training loss: 1017.730347, consuming time:61.7377 s\n",
      "[epoch 8550]: training loss: 943.277588, consuming time:62.1923 s\n",
      "[epoch 8551]: training loss: 984.224854, consuming time:63.4876 s\n",
      "[epoch 8552]: training loss: 914.384766, consuming time:62.8068 s\n",
      "[epoch 8553]: training loss: 663.826965, consuming time:62.3056 s\n",
      "[epoch 8554]: training loss: 701.171021, consuming time:62.3422 s\n",
      "[epoch 8555]: training loss: 1099.478271, consuming time:62.7617 s\n",
      "[epoch 8556]: training loss: 798.759888, consuming time:62.9695 s\n",
      "[epoch 8557]: training loss: 929.758911, consuming time:63.0617 s\n",
      "[epoch 8558]: training loss: 860.918274, consuming time:62.8416 s\n",
      "[epoch 8559]: training loss: 739.900391, consuming time:63.1741 s\n",
      "[epoch 8560]: training loss: 955.204346, consuming time:62.8864 s\n",
      "[epoch 8561]: training loss: 685.536499, consuming time:63.2994 s\n",
      "[epoch 8562]: training loss: 991.553894, consuming time:62.6282 s\n",
      "[epoch 8563]: training loss: 957.550964, consuming time:62.8074 s\n",
      "[epoch 8564]: training loss: 1005.058716, consuming time:62.7676 s\n",
      "[epoch 8565]: training loss: 816.614258, consuming time:63.0531 s\n",
      "[epoch 8566]: training loss: 946.385376, consuming time:61.5379 s\n",
      "[epoch 8567]: training loss: 1054.501221, consuming time:62.2402 s\n",
      "[epoch 8568]: training loss: 645.922852, consuming time:66.9840 s\n",
      "[epoch 8569]: training loss: 1152.769897, consuming time:65.1129 s\n",
      "[epoch 8570]: training loss: 892.964600, consuming time:65.8623 s\n",
      "[epoch 8571]: training loss: 895.871582, consuming time:65.2645 s\n",
      "[epoch 8572]: training loss: 1233.561523, consuming time:64.9004 s\n",
      "[epoch 8573]: training loss: 1004.658447, consuming time:66.5399 s\n",
      "[epoch 8574]: training loss: 768.272278, consuming time:65.3864 s\n",
      "[epoch 8575]: training loss: 804.975464, consuming time:63.2947 s\n",
      "[epoch 8576]: training loss: 944.491943, consuming time:62.6762 s\n",
      "[epoch 8577]: training loss: 773.672913, consuming time:64.5227 s\n",
      "[epoch 8578]: training loss: 836.756348, consuming time:61.6027 s\n",
      "[epoch 8579]: training loss: 838.997986, consuming time:61.7419 s\n",
      "[epoch 8580]: training loss: 932.115662, consuming time:62.1283 s\n",
      "[epoch 8581]: training loss: 831.422302, consuming time:69.0173 s\n",
      "[epoch 8582]: training loss: 779.713684, consuming time:71.9636 s\n",
      "[epoch 8583]: training loss: 877.530029, consuming time:69.9022 s\n",
      "[epoch 8584]: training loss: 1009.985046, consuming time:66.3119 s\n",
      "[epoch 8585]: training loss: 970.924011, consuming time:62.9316 s\n",
      "[epoch 8586]: training loss: 939.588379, consuming time:63.2437 s\n",
      "[epoch 8587]: training loss: 1464.063477, consuming time:62.3578 s\n",
      "[epoch 8588]: training loss: 1172.338623, consuming time:62.1187 s\n",
      "[epoch 8589]: training loss: 1011.984131, consuming time:61.7762 s\n",
      "[epoch 8590]: training loss: 979.041748, consuming time:62.1941 s\n",
      "[epoch 8591]: training loss: 883.228394, consuming time:62.0581 s\n",
      "[epoch 8592]: training loss: 1378.925903, consuming time:61.8800 s\n",
      "[epoch 8593]: training loss: 797.254150, consuming time:62.1708 s\n",
      "[epoch 8594]: training loss: 1068.504150, consuming time:61.8639 s\n",
      "[epoch 8595]: training loss: 1232.981201, consuming time:62.3393 s\n",
      "[epoch 8596]: training loss: 866.942871, consuming time:62.2885 s\n",
      "[epoch 8597]: training loss: 880.933228, consuming time:62.0906 s\n",
      "[epoch 8598]: training loss: 1096.709351, consuming time:62.1935 s\n",
      "[epoch 8599]: training loss: 1120.746338, consuming time:62.1040 s\n",
      "[epoch 8600]: training loss: 1136.983887, consuming time:62.3666 s\n",
      "[epoch 8601]: training loss: 914.583801, consuming time:62.2006 s\n",
      "[epoch 8602]: training loss: 897.116089, consuming time:62.2241 s\n",
      "[epoch 8603]: training loss: 745.582458, consuming time:62.2263 s\n",
      "[epoch 8604]: training loss: 876.242798, consuming time:62.0166 s\n",
      "[epoch 8605]: training loss: 1064.073242, consuming time:62.7408 s\n",
      "[epoch 8606]: training loss: 896.915710, consuming time:63.0424 s\n",
      "[epoch 8607]: training loss: 766.216614, consuming time:62.8665 s\n",
      "[epoch 8608]: training loss: 780.255066, consuming time:62.8135 s\n",
      "[epoch 8609]: training loss: 742.379883, consuming time:62.7206 s\n",
      "[epoch 8610]: training loss: 1166.882080, consuming time:62.3762 s\n",
      "[epoch 8611]: training loss: 907.453125, consuming time:62.0899 s\n",
      "[epoch 8612]: training loss: 747.056091, consuming time:62.1731 s\n",
      "[epoch 8613]: training loss: 975.375732, consuming time:62.2039 s\n",
      "[epoch 8614]: training loss: 985.454224, consuming time:63.0041 s\n",
      "[epoch 8615]: training loss: 1092.513428, consuming time:62.5465 s\n",
      "[epoch 8616]: training loss: 1034.739868, consuming time:62.3927 s\n",
      "[epoch 8617]: training loss: 997.017456, consuming time:62.6721 s\n",
      "[epoch 8618]: training loss: 990.520630, consuming time:62.3197 s\n",
      "[epoch 8619]: training loss: 798.250977, consuming time:62.3130 s\n",
      "[epoch 8620]: training loss: 982.877930, consuming time:62.5581 s\n",
      "[epoch 8621]: training loss: 717.782410, consuming time:62.3749 s\n",
      "[epoch 8622]: training loss: 857.727966, consuming time:62.5720 s\n",
      "[epoch 8623]: training loss: 1267.712280, consuming time:62.4905 s\n",
      "[epoch 8624]: training loss: 916.876709, consuming time:61.6629 s\n",
      "[epoch 8625]: training loss: 882.488037, consuming time:60.7454 s\n",
      "[epoch 8626]: training loss: 886.610229, consuming time:60.9754 s\n",
      "[epoch 8627]: training loss: 850.738220, consuming time:61.5551 s\n",
      "[epoch 8628]: training loss: 808.051636, consuming time:63.7796 s\n",
      "[epoch 8629]: training loss: 934.078857, consuming time:62.1167 s\n",
      "[epoch 8630]: training loss: 919.855896, consuming time:62.1272 s\n",
      "[epoch 8631]: training loss: 851.482178, consuming time:62.2250 s\n",
      "[epoch 8632]: training loss: 1204.185791, consuming time:62.5878 s\n",
      "[epoch 8633]: training loss: 1142.145020, consuming time:62.3504 s\n",
      "[epoch 8634]: training loss: 1058.064453, consuming time:61.9496 s\n",
      "[epoch 8635]: training loss: 1057.991211, consuming time:61.5229 s\n",
      "[epoch 8636]: training loss: 888.792114, consuming time:61.6088 s\n",
      "[epoch 8637]: training loss: 1118.135498, consuming time:61.6052 s\n",
      "[epoch 8638]: training loss: 567.310059, consuming time:61.7039 s\n",
      "[epoch 8639]: training loss: 820.022461, consuming time:61.7765 s\n",
      "[epoch 8640]: training loss: 863.519958, consuming time:62.1026 s\n",
      "[epoch 8641]: training loss: 917.355957, consuming time:62.3939 s\n",
      "[epoch 8642]: training loss: 1067.752563, consuming time:62.0477 s\n",
      "[epoch 8643]: training loss: 1014.246826, consuming time:62.1774 s\n",
      "[epoch 8644]: training loss: 788.231934, consuming time:62.8650 s\n",
      "[epoch 8645]: training loss: 957.820435, consuming time:62.4789 s\n",
      "[epoch 8646]: training loss: 819.226318, consuming time:61.1190 s\n",
      "[epoch 8647]: training loss: 701.395264, consuming time:62.6176 s\n",
      "[epoch 8648]: training loss: 916.791748, consuming time:60.5155 s\n",
      "[epoch 8649]: training loss: 945.442810, consuming time:60.8231 s\n",
      "[epoch 8650]: training loss: 1216.000000, consuming time:61.0241 s\n",
      "[epoch 8651]: training loss: 725.445435, consuming time:61.8105 s\n",
      "[epoch 8652]: training loss: 801.007263, consuming time:62.8615 s\n",
      "[epoch 8653]: training loss: 738.949707, consuming time:60.8917 s\n",
      "[epoch 8654]: training loss: 796.991577, consuming time:61.1306 s\n",
      "[epoch 8655]: training loss: 886.415222, consuming time:60.8687 s\n",
      "[epoch 8656]: training loss: 797.874146, consuming time:60.7949 s\n",
      "[epoch 8657]: training loss: 787.679565, consuming time:60.6542 s\n",
      "[epoch 8658]: training loss: 702.754517, consuming time:60.9844 s\n",
      "[epoch 8659]: training loss: 1259.731689, consuming time:60.8007 s\n",
      "[epoch 8660]: training loss: 885.168945, consuming time:60.5792 s\n",
      "[epoch 8661]: training loss: 963.877563, consuming time:60.8199 s\n",
      "[epoch 8662]: training loss: 882.570312, consuming time:61.6417 s\n",
      "[epoch 8663]: training loss: 804.095825, consuming time:65.0116 s\n",
      "[epoch 8664]: training loss: 985.962769, consuming time:65.0867 s\n",
      "[epoch 8665]: training loss: 839.926697, consuming time:63.9611 s\n",
      "[epoch 8666]: training loss: 786.801331, consuming time:61.8626 s\n",
      "[epoch 8667]: training loss: 1083.856934, consuming time:62.0720 s\n",
      "[epoch 8668]: training loss: 1037.981812, consuming time:61.4527 s\n",
      "[epoch 8669]: training loss: 805.909790, consuming time:61.0406 s\n",
      "[epoch 8670]: training loss: 1066.639771, consuming time:60.8027 s\n",
      "[epoch 8671]: training loss: 889.018799, consuming time:60.2114 s\n",
      "[epoch 8672]: training loss: 1223.305908, consuming time:60.8192 s\n",
      "[epoch 8673]: training loss: 1013.258362, consuming time:61.0865 s\n",
      "[epoch 8674]: training loss: 746.871948, consuming time:60.9333 s\n",
      "[epoch 8675]: training loss: 627.991577, consuming time:60.7482 s\n",
      "[epoch 8676]: training loss: 838.668152, consuming time:60.8067 s\n",
      "[epoch 8677]: training loss: 799.032227, consuming time:60.9427 s\n",
      "[epoch 8678]: training loss: 789.494019, consuming time:61.5712 s\n",
      "[epoch 8679]: training loss: 1214.184570, consuming time:60.9766 s\n",
      "[epoch 8680]: training loss: 1170.102295, consuming time:60.5935 s\n",
      "[epoch 8681]: training loss: 903.777344, consuming time:60.5657 s\n",
      "[epoch 8682]: training loss: 1005.604370, consuming time:61.2641 s\n",
      "[epoch 8683]: training loss: 1222.115234, consuming time:60.7299 s\n",
      "[epoch 8684]: training loss: 944.929565, consuming time:60.8927 s\n",
      "[epoch 8685]: training loss: 1112.246582, consuming time:60.9744 s\n",
      "[epoch 8686]: training loss: 867.488403, consuming time:60.8785 s\n",
      "[epoch 8687]: training loss: 932.384644, consuming time:60.5773 s\n",
      "[epoch 8688]: training loss: 784.395264, consuming time:60.9312 s\n",
      "[epoch 8689]: training loss: 975.958008, consuming time:61.0064 s\n",
      "[epoch 8690]: training loss: 1008.372742, consuming time:62.6350 s\n",
      "[epoch 8691]: training loss: 1446.971191, consuming time:62.3982 s\n",
      "[epoch 8692]: training loss: 1396.193848, consuming time:63.6630 s\n",
      "[epoch 8693]: training loss: 1073.229248, consuming time:60.9384 s\n",
      "[epoch 8694]: training loss: 796.075562, consuming time:61.2095 s\n",
      "[epoch 8695]: training loss: 1043.033691, consuming time:65.5167 s\n",
      "[epoch 8696]: training loss: 783.929565, consuming time:65.9218 s\n",
      "[epoch 8697]: training loss: 841.912354, consuming time:61.3550 s\n",
      "[epoch 8698]: training loss: 687.595947, consuming time:61.2426 s\n",
      "[epoch 8699]: training loss: 812.409668, consuming time:61.2317 s\n",
      "[epoch 8700]: training loss: 983.985107, consuming time:63.3166 s\n",
      "[epoch 8701]: training loss: 784.053955, consuming time:61.0122 s\n",
      "[epoch 8702]: training loss: 674.660828, consuming time:62.3770 s\n",
      "[epoch 8703]: training loss: 839.766724, consuming time:62.7319 s\n",
      "[epoch 8704]: training loss: 828.575867, consuming time:64.1281 s\n",
      "[epoch 8705]: training loss: 809.455811, consuming time:61.5541 s\n",
      "[epoch 8706]: training loss: 857.039795, consuming time:61.2153 s\n",
      "[epoch 8707]: training loss: 898.692017, consuming time:60.9450 s\n",
      "[epoch 8708]: training loss: 1006.403564, consuming time:61.1100 s\n",
      "[epoch 8709]: training loss: 1196.812500, consuming time:64.2974 s\n",
      "[epoch 8710]: training loss: 924.233276, consuming time:61.4047 s\n",
      "[epoch 8711]: training loss: 1080.620117, consuming time:64.2482 s\n",
      "[epoch 8712]: training loss: 1031.160522, consuming time:66.0316 s\n",
      "[epoch 8713]: training loss: 896.185669, consuming time:64.1097 s\n",
      "[epoch 8714]: training loss: 867.445801, consuming time:63.6849 s\n",
      "[epoch 8715]: training loss: 863.424194, consuming time:63.1466 s\n",
      "[epoch 8716]: training loss: 906.550903, consuming time:63.1892 s\n",
      "[epoch 8717]: training loss: 953.647339, consuming time:62.7891 s\n",
      "[epoch 8718]: training loss: 909.150330, consuming time:61.7638 s\n",
      "[epoch 8719]: training loss: 1001.582520, consuming time:62.0907 s\n",
      "[epoch 8720]: training loss: 602.989929, consuming time:61.3527 s\n",
      "[epoch 8721]: training loss: 941.068420, consuming time:65.9850 s\n",
      "[epoch 8722]: training loss: 1017.565430, consuming time:65.0126 s\n",
      "[epoch 8723]: training loss: 779.257935, consuming time:67.4125 s\n",
      "[epoch 8724]: training loss: 879.795288, consuming time:68.4438 s\n",
      "[epoch 8725]: training loss: 924.793457, consuming time:61.0967 s\n",
      "[epoch 8726]: training loss: 1089.316650, consuming time:60.6493 s\n",
      "[epoch 8727]: training loss: 699.137207, consuming time:63.4211 s\n",
      "[epoch 8728]: training loss: 980.046692, consuming time:64.4048 s\n",
      "[epoch 8729]: training loss: 1020.822754, consuming time:62.0459 s\n",
      "[epoch 8730]: training loss: 757.891357, consuming time:62.3264 s\n",
      "[epoch 8731]: training loss: 880.466980, consuming time:62.2815 s\n",
      "[epoch 8732]: training loss: 853.615906, consuming time:61.7566 s\n",
      "[epoch 8733]: training loss: 860.051208, consuming time:65.0649 s\n",
      "[epoch 8734]: training loss: 1369.593506, consuming time:63.6477 s\n",
      "[epoch 8735]: training loss: 725.532349, consuming time:67.3559 s\n",
      "[epoch 8736]: training loss: 1106.194214, consuming time:68.0642 s\n",
      "[epoch 8737]: training loss: 804.026428, consuming time:63.0227 s\n",
      "[epoch 8738]: training loss: 782.010498, consuming time:62.7566 s\n",
      "[epoch 8739]: training loss: 945.978455, consuming time:66.1507 s\n",
      "[epoch 8740]: training loss: 987.557739, consuming time:67.3197 s\n",
      "[epoch 8741]: training loss: 1002.674927, consuming time:67.9573 s\n",
      "[epoch 8742]: training loss: 767.738953, consuming time:65.2349 s\n",
      "[epoch 8743]: training loss: 991.101440, consuming time:66.1453 s\n",
      "[epoch 8744]: training loss: 1102.108276, consuming time:65.7623 s\n",
      "[epoch 8745]: training loss: 858.254517, consuming time:67.3896 s\n",
      "[epoch 8746]: training loss: 759.934570, consuming time:69.3941 s\n",
      "[epoch 8747]: training loss: 947.701416, consuming time:69.7261 s\n",
      "[epoch 8748]: training loss: 902.381592, consuming time:68.1295 s\n",
      "[epoch 8749]: training loss: 799.979736, consuming time:68.4041 s\n",
      "[epoch 8750]: training loss: 1129.063965, consuming time:67.1076 s\n",
      "[epoch 8751]: training loss: 996.963501, consuming time:62.9367 s\n",
      "[epoch 8752]: training loss: 918.380737, consuming time:61.5685 s\n",
      "[epoch 8753]: training loss: 950.716553, consuming time:61.5894 s\n",
      "[epoch 8754]: training loss: 872.341980, consuming time:61.5754 s\n",
      "[epoch 8755]: training loss: 1001.554504, consuming time:61.5540 s\n",
      "[epoch 8756]: training loss: 853.474976, consuming time:61.5281 s\n",
      "[epoch 8757]: training loss: 884.136475, consuming time:61.4814 s\n",
      "[epoch 8758]: training loss: 866.119873, consuming time:61.5573 s\n",
      "[epoch 8759]: training loss: 860.011475, consuming time:61.5491 s\n",
      "[epoch 8760]: training loss: 921.612671, consuming time:61.5264 s\n",
      "[epoch 8761]: training loss: 845.723022, consuming time:61.3028 s\n",
      "[epoch 8762]: training loss: 818.778931, consuming time:61.7366 s\n",
      "[epoch 8763]: training loss: 1059.859863, consuming time:61.7348 s\n",
      "[epoch 8764]: training loss: 765.800964, consuming time:61.6978 s\n",
      "[epoch 8765]: training loss: 922.331665, consuming time:61.7695 s\n",
      "[epoch 8766]: training loss: 927.342163, consuming time:61.6335 s\n",
      "[epoch 8767]: training loss: 1078.206055, consuming time:61.6145 s\n",
      "[epoch 8768]: training loss: 1117.758789, consuming time:61.4679 s\n",
      "[epoch 8769]: training loss: 1022.638184, consuming time:61.6945 s\n",
      "[epoch 8770]: training loss: 1041.198730, consuming time:61.7017 s\n",
      "[epoch 8771]: training loss: 1147.530762, consuming time:61.7062 s\n",
      "[epoch 8772]: training loss: 897.373657, consuming time:61.6824 s\n",
      "[epoch 8773]: training loss: 1036.196045, consuming time:61.6473 s\n",
      "[epoch 8774]: training loss: 973.257812, consuming time:61.6824 s\n",
      "[epoch 8775]: training loss: 964.821899, consuming time:61.6489 s\n",
      "[epoch 8776]: training loss: 792.758850, consuming time:61.7419 s\n",
      "[epoch 8777]: training loss: 1050.464966, consuming time:61.6289 s\n",
      "[epoch 8778]: training loss: 797.124878, consuming time:61.5944 s\n",
      "[epoch 8779]: training loss: 913.222839, consuming time:61.6036 s\n",
      "[epoch 8780]: training loss: 726.230347, consuming time:61.7166 s\n",
      "[epoch 8781]: training loss: 1202.057739, consuming time:61.5792 s\n",
      "[epoch 8782]: training loss: 1089.291870, consuming time:61.5010 s\n",
      "[epoch 8783]: training loss: 645.743713, consuming time:61.6790 s\n",
      "[epoch 8784]: training loss: 734.260193, consuming time:61.6580 s\n",
      "[epoch 8785]: training loss: 1038.472900, consuming time:61.5732 s\n",
      "[epoch 8786]: training loss: 1198.130859, consuming time:61.4004 s\n",
      "[epoch 8787]: training loss: 728.113647, consuming time:61.6572 s\n",
      "[epoch 8788]: training loss: 604.553101, consuming time:61.7507 s\n",
      "[epoch 8789]: training loss: 903.512573, consuming time:61.7166 s\n",
      "[epoch 8790]: training loss: 1051.864990, consuming time:61.6100 s\n",
      "[epoch 8791]: training loss: 613.744873, consuming time:61.5815 s\n",
      "[epoch 8792]: training loss: 789.874817, consuming time:61.6762 s\n",
      "[epoch 8793]: training loss: 792.499512, consuming time:61.8699 s\n",
      "[epoch 8794]: training loss: 1023.752686, consuming time:61.6293 s\n",
      "[epoch 8795]: training loss: 973.218201, consuming time:61.7069 s\n",
      "[epoch 8796]: training loss: 735.764771, consuming time:61.6530 s\n",
      "[epoch 8797]: training loss: 869.111816, consuming time:61.6088 s\n",
      "[epoch 8798]: training loss: 938.256714, consuming time:61.7321 s\n",
      "[epoch 8799]: training loss: 1132.269897, consuming time:61.7565 s\n",
      "[epoch 8800]: training loss: 1018.838501, consuming time:61.8927 s\n",
      "[epoch 8801]: training loss: 890.644287, consuming time:61.8414 s\n",
      "[epoch 8802]: training loss: 787.622253, consuming time:61.4926 s\n",
      "[epoch 8803]: training loss: 920.163330, consuming time:61.4344 s\n",
      "[epoch 8804]: training loss: 870.328979, consuming time:61.4771 s\n",
      "[epoch 8805]: training loss: 1063.126831, consuming time:61.7014 s\n",
      "[epoch 8806]: training loss: 971.428833, consuming time:61.5019 s\n",
      "[epoch 8807]: training loss: 797.396973, consuming time:61.7078 s\n",
      "[epoch 8808]: training loss: 661.432434, consuming time:61.4932 s\n",
      "[epoch 8809]: training loss: 810.486328, consuming time:61.5108 s\n",
      "[epoch 8810]: training loss: 1053.042603, consuming time:61.7581 s\n",
      "[epoch 8811]: training loss: 941.382507, consuming time:61.6391 s\n",
      "[epoch 8812]: training loss: 1125.178711, consuming time:61.6191 s\n",
      "[epoch 8813]: training loss: 782.725220, consuming time:61.6032 s\n",
      "[epoch 8814]: training loss: 874.748535, consuming time:61.6859 s\n",
      "[epoch 8815]: training loss: 1152.509033, consuming time:61.7217 s\n",
      "[epoch 8816]: training loss: 936.604797, consuming time:61.7250 s\n",
      "[epoch 8817]: training loss: 973.794861, consuming time:61.5385 s\n",
      "[epoch 8818]: training loss: 962.397888, consuming time:61.5925 s\n",
      "[epoch 8819]: training loss: 900.450317, consuming time:61.3309 s\n",
      "[epoch 8820]: training loss: 916.223877, consuming time:61.6764 s\n",
      "[epoch 8821]: training loss: 616.418579, consuming time:61.7860 s\n",
      "[epoch 8822]: training loss: 838.536316, consuming time:61.7708 s\n",
      "[epoch 8823]: training loss: 814.764893, consuming time:61.6809 s\n",
      "[epoch 8824]: training loss: 963.795410, consuming time:61.6388 s\n",
      "[epoch 8825]: training loss: 1174.083496, consuming time:61.5936 s\n",
      "[epoch 8826]: training loss: 834.411377, consuming time:61.5958 s\n",
      "[epoch 8827]: training loss: 1151.893066, consuming time:61.5584 s\n",
      "[epoch 8828]: training loss: 711.730347, consuming time:61.8292 s\n",
      "[epoch 8829]: training loss: 927.446228, consuming time:61.7878 s\n",
      "[epoch 8830]: training loss: 694.451904, consuming time:61.8216 s\n",
      "[epoch 8831]: training loss: 823.384155, consuming time:61.6179 s\n",
      "[epoch 8832]: training loss: 994.972900, consuming time:61.7263 s\n",
      "[epoch 8833]: training loss: 748.988892, consuming time:61.7956 s\n",
      "[epoch 8834]: training loss: 1156.135132, consuming time:61.7523 s\n",
      "[epoch 8835]: training loss: 815.907104, consuming time:61.8603 s\n",
      "[epoch 8836]: training loss: 837.186157, consuming time:61.8861 s\n",
      "[epoch 8837]: training loss: 1082.682373, consuming time:61.6101 s\n",
      "[epoch 8838]: training loss: 828.478638, consuming time:61.7792 s\n",
      "[epoch 8839]: training loss: 1053.324951, consuming time:61.7881 s\n",
      "[epoch 8840]: training loss: 886.340576, consuming time:61.8095 s\n",
      "[epoch 8841]: training loss: 1037.966064, consuming time:61.6731 s\n",
      "[epoch 8842]: training loss: 686.768188, consuming time:61.6199 s\n",
      "[epoch 8843]: training loss: 1081.757812, consuming time:61.7224 s\n",
      "[epoch 8844]: training loss: 683.466064, consuming time:61.6763 s\n",
      "[epoch 8845]: training loss: 804.999634, consuming time:61.8527 s\n",
      "[epoch 8846]: training loss: 972.121582, consuming time:61.7016 s\n",
      "[epoch 8847]: training loss: 994.062256, consuming time:61.7845 s\n",
      "[epoch 8848]: training loss: 1417.111328, consuming time:61.7899 s\n",
      "[epoch 8849]: training loss: 670.035400, consuming time:61.7661 s\n",
      "[epoch 8850]: training loss: 799.850708, consuming time:61.7415 s\n",
      "[epoch 8851]: training loss: 947.148926, consuming time:61.6284 s\n",
      "[epoch 8852]: training loss: 848.991638, consuming time:61.8203 s\n",
      "[epoch 8853]: training loss: 1005.980286, consuming time:61.7863 s\n",
      "[epoch 8854]: training loss: 932.593750, consuming time:61.6646 s\n",
      "[epoch 8855]: training loss: 1097.798218, consuming time:61.4981 s\n",
      "[epoch 8856]: training loss: 782.182922, consuming time:61.7802 s\n",
      "[epoch 8857]: training loss: 1048.354858, consuming time:61.6789 s\n",
      "[epoch 8858]: training loss: 1035.286621, consuming time:61.6715 s\n",
      "[epoch 8859]: training loss: 774.414551, consuming time:61.7995 s\n",
      "[epoch 8860]: training loss: 983.842285, consuming time:61.5764 s\n",
      "[epoch 8861]: training loss: 711.395996, consuming time:61.7031 s\n",
      "[epoch 8862]: training loss: 758.687378, consuming time:61.8306 s\n",
      "[epoch 8863]: training loss: 914.952637, consuming time:61.6453 s\n",
      "[epoch 8864]: training loss: 1047.453613, consuming time:61.7575 s\n",
      "[epoch 8865]: training loss: 1040.406738, consuming time:61.6673 s\n",
      "[epoch 8866]: training loss: 914.996155, consuming time:61.5588 s\n",
      "[epoch 8867]: training loss: 925.987854, consuming time:61.8648 s\n",
      "[epoch 8868]: training loss: 584.297363, consuming time:61.6278 s\n",
      "[epoch 8869]: training loss: 1079.753540, consuming time:61.7960 s\n",
      "[epoch 8870]: training loss: 779.372559, consuming time:61.6537 s\n",
      "[epoch 8871]: training loss: 917.886230, consuming time:61.8152 s\n",
      "[epoch 8872]: training loss: 869.644348, consuming time:61.5363 s\n",
      "[epoch 8873]: training loss: 683.955322, consuming time:61.7329 s\n",
      "[epoch 8874]: training loss: 903.504272, consuming time:61.3528 s\n",
      "[epoch 8875]: training loss: 810.531494, consuming time:61.7467 s\n",
      "[epoch 8876]: training loss: 969.616943, consuming time:61.6359 s\n",
      "[epoch 8877]: training loss: 776.414062, consuming time:61.5796 s\n",
      "[epoch 8878]: training loss: 915.785217, consuming time:61.5101 s\n",
      "[epoch 8879]: training loss: 793.546143, consuming time:61.4738 s\n",
      "[epoch 8880]: training loss: 1113.624756, consuming time:61.8215 s\n",
      "[epoch 8881]: training loss: 1168.727539, consuming time:61.7372 s\n",
      "[epoch 8882]: training loss: 862.727051, consuming time:61.6932 s\n",
      "[epoch 8883]: training loss: 987.527100, consuming time:61.5868 s\n",
      "[epoch 8884]: training loss: 758.824341, consuming time:61.5527 s\n",
      "[epoch 8885]: training loss: 960.870789, consuming time:61.5248 s\n",
      "[epoch 8886]: training loss: 1049.888550, consuming time:61.6240 s\n",
      "[epoch 8887]: training loss: 875.647888, consuming time:61.5947 s\n",
      "[epoch 8888]: training loss: 997.504395, consuming time:61.5706 s\n",
      "[epoch 8889]: training loss: 905.029602, consuming time:61.5940 s\n",
      "[epoch 8890]: training loss: 793.687012, consuming time:61.6729 s\n",
      "[epoch 8891]: training loss: 941.988281, consuming time:61.6646 s\n",
      "[epoch 8892]: training loss: 950.238831, consuming time:61.6928 s\n",
      "[epoch 8893]: training loss: 914.539978, consuming time:61.7800 s\n",
      "[epoch 8894]: training loss: 737.401306, consuming time:61.7465 s\n",
      "[epoch 8895]: training loss: 881.606018, consuming time:61.5914 s\n",
      "[epoch 8896]: training loss: 994.123901, consuming time:61.6024 s\n",
      "[epoch 8897]: training loss: 1122.659912, consuming time:61.7112 s\n",
      "[epoch 8898]: training loss: 949.547546, consuming time:61.4009 s\n",
      "[epoch 8899]: training loss: 820.684326, consuming time:61.5974 s\n",
      "[epoch 8900]: training loss: 843.009521, consuming time:61.7086 s\n",
      "[epoch 8901]: training loss: 806.682861, consuming time:61.5671 s\n",
      "[epoch 8902]: training loss: 889.253662, consuming time:61.6697 s\n",
      "[epoch 8903]: training loss: 1155.206543, consuming time:61.6435 s\n",
      "[epoch 8904]: training loss: 1059.963379, consuming time:61.4389 s\n",
      "[epoch 8905]: training loss: 949.523254, consuming time:61.5229 s\n",
      "[epoch 8906]: training loss: 1386.445312, consuming time:61.5730 s\n",
      "[epoch 8907]: training loss: 926.376587, consuming time:61.4384 s\n",
      "[epoch 8908]: training loss: 1189.762817, consuming time:61.6367 s\n",
      "[epoch 8909]: training loss: 963.207031, consuming time:61.6548 s\n",
      "[epoch 8910]: training loss: 887.820557, consuming time:61.7793 s\n",
      "[epoch 8911]: training loss: 769.644897, consuming time:61.6477 s\n",
      "[epoch 8912]: training loss: 938.965942, consuming time:61.4416 s\n",
      "[epoch 8913]: training loss: 760.831238, consuming time:61.6585 s\n",
      "[epoch 8914]: training loss: 934.198975, consuming time:61.5477 s\n",
      "[epoch 8915]: training loss: 1076.991333, consuming time:61.6432 s\n",
      "[epoch 8916]: training loss: 1014.002991, consuming time:61.4019 s\n",
      "[epoch 8917]: training loss: 1049.090576, consuming time:61.7596 s\n",
      "[epoch 8918]: training loss: 709.526733, consuming time:61.5612 s\n",
      "[epoch 8919]: training loss: 769.034302, consuming time:61.4804 s\n",
      "[epoch 8920]: training loss: 1063.227417, consuming time:61.5906 s\n",
      "[epoch 8921]: training loss: 968.201843, consuming time:61.6398 s\n",
      "[epoch 8922]: training loss: 784.040894, consuming time:61.6494 s\n",
      "[epoch 8923]: training loss: 639.084229, consuming time:61.6988 s\n",
      "[epoch 8924]: training loss: 790.544922, consuming time:61.5307 s\n",
      "[epoch 8925]: training loss: 804.415833, consuming time:61.5538 s\n",
      "[epoch 8926]: training loss: 582.707031, consuming time:61.5450 s\n",
      "[epoch 8927]: training loss: 926.125854, consuming time:61.5846 s\n",
      "[epoch 8928]: training loss: 907.062561, consuming time:61.6217 s\n",
      "[epoch 8929]: training loss: 1124.178711, consuming time:61.6822 s\n",
      "[epoch 8930]: training loss: 1004.454712, consuming time:61.5470 s\n",
      "[epoch 8931]: training loss: 953.987061, consuming time:61.7896 s\n",
      "[epoch 8932]: training loss: 955.484619, consuming time:61.6430 s\n",
      "[epoch 8933]: training loss: 1003.555847, consuming time:61.5498 s\n",
      "[epoch 8934]: training loss: 1119.947266, consuming time:61.5032 s\n",
      "[epoch 8935]: training loss: 897.966125, consuming time:61.5289 s\n",
      "[epoch 8936]: training loss: 957.525696, consuming time:61.4633 s\n",
      "[epoch 8937]: training loss: 968.801453, consuming time:61.5941 s\n",
      "[epoch 8938]: training loss: 638.240479, consuming time:61.6275 s\n",
      "[epoch 8939]: training loss: 735.606506, consuming time:61.5761 s\n",
      "[epoch 8940]: training loss: 1017.342651, consuming time:61.6483 s\n",
      "[epoch 8941]: training loss: 575.141785, consuming time:61.5777 s\n",
      "[epoch 8942]: training loss: 896.094482, consuming time:61.6158 s\n",
      "[epoch 8943]: training loss: 1394.813477, consuming time:61.3255 s\n",
      "[epoch 8944]: training loss: 1024.145874, consuming time:61.4041 s\n",
      "[epoch 8945]: training loss: 1022.609985, consuming time:61.5467 s\n",
      "[epoch 8946]: training loss: 759.303101, consuming time:61.6357 s\n",
      "[epoch 8947]: training loss: 949.902344, consuming time:61.7626 s\n",
      "[epoch 8948]: training loss: 773.761475, consuming time:61.6222 s\n",
      "[epoch 8949]: training loss: 822.478149, consuming time:61.6514 s\n",
      "[epoch 8950]: training loss: 818.492493, consuming time:61.5632 s\n",
      "[epoch 8951]: training loss: 1103.654663, consuming time:61.6838 s\n",
      "[epoch 8952]: training loss: 870.414062, consuming time:61.5585 s\n",
      "[epoch 8953]: training loss: 882.186523, consuming time:61.8162 s\n",
      "[epoch 8954]: training loss: 1019.903198, consuming time:61.6073 s\n",
      "[epoch 8955]: training loss: 934.719971, consuming time:61.5862 s\n",
      "[epoch 8956]: training loss: 972.604004, consuming time:61.7442 s\n",
      "[epoch 8957]: training loss: 950.718872, consuming time:61.7102 s\n",
      "[epoch 8958]: training loss: 772.109619, consuming time:61.9044 s\n",
      "[epoch 8959]: training loss: 1168.137085, consuming time:61.6106 s\n",
      "[epoch 8960]: training loss: 815.589966, consuming time:61.5618 s\n",
      "[epoch 8961]: training loss: 975.072021, consuming time:61.4928 s\n",
      "[epoch 8962]: training loss: 1157.001465, consuming time:61.6015 s\n",
      "[epoch 8963]: training loss: 553.300842, consuming time:61.7877 s\n",
      "[epoch 8964]: training loss: 837.654053, consuming time:61.5520 s\n",
      "[epoch 8965]: training loss: 927.330811, consuming time:61.3837 s\n",
      "[epoch 8966]: training loss: 779.056885, consuming time:61.7857 s\n",
      "[epoch 8967]: training loss: 1030.022583, consuming time:61.5657 s\n",
      "[epoch 8968]: training loss: 777.286133, consuming time:61.7518 s\n",
      "[epoch 8969]: training loss: 865.541504, consuming time:61.4217 s\n",
      "[epoch 8970]: training loss: 856.345947, consuming time:61.7000 s\n",
      "[epoch 8971]: training loss: 900.052490, consuming time:61.4988 s\n",
      "[epoch 8972]: training loss: 1054.157227, consuming time:61.5284 s\n",
      "[epoch 8973]: training loss: 1109.625977, consuming time:61.6326 s\n",
      "[epoch 8974]: training loss: 768.749146, consuming time:61.7420 s\n",
      "[epoch 8975]: training loss: 974.895447, consuming time:61.6379 s\n",
      "[epoch 8976]: training loss: 1125.829224, consuming time:61.7352 s\n",
      "[epoch 8977]: training loss: 983.994202, consuming time:61.5057 s\n",
      "[epoch 8978]: training loss: 899.655640, consuming time:61.2941 s\n",
      "[epoch 8979]: training loss: 825.122925, consuming time:61.6095 s\n",
      "[epoch 8980]: training loss: 1033.263672, consuming time:61.6101 s\n",
      "[epoch 8981]: training loss: 869.891113, consuming time:61.5928 s\n",
      "[epoch 8982]: training loss: 1038.287964, consuming time:61.7107 s\n",
      "[epoch 8983]: training loss: 881.645996, consuming time:61.5578 s\n",
      "[epoch 8984]: training loss: 1156.661255, consuming time:61.5401 s\n",
      "[epoch 8985]: training loss: 897.196777, consuming time:61.5933 s\n",
      "[epoch 8986]: training loss: 972.607300, consuming time:65.1138 s\n",
      "[epoch 8987]: training loss: 1016.198486, consuming time:61.4831 s\n",
      "[epoch 8988]: training loss: 847.940063, consuming time:61.6566 s\n",
      "[epoch 8989]: training loss: 986.375061, consuming time:61.4712 s\n",
      "[epoch 8990]: training loss: 881.436523, consuming time:61.5752 s\n",
      "[epoch 8991]: training loss: 904.151978, consuming time:61.5853 s\n",
      "[epoch 8992]: training loss: 904.973816, consuming time:61.6076 s\n",
      "[epoch 8993]: training loss: 1045.215332, consuming time:61.6101 s\n",
      "[epoch 8994]: training loss: 912.082153, consuming time:61.6043 s\n",
      "[epoch 8995]: training loss: 1167.119873, consuming time:61.5149 s\n",
      "[epoch 8996]: training loss: 727.257446, consuming time:61.5030 s\n",
      "[epoch 8997]: training loss: 861.552490, consuming time:61.7793 s\n",
      "[epoch 8998]: training loss: 879.072754, consuming time:61.6158 s\n",
      "[epoch 8999]: training loss: 1010.871765, consuming time:61.5490 s\n",
      "[epoch 9000]: training loss: 1095.713379, consuming time:61.6224 s\n",
      "[epoch 9001]: training loss: 961.519043, consuming time:61.4935 s\n",
      "[epoch 9002]: training loss: 1091.280029, consuming time:61.5689 s\n",
      "[epoch 9003]: training loss: 780.762695, consuming time:61.6044 s\n",
      "[epoch 9004]: training loss: 746.385010, consuming time:61.6549 s\n",
      "[epoch 9005]: training loss: 886.427917, consuming time:61.5633 s\n",
      "[epoch 9006]: training loss: 974.828796, consuming time:61.5678 s\n",
      "[epoch 9007]: training loss: 1043.081787, consuming time:61.5217 s\n",
      "[epoch 9008]: training loss: 1092.562744, consuming time:61.6234 s\n",
      "[epoch 9009]: training loss: 761.410034, consuming time:61.6022 s\n",
      "[epoch 9010]: training loss: 784.084351, consuming time:61.6407 s\n",
      "[epoch 9011]: training loss: 810.877441, consuming time:61.5511 s\n",
      "[epoch 9012]: training loss: 735.191406, consuming time:61.5990 s\n",
      "[epoch 9013]: training loss: 1022.753845, consuming time:61.6038 s\n",
      "[epoch 9014]: training loss: 978.570923, consuming time:61.6900 s\n",
      "[epoch 9015]: training loss: 999.381348, consuming time:61.5732 s\n",
      "[epoch 9016]: training loss: 840.980713, consuming time:61.6200 s\n",
      "[epoch 9017]: training loss: 746.492432, consuming time:61.6863 s\n",
      "[epoch 9018]: training loss: 858.859924, consuming time:61.5987 s\n",
      "[epoch 9019]: training loss: 966.426208, consuming time:61.3603 s\n",
      "[epoch 9020]: training loss: 805.790283, consuming time:61.7173 s\n",
      "[epoch 9021]: training loss: 856.989136, consuming time:61.6739 s\n",
      "[epoch 9022]: training loss: 986.811768, consuming time:61.7255 s\n",
      "[epoch 9023]: training loss: 873.007385, consuming time:61.5683 s\n",
      "[epoch 9024]: training loss: 1008.802002, consuming time:61.6173 s\n",
      "[epoch 9025]: training loss: 758.129272, consuming time:61.5607 s\n",
      "[epoch 9026]: training loss: 924.442383, consuming time:61.6933 s\n",
      "[epoch 9027]: training loss: 811.150513, consuming time:61.5855 s\n",
      "[epoch 9028]: training loss: 1242.150879, consuming time:61.5838 s\n",
      "[epoch 9029]: training loss: 876.473022, consuming time:61.6419 s\n",
      "[epoch 9030]: training loss: 984.541260, consuming time:61.7952 s\n",
      "[epoch 9031]: training loss: 932.249023, consuming time:61.5753 s\n",
      "[epoch 9032]: training loss: 843.822449, consuming time:61.6600 s\n",
      "[epoch 9033]: training loss: 747.034790, consuming time:61.5651 s\n",
      "[epoch 9034]: training loss: 743.507935, consuming time:61.8128 s\n",
      "[epoch 9035]: training loss: 851.320312, consuming time:61.8626 s\n",
      "[epoch 9036]: training loss: 647.744751, consuming time:61.6296 s\n",
      "[epoch 9037]: training loss: 895.186035, consuming time:61.6668 s\n",
      "[epoch 9038]: training loss: 690.411499, consuming time:61.7915 s\n",
      "[epoch 9039]: training loss: 839.446289, consuming time:61.6774 s\n",
      "[epoch 9040]: training loss: 880.222534, consuming time:61.6580 s\n",
      "[epoch 9041]: training loss: 919.093018, consuming time:61.6629 s\n",
      "[epoch 9042]: training loss: 1334.774658, consuming time:61.6801 s\n",
      "[epoch 9043]: training loss: 1065.193848, consuming time:61.7722 s\n",
      "[epoch 9044]: training loss: 864.111694, consuming time:61.4136 s\n",
      "[epoch 9045]: training loss: 934.706482, consuming time:61.6719 s\n",
      "[epoch 9046]: training loss: 1057.477173, consuming time:61.5727 s\n",
      "[epoch 9047]: training loss: 853.627319, consuming time:61.3664 s\n",
      "[epoch 9048]: training loss: 838.543213, consuming time:61.5614 s\n",
      "[epoch 9049]: training loss: 812.954590, consuming time:61.6123 s\n",
      "[epoch 9050]: training loss: 854.161926, consuming time:61.5646 s\n",
      "[epoch 9051]: training loss: 851.578003, consuming time:61.6249 s\n",
      "[epoch 9052]: training loss: 775.707275, consuming time:61.6392 s\n",
      "[epoch 9053]: training loss: 988.109009, consuming time:61.6496 s\n",
      "[epoch 9054]: training loss: 1289.626709, consuming time:61.4564 s\n",
      "[epoch 9055]: training loss: 870.485840, consuming time:61.7393 s\n",
      "[epoch 9056]: training loss: 1247.819824, consuming time:61.4732 s\n",
      "[epoch 9057]: training loss: 982.450928, consuming time:61.6359 s\n",
      "[epoch 9058]: training loss: 1205.578979, consuming time:61.6109 s\n",
      "[epoch 9059]: training loss: 959.759949, consuming time:61.5794 s\n",
      "[epoch 9060]: training loss: 768.729614, consuming time:61.4588 s\n",
      "[epoch 9061]: training loss: 1073.228271, consuming time:61.7124 s\n",
      "[epoch 9062]: training loss: 1087.859497, consuming time:61.6210 s\n",
      "[epoch 9063]: training loss: 1391.385742, consuming time:61.7045 s\n",
      "[epoch 9064]: training loss: 1018.723755, consuming time:61.6207 s\n",
      "[epoch 9065]: training loss: 981.080505, consuming time:61.6931 s\n",
      "[epoch 9066]: training loss: 1103.703247, consuming time:61.6203 s\n",
      "[epoch 9067]: training loss: 917.317871, consuming time:61.6055 s\n",
      "[epoch 9068]: training loss: 995.273926, consuming time:61.8445 s\n",
      "[epoch 9069]: training loss: 853.424744, consuming time:61.9051 s\n",
      "[epoch 9070]: training loss: 1167.123657, consuming time:61.6982 s\n",
      "[epoch 9071]: training loss: 662.742065, consuming time:61.8190 s\n",
      "[epoch 9072]: training loss: 977.628052, consuming time:61.6218 s\n",
      "[epoch 9073]: training loss: 1047.089600, consuming time:61.7566 s\n",
      "[epoch 9074]: training loss: 793.711670, consuming time:61.7049 s\n",
      "[epoch 9075]: training loss: 764.775757, consuming time:61.8310 s\n",
      "[epoch 9076]: training loss: 923.397766, consuming time:61.7338 s\n",
      "[epoch 9077]: training loss: 1135.358276, consuming time:61.7084 s\n",
      "[epoch 9078]: training loss: 966.575928, consuming time:61.7054 s\n",
      "[epoch 9079]: training loss: 938.385498, consuming time:61.5896 s\n",
      "[epoch 9080]: training loss: 1071.052979, consuming time:61.7337 s\n",
      "[epoch 9081]: training loss: 825.373779, consuming time:61.6422 s\n",
      "[epoch 9082]: training loss: 887.531555, consuming time:61.6284 s\n",
      "[epoch 9083]: training loss: 901.340454, consuming time:61.5909 s\n",
      "[epoch 9084]: training loss: 670.436646, consuming time:61.7315 s\n",
      "[epoch 9085]: training loss: 970.507568, consuming time:61.6009 s\n",
      "[epoch 9086]: training loss: 1013.983765, consuming time:61.6577 s\n",
      "[epoch 9087]: training loss: 821.184021, consuming time:61.4828 s\n",
      "[epoch 9088]: training loss: 907.318359, consuming time:61.7274 s\n",
      "[epoch 9089]: training loss: 916.924072, consuming time:61.3451 s\n",
      "[epoch 9090]: training loss: 950.340393, consuming time:61.5335 s\n",
      "[epoch 9091]: training loss: 1060.395874, consuming time:61.6897 s\n",
      "[epoch 9092]: training loss: 1036.980103, consuming time:61.6044 s\n",
      "[epoch 9093]: training loss: 849.001709, consuming time:61.6615 s\n",
      "[epoch 9094]: training loss: 634.117371, consuming time:61.7516 s\n",
      "[epoch 9095]: training loss: 839.096069, consuming time:61.5587 s\n",
      "[epoch 9096]: training loss: 800.826965, consuming time:61.6650 s\n",
      "[epoch 9097]: training loss: 1069.023926, consuming time:61.6333 s\n",
      "[epoch 9098]: training loss: 739.249207, consuming time:61.6011 s\n",
      "[epoch 9099]: training loss: 919.266113, consuming time:61.8762 s\n",
      "[epoch 9100]: training loss: 829.323059, consuming time:61.6394 s\n",
      "[epoch 9101]: training loss: 707.623596, consuming time:61.5654 s\n",
      "[epoch 9102]: training loss: 1003.390625, consuming time:61.9004 s\n",
      "[epoch 9103]: training loss: 809.288574, consuming time:61.7652 s\n",
      "[epoch 9104]: training loss: 920.881226, consuming time:61.6851 s\n",
      "[epoch 9105]: training loss: 664.542969, consuming time:61.6021 s\n",
      "[epoch 9106]: training loss: 913.522156, consuming time:61.7523 s\n",
      "[epoch 9107]: training loss: 1082.288574, consuming time:61.4532 s\n",
      "[epoch 9108]: training loss: 1232.347046, consuming time:61.7645 s\n",
      "[epoch 9109]: training loss: 765.698608, consuming time:61.7111 s\n",
      "[epoch 9110]: training loss: 974.765503, consuming time:61.5245 s\n",
      "[epoch 9111]: training loss: 740.194885, consuming time:61.6561 s\n",
      "[epoch 9112]: training loss: 1014.972534, consuming time:61.8035 s\n",
      "[epoch 9113]: training loss: 1117.626099, consuming time:61.7587 s\n",
      "[epoch 9114]: training loss: 930.872559, consuming time:61.6391 s\n",
      "[epoch 9115]: training loss: 897.352417, consuming time:61.7982 s\n",
      "[epoch 9116]: training loss: 694.940674, consuming time:61.7267 s\n",
      "[epoch 9117]: training loss: 1016.686523, consuming time:61.8994 s\n",
      "[epoch 9118]: training loss: 779.249634, consuming time:61.5188 s\n",
      "[epoch 9119]: training loss: 949.054504, consuming time:61.6563 s\n",
      "[epoch 9120]: training loss: 677.718384, consuming time:61.6517 s\n",
      "[epoch 9121]: training loss: 671.696777, consuming time:61.9242 s\n",
      "[epoch 9122]: training loss: 1030.739990, consuming time:61.9042 s\n",
      "[epoch 9123]: training loss: 906.480347, consuming time:61.5974 s\n",
      "[epoch 9124]: training loss: 1045.674194, consuming time:61.6709 s\n",
      "[epoch 9125]: training loss: 877.096436, consuming time:61.7914 s\n",
      "[epoch 9126]: training loss: 832.506104, consuming time:61.5674 s\n",
      "[epoch 9127]: training loss: 945.741150, consuming time:61.7413 s\n",
      "[epoch 9128]: training loss: 794.562988, consuming time:61.6864 s\n",
      "[epoch 9129]: training loss: 952.432983, consuming time:61.6078 s\n",
      "[epoch 9130]: training loss: 751.506104, consuming time:61.6562 s\n",
      "[epoch 9131]: training loss: 1124.656494, consuming time:61.6747 s\n",
      "[epoch 9132]: training loss: 779.275879, consuming time:61.5239 s\n",
      "[epoch 9133]: training loss: 847.788696, consuming time:61.4335 s\n",
      "[epoch 9134]: training loss: 777.252686, consuming time:61.5881 s\n",
      "[epoch 9135]: training loss: 1208.246582, consuming time:61.5924 s\n",
      "[epoch 9136]: training loss: 830.856323, consuming time:61.6556 s\n",
      "[epoch 9137]: training loss: 966.149658, consuming time:61.6431 s\n",
      "[epoch 9138]: training loss: 913.231628, consuming time:61.7426 s\n",
      "[epoch 9139]: training loss: 792.823975, consuming time:61.7380 s\n",
      "[epoch 9140]: training loss: 887.608521, consuming time:61.8377 s\n",
      "[epoch 9141]: training loss: 807.010559, consuming time:61.7236 s\n",
      "[epoch 9142]: training loss: 1039.189209, consuming time:61.7681 s\n",
      "[epoch 9143]: training loss: 1063.795654, consuming time:61.6830 s\n",
      "[epoch 9144]: training loss: 910.676514, consuming time:61.7669 s\n",
      "[epoch 9145]: training loss: 883.534424, consuming time:61.6691 s\n",
      "[epoch 9146]: training loss: 1008.737122, consuming time:61.5304 s\n",
      "[epoch 9147]: training loss: 1061.178223, consuming time:61.7103 s\n",
      "[epoch 9148]: training loss: 942.550354, consuming time:61.6481 s\n",
      "[epoch 9149]: training loss: 838.310608, consuming time:62.0181 s\n",
      "[epoch 9150]: training loss: 942.698425, consuming time:61.7267 s\n",
      "[epoch 9151]: training loss: 1046.042236, consuming time:61.6838 s\n",
      "[epoch 9152]: training loss: 1136.803711, consuming time:61.5061 s\n",
      "[epoch 9153]: training loss: 865.000488, consuming time:61.5920 s\n",
      "[epoch 9154]: training loss: 829.762329, consuming time:61.6950 s\n",
      "[epoch 9155]: training loss: 889.016846, consuming time:61.8019 s\n",
      "[epoch 9156]: training loss: 777.309082, consuming time:61.7237 s\n",
      "[epoch 9157]: training loss: 1169.280273, consuming time:61.7073 s\n",
      "[epoch 9158]: training loss: 869.887817, consuming time:61.8532 s\n",
      "[epoch 9159]: training loss: 949.240601, consuming time:61.6679 s\n",
      "[epoch 9160]: training loss: 1055.458130, consuming time:61.6936 s\n",
      "[epoch 9161]: training loss: 1001.832886, consuming time:61.7220 s\n",
      "[epoch 9162]: training loss: 838.561340, consuming time:61.6315 s\n",
      "[epoch 9163]: training loss: 921.374939, consuming time:61.6887 s\n",
      "[epoch 9164]: training loss: 963.003113, consuming time:61.7272 s\n",
      "[epoch 9165]: training loss: 650.690796, consuming time:61.7236 s\n",
      "[epoch 9166]: training loss: 991.574951, consuming time:61.5969 s\n",
      "[epoch 9167]: training loss: 1275.461548, consuming time:61.6370 s\n",
      "[epoch 9168]: training loss: 853.751465, consuming time:61.8879 s\n",
      "[epoch 9169]: training loss: 864.791748, consuming time:61.6011 s\n",
      "[epoch 9170]: training loss: 969.624268, consuming time:61.6423 s\n",
      "[epoch 9171]: training loss: 1081.532471, consuming time:61.4457 s\n",
      "[epoch 9172]: training loss: 917.501404, consuming time:61.5056 s\n",
      "[epoch 9173]: training loss: 1102.655518, consuming time:61.6045 s\n",
      "[epoch 9174]: training loss: 970.942261, consuming time:61.6776 s\n",
      "[epoch 9175]: training loss: 866.622925, consuming time:61.5693 s\n",
      "[epoch 9176]: training loss: 898.319763, consuming time:61.5400 s\n",
      "[epoch 9177]: training loss: 626.673096, consuming time:61.6595 s\n",
      "[epoch 9178]: training loss: 1327.891357, consuming time:61.8513 s\n",
      "[epoch 9179]: training loss: 973.084473, consuming time:61.7896 s\n",
      "[epoch 9180]: training loss: 825.044556, consuming time:61.8371 s\n",
      "[epoch 9181]: training loss: 873.680359, consuming time:61.6015 s\n",
      "[epoch 9182]: training loss: 897.419434, consuming time:61.5722 s\n",
      "[epoch 9183]: training loss: 768.231018, consuming time:61.5310 s\n",
      "[epoch 9184]: training loss: 1041.200684, consuming time:61.7461 s\n",
      "[epoch 9185]: training loss: 873.617798, consuming time:61.7834 s\n",
      "[epoch 9186]: training loss: 739.185913, consuming time:61.7546 s\n",
      "[epoch 9187]: training loss: 889.914673, consuming time:61.6566 s\n",
      "[epoch 9188]: training loss: 912.729492, consuming time:61.6440 s\n",
      "[epoch 9189]: training loss: 943.684509, consuming time:61.7237 s\n",
      "[epoch 9190]: training loss: 1427.875000, consuming time:61.7547 s\n",
      "[epoch 9191]: training loss: 866.323547, consuming time:61.8471 s\n",
      "[epoch 9192]: training loss: 789.082886, consuming time:61.9017 s\n",
      "[epoch 9193]: training loss: 959.670166, consuming time:61.5331 s\n",
      "[epoch 9194]: training loss: 1053.176514, consuming time:61.9383 s\n",
      "[epoch 9195]: training loss: 1123.450439, consuming time:61.7565 s\n",
      "[epoch 9196]: training loss: 1038.363037, consuming time:61.6628 s\n",
      "[epoch 9197]: training loss: 896.225098, consuming time:61.5011 s\n",
      "[epoch 9198]: training loss: 920.584106, consuming time:61.7993 s\n",
      "[epoch 9199]: training loss: 822.236328, consuming time:61.6829 s\n",
      "[epoch 9200]: training loss: 1054.014893, consuming time:61.9258 s\n",
      "[epoch 9201]: training loss: 837.252686, consuming time:61.7298 s\n",
      "[epoch 9202]: training loss: 1038.182129, consuming time:61.7927 s\n",
      "[epoch 9203]: training loss: 1117.027832, consuming time:61.7065 s\n",
      "[epoch 9204]: training loss: 844.368347, consuming time:61.6638 s\n",
      "[epoch 9205]: training loss: 839.027588, consuming time:61.6375 s\n",
      "[epoch 9206]: training loss: 1146.929199, consuming time:61.5313 s\n",
      "[epoch 9207]: training loss: 1182.926147, consuming time:61.4356 s\n",
      "[epoch 9208]: training loss: 824.587036, consuming time:61.7364 s\n",
      "[epoch 9209]: training loss: 1064.581787, consuming time:61.7278 s\n",
      "[epoch 9210]: training loss: 1032.613525, consuming time:61.6235 s\n",
      "[epoch 9211]: training loss: 814.007568, consuming time:61.5122 s\n",
      "[epoch 9212]: training loss: 952.606873, consuming time:61.5892 s\n",
      "[epoch 9213]: training loss: 936.299255, consuming time:61.6285 s\n",
      "[epoch 9214]: training loss: 939.450195, consuming time:61.8923 s\n",
      "[epoch 9215]: training loss: 857.294128, consuming time:61.6714 s\n",
      "[epoch 9216]: training loss: 909.613647, consuming time:61.3878 s\n",
      "[epoch 9217]: training loss: 1001.701355, consuming time:61.5690 s\n",
      "[epoch 9218]: training loss: 923.548950, consuming time:61.4501 s\n",
      "[epoch 9219]: training loss: 767.004272, consuming time:61.4594 s\n",
      "[epoch 9220]: training loss: 1049.077881, consuming time:61.6133 s\n",
      "[epoch 9221]: training loss: 796.500000, consuming time:61.4646 s\n",
      "[epoch 9222]: training loss: 944.912109, consuming time:61.5380 s\n",
      "[epoch 9223]: training loss: 1089.217529, consuming time:61.7190 s\n",
      "[epoch 9224]: training loss: 1163.136230, consuming time:61.3665 s\n",
      "[epoch 9225]: training loss: 1036.308838, consuming time:61.6543 s\n",
      "[epoch 9226]: training loss: 1096.611206, consuming time:61.7688 s\n",
      "[epoch 9227]: training loss: 1177.925537, consuming time:61.8288 s\n",
      "[epoch 9228]: training loss: 1273.185547, consuming time:61.6802 s\n",
      "[epoch 9229]: training loss: 603.057251, consuming time:61.6774 s\n",
      "[epoch 9230]: training loss: 1229.530762, consuming time:61.6755 s\n",
      "[epoch 9231]: training loss: 849.231140, consuming time:61.5450 s\n",
      "[epoch 9232]: training loss: 919.105469, consuming time:61.8328 s\n",
      "[epoch 9233]: training loss: 1075.566895, consuming time:61.7761 s\n",
      "[epoch 9234]: training loss: 937.586121, consuming time:61.6863 s\n",
      "[epoch 9235]: training loss: 913.362488, consuming time:61.5594 s\n",
      "[epoch 9236]: training loss: 810.652710, consuming time:61.6556 s\n",
      "[epoch 9237]: training loss: 721.996155, consuming time:61.5713 s\n",
      "[epoch 9238]: training loss: 875.138550, consuming time:61.6541 s\n",
      "[epoch 9239]: training loss: 1009.963623, consuming time:61.7124 s\n",
      "[epoch 9240]: training loss: 763.193848, consuming time:61.5005 s\n",
      "[epoch 9241]: training loss: 940.440918, consuming time:61.6399 s\n",
      "[epoch 9242]: training loss: 1012.720581, consuming time:61.6931 s\n",
      "[epoch 9243]: training loss: 935.817871, consuming time:61.7732 s\n",
      "[epoch 9244]: training loss: 900.783691, consuming time:61.7660 s\n",
      "[epoch 9245]: training loss: 1065.120117, consuming time:61.7208 s\n",
      "[epoch 9246]: training loss: 889.000183, consuming time:61.6877 s\n",
      "[epoch 9247]: training loss: 1199.167603, consuming time:61.8131 s\n",
      "[epoch 9248]: training loss: 1023.722473, consuming time:61.7661 s\n",
      "[epoch 9249]: training loss: 1108.860840, consuming time:61.7550 s\n",
      "[epoch 9250]: training loss: 706.675659, consuming time:61.8230 s\n",
      "[epoch 9251]: training loss: 827.864624, consuming time:61.6838 s\n",
      "[epoch 9252]: training loss: 901.746826, consuming time:61.8011 s\n",
      "[epoch 9253]: training loss: 697.671692, consuming time:61.6871 s\n",
      "[epoch 9254]: training loss: 898.468384, consuming time:61.8883 s\n",
      "[epoch 9255]: training loss: 963.396240, consuming time:61.7437 s\n",
      "[epoch 9256]: training loss: 933.045105, consuming time:61.8760 s\n",
      "[epoch 9257]: training loss: 804.560913, consuming time:61.5526 s\n",
      "[epoch 9258]: training loss: 940.657959, consuming time:61.6710 s\n",
      "[epoch 9259]: training loss: 1133.969360, consuming time:61.7887 s\n",
      "[epoch 9260]: training loss: 905.744812, consuming time:61.8389 s\n",
      "[epoch 9261]: training loss: 839.752625, consuming time:61.7144 s\n",
      "[epoch 9262]: training loss: 876.039185, consuming time:61.8698 s\n",
      "[epoch 9263]: training loss: 863.908691, consuming time:61.6049 s\n",
      "[epoch 9264]: training loss: 930.058594, consuming time:61.8009 s\n",
      "[epoch 9265]: training loss: 1146.805542, consuming time:61.5207 s\n",
      "[epoch 9266]: training loss: 1019.890259, consuming time:61.5963 s\n",
      "[epoch 9267]: training loss: 800.550354, consuming time:61.7945 s\n",
      "[epoch 9268]: training loss: 1034.496338, consuming time:61.8241 s\n",
      "[epoch 9269]: training loss: 882.144897, consuming time:61.7371 s\n",
      "[epoch 9270]: training loss: 980.419739, consuming time:61.5912 s\n",
      "[epoch 9271]: training loss: 938.596436, consuming time:61.5163 s\n",
      "[epoch 9272]: training loss: 1132.432251, consuming time:61.6765 s\n",
      "[epoch 9273]: training loss: 1066.544189, consuming time:61.7586 s\n",
      "[epoch 9274]: training loss: 842.633057, consuming time:61.8920 s\n",
      "[epoch 9275]: training loss: 957.376953, consuming time:61.6905 s\n",
      "[epoch 9276]: training loss: 1036.374756, consuming time:61.7705 s\n",
      "[epoch 9277]: training loss: 1045.878052, consuming time:61.8116 s\n",
      "[epoch 9278]: training loss: 812.247925, consuming time:61.9854 s\n",
      "[epoch 9279]: training loss: 1088.671997, consuming time:61.7051 s\n",
      "[epoch 9280]: training loss: 891.542908, consuming time:61.7511 s\n",
      "[epoch 9281]: training loss: 986.723145, consuming time:61.7791 s\n",
      "[epoch 9282]: training loss: 911.118835, consuming time:61.7824 s\n",
      "[epoch 9283]: training loss: 677.437988, consuming time:61.6459 s\n",
      "[epoch 9284]: training loss: 871.293091, consuming time:61.6707 s\n",
      "[epoch 9285]: training loss: 1026.772949, consuming time:61.6803 s\n",
      "[epoch 9286]: training loss: 653.506470, consuming time:61.7213 s\n",
      "[epoch 9287]: training loss: 1220.498047, consuming time:61.7792 s\n",
      "[epoch 9288]: training loss: 774.515015, consuming time:61.8359 s\n",
      "[epoch 9289]: training loss: 672.258423, consuming time:61.7449 s\n",
      "[epoch 9290]: training loss: 922.335510, consuming time:61.7535 s\n",
      "[epoch 9291]: training loss: 817.015503, consuming time:61.7301 s\n",
      "[epoch 9292]: training loss: 817.487183, consuming time:61.6987 s\n",
      "[epoch 9293]: training loss: 723.344849, consuming time:61.6170 s\n",
      "[epoch 9294]: training loss: 695.873779, consuming time:61.7041 s\n",
      "[epoch 9295]: training loss: 978.535645, consuming time:61.8065 s\n",
      "[epoch 9296]: training loss: 786.868103, consuming time:61.7858 s\n",
      "[epoch 9297]: training loss: 888.796387, consuming time:61.8111 s\n",
      "[epoch 9298]: training loss: 1037.159424, consuming time:61.7383 s\n",
      "[epoch 9299]: training loss: 868.292053, consuming time:61.7493 s\n",
      "[epoch 9300]: training loss: 910.135315, consuming time:61.7407 s\n",
      "[epoch 9301]: training loss: 939.334656, consuming time:61.8892 s\n",
      "[epoch 9302]: training loss: 721.978516, consuming time:61.7408 s\n",
      "[epoch 9303]: training loss: 938.595764, consuming time:61.7842 s\n",
      "[epoch 9304]: training loss: 1092.359009, consuming time:61.6571 s\n",
      "[epoch 9305]: training loss: 887.772705, consuming time:61.6027 s\n",
      "[epoch 9306]: training loss: 821.153931, consuming time:61.8706 s\n",
      "[epoch 9307]: training loss: 895.492310, consuming time:61.6886 s\n",
      "[epoch 9308]: training loss: 941.727173, consuming time:61.6489 s\n",
      "[epoch 9309]: training loss: 835.558289, consuming time:61.6611 s\n",
      "[epoch 9310]: training loss: 735.870667, consuming time:61.8761 s\n",
      "[epoch 9311]: training loss: 880.880371, consuming time:61.7167 s\n",
      "[epoch 9312]: training loss: 887.831421, consuming time:61.7796 s\n",
      "[epoch 9313]: training loss: 1088.017334, consuming time:61.6669 s\n",
      "[epoch 9314]: training loss: 857.660583, consuming time:61.7943 s\n",
      "[epoch 9315]: training loss: 1099.701782, consuming time:61.7932 s\n",
      "[epoch 9316]: training loss: 731.874939, consuming time:61.6398 s\n",
      "[epoch 9317]: training loss: 891.918884, consuming time:61.7737 s\n",
      "[epoch 9318]: training loss: 821.966309, consuming time:61.6833 s\n",
      "[epoch 9319]: training loss: 900.706482, consuming time:61.7067 s\n",
      "[epoch 9320]: training loss: 805.102905, consuming time:61.8160 s\n",
      "[epoch 9321]: training loss: 1079.794922, consuming time:61.8531 s\n",
      "[epoch 9322]: training loss: 898.478333, consuming time:61.6792 s\n",
      "[epoch 9323]: training loss: 732.055176, consuming time:61.6522 s\n",
      "[epoch 9324]: training loss: 978.263672, consuming time:61.8589 s\n",
      "[epoch 9325]: training loss: 953.244751, consuming time:61.8282 s\n",
      "[epoch 9326]: training loss: 929.859741, consuming time:61.8220 s\n",
      "[epoch 9327]: training loss: 896.108398, consuming time:61.7782 s\n",
      "[epoch 9328]: training loss: 1068.831421, consuming time:61.7394 s\n",
      "[epoch 9329]: training loss: 1068.696289, consuming time:61.7815 s\n",
      "[epoch 9330]: training loss: 862.752014, consuming time:61.7938 s\n",
      "[epoch 9331]: training loss: 759.221069, consuming time:61.6492 s\n",
      "[epoch 9332]: training loss: 816.306152, consuming time:61.9666 s\n",
      "[epoch 9333]: training loss: 988.927979, consuming time:61.7557 s\n",
      "[epoch 9334]: training loss: 1075.240234, consuming time:61.7717 s\n",
      "[epoch 9335]: training loss: 1001.283752, consuming time:61.8580 s\n",
      "[epoch 9336]: training loss: 729.258301, consuming time:61.8843 s\n",
      "[epoch 9337]: training loss: 969.193054, consuming time:61.9819 s\n",
      "[epoch 9338]: training loss: 923.203735, consuming time:61.9970 s\n",
      "[epoch 9339]: training loss: 1044.195679, consuming time:61.8204 s\n",
      "[epoch 9340]: training loss: 764.663940, consuming time:61.9077 s\n",
      "[epoch 9341]: training loss: 886.674683, consuming time:61.8345 s\n",
      "[epoch 9342]: training loss: 1057.319580, consuming time:61.8445 s\n",
      "[epoch 9343]: training loss: 1013.591553, consuming time:61.9525 s\n",
      "[epoch 9344]: training loss: 875.824463, consuming time:61.7589 s\n",
      "[epoch 9345]: training loss: 845.671753, consuming time:61.7918 s\n",
      "[epoch 9346]: training loss: 961.293335, consuming time:61.7658 s\n",
      "[epoch 9347]: training loss: 1018.477356, consuming time:61.8009 s\n",
      "[epoch 9348]: training loss: 1074.433716, consuming time:61.8320 s\n",
      "[epoch 9349]: training loss: 959.270996, consuming time:61.6833 s\n",
      "[epoch 9350]: training loss: 643.020081, consuming time:61.7211 s\n",
      "[epoch 9351]: training loss: 981.513000, consuming time:61.6300 s\n",
      "[epoch 9352]: training loss: 596.354980, consuming time:61.7456 s\n",
      "[epoch 9353]: training loss: 892.877441, consuming time:61.5278 s\n",
      "[epoch 9354]: training loss: 1041.003906, consuming time:61.6480 s\n",
      "[epoch 9355]: training loss: 803.037476, consuming time:61.7822 s\n",
      "[epoch 9356]: training loss: 1064.359253, consuming time:61.8044 s\n",
      "[epoch 9357]: training loss: 709.312256, consuming time:61.6873 s\n",
      "[epoch 9358]: training loss: 713.401001, consuming time:61.4110 s\n",
      "[epoch 9359]: training loss: 743.951843, consuming time:61.5976 s\n",
      "[epoch 9360]: training loss: 832.679443, consuming time:61.5402 s\n",
      "[epoch 9361]: training loss: 1084.292725, consuming time:61.8288 s\n",
      "[epoch 9362]: training loss: 1244.247314, consuming time:65.0187 s\n",
      "[epoch 9363]: training loss: 1059.251465, consuming time:62.6470 s\n",
      "[epoch 9364]: training loss: 905.765869, consuming time:63.2353 s\n",
      "[epoch 9365]: training loss: 1113.505981, consuming time:63.0310 s\n",
      "[epoch 9366]: training loss: 1168.375488, consuming time:62.8536 s\n",
      "[epoch 9367]: training loss: 844.753540, consuming time:63.1088 s\n",
      "[epoch 9368]: training loss: 938.503418, consuming time:63.1766 s\n",
      "[epoch 9369]: training loss: 773.341919, consuming time:63.0747 s\n",
      "[epoch 9370]: training loss: 964.672302, consuming time:63.1179 s\n",
      "[epoch 9371]: training loss: 1026.187378, consuming time:63.1006 s\n",
      "[epoch 9372]: training loss: 741.967896, consuming time:62.7440 s\n",
      "[epoch 9373]: training loss: 786.139404, consuming time:62.6872 s\n",
      "[epoch 9374]: training loss: 1033.126953, consuming time:67.3002 s\n",
      "[epoch 9375]: training loss: 835.601318, consuming time:63.0681 s\n",
      "[epoch 9376]: training loss: 613.243896, consuming time:63.0934 s\n",
      "[epoch 9377]: training loss: 906.776978, consuming time:62.9655 s\n",
      "[epoch 9378]: training loss: 1169.082275, consuming time:62.9745 s\n",
      "[epoch 9379]: training loss: 1021.311401, consuming time:62.9595 s\n",
      "[epoch 9380]: training loss: 918.654297, consuming time:62.5682 s\n",
      "[epoch 9381]: training loss: 903.394165, consuming time:62.9934 s\n",
      "[epoch 9382]: training loss: 929.674255, consuming time:63.1445 s\n",
      "[epoch 9383]: training loss: 1070.257324, consuming time:62.6512 s\n",
      "[epoch 9384]: training loss: 917.427002, consuming time:62.7430 s\n",
      "[epoch 9385]: training loss: 1092.622559, consuming time:62.8461 s\n",
      "[epoch 9386]: training loss: 922.249878, consuming time:62.6222 s\n",
      "[epoch 9387]: training loss: 1146.021484, consuming time:63.0701 s\n",
      "[epoch 9388]: training loss: 956.347717, consuming time:62.8744 s\n",
      "[epoch 9389]: training loss: 842.428467, consuming time:62.9769 s\n",
      "[epoch 9390]: training loss: 1289.898926, consuming time:63.0447 s\n",
      "[epoch 9391]: training loss: 1062.827148, consuming time:62.8714 s\n",
      "[epoch 9392]: training loss: 653.988953, consuming time:63.0344 s\n",
      "[epoch 9393]: training loss: 980.292847, consuming time:63.2096 s\n",
      "[epoch 9394]: training loss: 919.011963, consuming time:62.8579 s\n",
      "[epoch 9395]: training loss: 811.963867, consuming time:62.7109 s\n",
      "[epoch 9396]: training loss: 1058.815308, consuming time:63.0003 s\n",
      "[epoch 9397]: training loss: 1008.325195, consuming time:63.2191 s\n",
      "[epoch 9398]: training loss: 1131.056763, consuming time:62.9413 s\n",
      "[epoch 9399]: training loss: 870.249634, consuming time:62.9875 s\n",
      "[epoch 9400]: training loss: 823.501709, consuming time:63.0410 s\n",
      "[epoch 9401]: training loss: 1003.684937, consuming time:62.9909 s\n",
      "[epoch 9402]: training loss: 940.112549, consuming time:62.9866 s\n",
      "[epoch 9403]: training loss: 1003.210632, consuming time:62.8982 s\n",
      "[epoch 9404]: training loss: 762.215210, consuming time:62.9838 s\n",
      "[epoch 9405]: training loss: 690.946472, consuming time:62.9524 s\n",
      "[epoch 9406]: training loss: 955.510498, consuming time:62.8918 s\n",
      "[epoch 9407]: training loss: 957.175842, consuming time:63.1601 s\n",
      "[epoch 9408]: training loss: 898.019287, consuming time:62.9045 s\n",
      "[epoch 9409]: training loss: 1259.149292, consuming time:63.3870 s\n",
      "[epoch 9410]: training loss: 905.720032, consuming time:63.0247 s\n",
      "[epoch 9411]: training loss: 703.879028, consuming time:62.9664 s\n",
      "[epoch 9412]: training loss: 1250.917114, consuming time:63.1014 s\n",
      "[epoch 9413]: training loss: 710.493103, consuming time:62.7038 s\n",
      "[epoch 9414]: training loss: 808.858704, consuming time:63.0946 s\n",
      "[epoch 9415]: training loss: 843.596008, consuming time:63.0879 s\n",
      "[epoch 9416]: training loss: 1002.053955, consuming time:63.1764 s\n",
      "[epoch 9417]: training loss: 887.800354, consuming time:63.0621 s\n",
      "[epoch 9418]: training loss: 975.630737, consuming time:62.7696 s\n",
      "[epoch 9419]: training loss: 760.515015, consuming time:62.8002 s\n",
      "[epoch 9420]: training loss: 900.569885, consuming time:63.2468 s\n",
      "[epoch 9421]: training loss: 933.727051, consuming time:63.3546 s\n",
      "[epoch 9422]: training loss: 936.035767, consuming time:62.9560 s\n",
      "[epoch 9423]: training loss: 799.053955, consuming time:63.8326 s\n",
      "[epoch 9424]: training loss: 895.472778, consuming time:64.0052 s\n",
      "[epoch 9425]: training loss: 1004.270752, consuming time:66.5184 s\n",
      "[epoch 9426]: training loss: 821.869385, consuming time:63.2910 s\n",
      "[epoch 9427]: training loss: 923.455566, consuming time:62.8440 s\n",
      "[epoch 9428]: training loss: 1215.949341, consuming time:62.7056 s\n",
      "[epoch 9429]: training loss: 871.126160, consuming time:64.9982 s\n",
      "[epoch 9430]: training loss: 1176.715454, consuming time:62.3062 s\n",
      "[epoch 9431]: training loss: 1127.452881, consuming time:64.1386 s\n",
      "[epoch 9432]: training loss: 1090.794556, consuming time:63.1903 s\n",
      "[epoch 9433]: training loss: 862.471069, consuming time:61.6841 s\n",
      "[epoch 9434]: training loss: 1067.406128, consuming time:63.9996 s\n",
      "[epoch 9435]: training loss: 808.363525, consuming time:66.2328 s\n",
      "[epoch 9436]: training loss: 1006.663696, consuming time:65.2041 s\n",
      "[epoch 9437]: training loss: 868.377136, consuming time:65.8621 s\n",
      "[epoch 9438]: training loss: 973.219971, consuming time:66.1126 s\n",
      "[epoch 9439]: training loss: 767.856995, consuming time:64.8292 s\n",
      "[epoch 9440]: training loss: 759.614746, consuming time:68.8497 s\n",
      "[epoch 9441]: training loss: 762.858887, consuming time:61.6578 s\n",
      "[epoch 9442]: training loss: 1148.140259, consuming time:61.0648 s\n",
      "[epoch 9443]: training loss: 945.699219, consuming time:61.3618 s\n",
      "[epoch 9444]: training loss: 951.978882, consuming time:64.3739 s\n",
      "[epoch 9445]: training loss: 1018.608276, consuming time:67.4726 s\n",
      "[epoch 9446]: training loss: 1086.367920, consuming time:64.4470 s\n",
      "[epoch 9447]: training loss: 887.112671, consuming time:63.7698 s\n",
      "[epoch 9448]: training loss: 1075.293335, consuming time:67.2004 s\n",
      "[epoch 9449]: training loss: 1021.019409, consuming time:66.6904 s\n",
      "[epoch 9450]: training loss: 788.544312, consuming time:72.2346 s\n",
      "[epoch 9451]: training loss: 1039.682617, consuming time:66.1427 s\n",
      "[epoch 9452]: training loss: 898.040405, consuming time:64.5630 s\n",
      "[epoch 9453]: training loss: 851.583862, consuming time:66.6448 s\n",
      "[epoch 9454]: training loss: 833.866516, consuming time:62.3706 s\n",
      "[epoch 9455]: training loss: 1029.706055, consuming time:62.0800 s\n",
      "[epoch 9456]: training loss: 831.331421, consuming time:62.5766 s\n",
      "[epoch 9457]: training loss: 985.428955, consuming time:61.7508 s\n",
      "[epoch 9458]: training loss: 977.338135, consuming time:62.4676 s\n",
      "[epoch 9459]: training loss: 1118.748047, consuming time:61.9278 s\n",
      "[epoch 9460]: training loss: 664.177063, consuming time:62.5612 s\n",
      "[epoch 9461]: training loss: 896.971008, consuming time:62.9621 s\n",
      "[epoch 9462]: training loss: 972.457520, consuming time:62.7405 s\n",
      "[epoch 9463]: training loss: 841.011719, consuming time:63.1585 s\n",
      "[epoch 9464]: training loss: 1185.536133, consuming time:62.2178 s\n",
      "[epoch 9465]: training loss: 768.489441, consuming time:62.0413 s\n",
      "[epoch 9466]: training loss: 906.632690, consuming time:62.0712 s\n",
      "[epoch 9467]: training loss: 791.206055, consuming time:61.5994 s\n",
      "[epoch 9468]: training loss: 1036.546143, consuming time:61.4919 s\n",
      "[epoch 9469]: training loss: 1094.252197, consuming time:61.2272 s\n",
      "[epoch 9470]: training loss: 1026.539795, consuming time:61.4543 s\n",
      "[epoch 9471]: training loss: 990.616211, consuming time:61.6800 s\n",
      "[epoch 9472]: training loss: 733.223267, consuming time:61.5081 s\n",
      "[epoch 9473]: training loss: 962.491577, consuming time:61.3849 s\n",
      "[epoch 9474]: training loss: 906.574341, consuming time:61.2781 s\n",
      "[epoch 9475]: training loss: 837.098999, consuming time:61.3558 s\n",
      "[epoch 9476]: training loss: 1061.544678, consuming time:61.2150 s\n",
      "[epoch 9477]: training loss: 777.848206, consuming time:61.3790 s\n",
      "[epoch 9478]: training loss: 1140.513916, consuming time:61.2004 s\n",
      "[epoch 9479]: training loss: 813.320190, consuming time:61.2672 s\n",
      "[epoch 9480]: training loss: 789.945068, consuming time:61.2107 s\n",
      "[epoch 9481]: training loss: 936.627747, consuming time:61.1807 s\n",
      "[epoch 9482]: training loss: 979.191040, consuming time:61.3058 s\n",
      "[epoch 9483]: training loss: 911.162720, consuming time:61.1451 s\n",
      "[epoch 9484]: training loss: 733.181396, consuming time:61.3843 s\n",
      "[epoch 9485]: training loss: 898.557373, consuming time:61.4054 s\n",
      "[epoch 9486]: training loss: 712.658752, consuming time:61.6065 s\n",
      "[epoch 9487]: training loss: 746.644165, consuming time:61.4386 s\n",
      "[epoch 9488]: training loss: 822.042725, consuming time:61.4541 s\n",
      "[epoch 9489]: training loss: 974.780762, consuming time:61.2281 s\n",
      "[epoch 9490]: training loss: 894.184875, consuming time:61.3559 s\n",
      "[epoch 9491]: training loss: 640.615234, consuming time:61.5122 s\n",
      "[epoch 9492]: training loss: 1167.454224, consuming time:61.6881 s\n",
      "[epoch 9493]: training loss: 809.681396, consuming time:61.3299 s\n",
      "[epoch 9494]: training loss: 956.102539, consuming time:61.4451 s\n",
      "[epoch 9495]: training loss: 913.088928, consuming time:61.3686 s\n",
      "[epoch 9496]: training loss: 1006.583008, consuming time:61.5078 s\n",
      "[epoch 9497]: training loss: 777.736328, consuming time:61.5698 s\n",
      "[epoch 9498]: training loss: 1021.911743, consuming time:61.4442 s\n",
      "[epoch 9499]: training loss: 1059.006592, consuming time:61.4621 s\n",
      "[epoch 9500]: training loss: 971.906311, consuming time:61.3295 s\n",
      "[epoch 9501]: training loss: 776.232422, consuming time:61.1474 s\n",
      "[epoch 9502]: training loss: 809.131653, consuming time:61.3917 s\n",
      "[epoch 9503]: training loss: 919.130371, consuming time:61.4216 s\n",
      "[epoch 9504]: training loss: 739.749084, consuming time:61.2845 s\n",
      "[epoch 9505]: training loss: 682.960083, consuming time:61.3296 s\n",
      "[epoch 9506]: training loss: 985.475220, consuming time:61.4228 s\n",
      "[epoch 9507]: training loss: 851.731934, consuming time:61.3599 s\n",
      "[epoch 9508]: training loss: 762.957031, consuming time:61.2871 s\n",
      "[epoch 9509]: training loss: 1006.779907, consuming time:61.3674 s\n",
      "[epoch 9510]: training loss: 964.328125, consuming time:61.1434 s\n",
      "[epoch 9511]: training loss: 949.223633, consuming time:61.4938 s\n",
      "[epoch 9512]: training loss: 883.308594, consuming time:61.0716 s\n",
      "[epoch 9513]: training loss: 757.071106, consuming time:61.4058 s\n",
      "[epoch 9514]: training loss: 1112.241089, consuming time:61.3695 s\n",
      "[epoch 9515]: training loss: 1002.988770, consuming time:61.5135 s\n",
      "[epoch 9516]: training loss: 919.939697, consuming time:61.3813 s\n",
      "[epoch 9517]: training loss: 755.104858, consuming time:61.4079 s\n",
      "[epoch 9518]: training loss: 753.145874, consuming time:61.4480 s\n",
      "[epoch 9519]: training loss: 1085.478149, consuming time:61.5245 s\n",
      "[epoch 9520]: training loss: 814.816650, consuming time:61.2432 s\n",
      "[epoch 9521]: training loss: 902.871277, consuming time:61.3078 s\n",
      "[epoch 9522]: training loss: 827.875122, consuming time:61.5681 s\n",
      "[epoch 9523]: training loss: 973.743896, consuming time:61.3210 s\n",
      "[epoch 9524]: training loss: 811.777466, consuming time:61.4641 s\n",
      "[epoch 9525]: training loss: 934.332642, consuming time:61.1309 s\n",
      "[epoch 9526]: training loss: 1231.292480, consuming time:61.7124 s\n",
      "[epoch 9527]: training loss: 879.713867, consuming time:61.5288 s\n",
      "[epoch 9528]: training loss: 1021.632202, consuming time:61.5529 s\n",
      "[epoch 9529]: training loss: 690.220093, consuming time:61.2300 s\n",
      "[epoch 9530]: training loss: 956.182007, consuming time:61.3644 s\n",
      "[epoch 9531]: training loss: 958.630615, consuming time:61.4031 s\n",
      "[epoch 9532]: training loss: 1006.333618, consuming time:61.5519 s\n",
      "[epoch 9533]: training loss: 1015.947693, consuming time:61.4766 s\n",
      "[epoch 9534]: training loss: 876.172791, consuming time:61.2032 s\n",
      "[epoch 9535]: training loss: 786.061279, consuming time:61.2813 s\n",
      "[epoch 9536]: training loss: 796.687744, consuming time:61.2637 s\n",
      "[epoch 9537]: training loss: 924.907043, consuming time:61.6809 s\n",
      "[epoch 9538]: training loss: 927.132446, consuming time:61.4021 s\n",
      "[epoch 9539]: training loss: 947.300659, consuming time:61.4337 s\n",
      "[epoch 9540]: training loss: 1001.896423, consuming time:61.4054 s\n",
      "[epoch 9541]: training loss: 652.111938, consuming time:61.6774 s\n",
      "[epoch 9542]: training loss: 869.943970, consuming time:61.4336 s\n",
      "[epoch 9543]: training loss: 1069.595947, consuming time:61.4704 s\n",
      "[epoch 9544]: training loss: 921.973999, consuming time:61.6610 s\n",
      "[epoch 9545]: training loss: 985.302490, consuming time:61.8198 s\n",
      "[epoch 9546]: training loss: 800.316895, consuming time:61.2560 s\n",
      "[epoch 9547]: training loss: 985.301025, consuming time:61.5487 s\n",
      "[epoch 9548]: training loss: 940.908936, consuming time:61.6962 s\n",
      "[epoch 9549]: training loss: 964.334229, consuming time:61.4328 s\n",
      "[epoch 9550]: training loss: 1054.266846, consuming time:61.4264 s\n",
      "[epoch 9551]: training loss: 1016.885254, consuming time:61.6930 s\n",
      "[epoch 9552]: training loss: 1144.112305, consuming time:61.4039 s\n",
      "[epoch 9553]: training loss: 723.260681, consuming time:61.5132 s\n",
      "[epoch 9554]: training loss: 910.139282, consuming time:61.5134 s\n",
      "[epoch 9555]: training loss: 723.676819, consuming time:61.5547 s\n",
      "[epoch 9556]: training loss: 1078.278809, consuming time:61.6945 s\n",
      "[epoch 9557]: training loss: 827.381104, consuming time:61.3937 s\n",
      "[epoch 9558]: training loss: 818.834717, consuming time:61.4139 s\n",
      "[epoch 9559]: training loss: 731.875916, consuming time:61.3452 s\n",
      "[epoch 9560]: training loss: 1072.854736, consuming time:61.4525 s\n",
      "[epoch 9561]: training loss: 1481.050293, consuming time:61.3821 s\n",
      "[epoch 9562]: training loss: 1000.561035, consuming time:61.5145 s\n",
      "[epoch 9563]: training loss: 969.319336, consuming time:61.4075 s\n",
      "[epoch 9564]: training loss: 767.086792, consuming time:61.2754 s\n",
      "[epoch 9565]: training loss: 941.753052, consuming time:61.3198 s\n",
      "[epoch 9566]: training loss: 935.791138, consuming time:61.6821 s\n",
      "[epoch 9567]: training loss: 825.528931, consuming time:61.5920 s\n",
      "[epoch 9568]: training loss: 794.932495, consuming time:61.3044 s\n",
      "[epoch 9569]: training loss: 859.511475, consuming time:61.1140 s\n",
      "[epoch 9570]: training loss: 1064.829834, consuming time:61.0342 s\n",
      "[epoch 9571]: training loss: 980.216125, consuming time:61.1974 s\n",
      "[epoch 9572]: training loss: 1073.889771, consuming time:61.1490 s\n",
      "[epoch 9573]: training loss: 1012.858704, consuming time:61.3877 s\n",
      "[epoch 9574]: training loss: 1093.259399, consuming time:61.3920 s\n",
      "[epoch 9575]: training loss: 876.306641, consuming time:61.2339 s\n",
      "[epoch 9576]: training loss: 848.019104, consuming time:61.2787 s\n",
      "[epoch 9577]: training loss: 1027.214111, consuming time:60.9485 s\n",
      "[epoch 9578]: training loss: 856.733582, consuming time:61.1359 s\n",
      "[epoch 9579]: training loss: 1013.846924, consuming time:61.2329 s\n",
      "[epoch 9580]: training loss: 1013.303162, consuming time:61.0517 s\n",
      "[epoch 9581]: training loss: 716.784546, consuming time:61.5475 s\n",
      "[epoch 9582]: training loss: 1086.046631, consuming time:61.3814 s\n",
      "[epoch 9583]: training loss: 846.379883, consuming time:61.0156 s\n",
      "[epoch 9584]: training loss: 1076.963135, consuming time:61.2469 s\n",
      "[epoch 9585]: training loss: 975.460693, consuming time:61.2755 s\n",
      "[epoch 9586]: training loss: 921.690674, consuming time:61.5118 s\n",
      "[epoch 9587]: training loss: 1012.353577, consuming time:61.2038 s\n",
      "[epoch 9588]: training loss: 788.359619, consuming time:61.3509 s\n",
      "[epoch 9589]: training loss: 904.138794, consuming time:61.0509 s\n",
      "[epoch 9590]: training loss: 819.771179, consuming time:61.4592 s\n",
      "[epoch 9591]: training loss: 641.135254, consuming time:61.3946 s\n",
      "[epoch 9592]: training loss: 873.674683, consuming time:61.3285 s\n",
      "[epoch 9593]: training loss: 877.158936, consuming time:61.0825 s\n",
      "[epoch 9594]: training loss: 1162.033203, consuming time:61.2042 s\n",
      "[epoch 9595]: training loss: 829.625854, consuming time:61.3031 s\n",
      "[epoch 9596]: training loss: 655.591919, consuming time:61.3893 s\n",
      "[epoch 9597]: training loss: 1027.064453, consuming time:61.0750 s\n",
      "[epoch 9598]: training loss: 1152.711792, consuming time:61.3592 s\n",
      "[epoch 9599]: training loss: 870.221863, consuming time:61.1527 s\n",
      "[epoch 9600]: training loss: 1140.392700, consuming time:61.4671 s\n",
      "[epoch 9601]: training loss: 807.866943, consuming time:61.2592 s\n",
      "[epoch 9602]: training loss: 1155.426270, consuming time:61.1528 s\n",
      "[epoch 9603]: training loss: 956.222595, consuming time:61.1505 s\n",
      "[epoch 9604]: training loss: 682.296997, consuming time:61.2792 s\n",
      "[epoch 9605]: training loss: 715.955200, consuming time:61.5196 s\n",
      "[epoch 9606]: training loss: 1016.210388, consuming time:61.4608 s\n",
      "[epoch 9607]: training loss: 880.057007, consuming time:61.2645 s\n",
      "[epoch 9608]: training loss: 1134.823975, consuming time:61.2233 s\n",
      "[epoch 9609]: training loss: 861.590332, consuming time:61.3302 s\n",
      "[epoch 9610]: training loss: 945.946167, consuming time:61.5112 s\n",
      "[epoch 9611]: training loss: 819.363037, consuming time:61.3257 s\n",
      "[epoch 9612]: training loss: 1039.130371, consuming time:60.9715 s\n",
      "[epoch 9613]: training loss: 742.556274, consuming time:61.2821 s\n",
      "[epoch 9614]: training loss: 782.852295, consuming time:61.2544 s\n",
      "[epoch 9615]: training loss: 889.419312, consuming time:61.1851 s\n",
      "[epoch 9616]: training loss: 976.010254, consuming time:61.1184 s\n",
      "[epoch 9617]: training loss: 856.486938, consuming time:61.2643 s\n",
      "[epoch 9618]: training loss: 930.028259, consuming time:61.0076 s\n",
      "[epoch 9619]: training loss: 712.782104, consuming time:61.3865 s\n",
      "[epoch 9620]: training loss: 1290.602295, consuming time:61.2379 s\n",
      "[epoch 9621]: training loss: 942.564087, consuming time:61.2697 s\n",
      "[epoch 9622]: training loss: 893.504517, consuming time:61.1034 s\n",
      "[epoch 9623]: training loss: 662.302124, consuming time:61.1696 s\n",
      "[epoch 9624]: training loss: 969.683105, consuming time:61.1561 s\n",
      "[epoch 9625]: training loss: 701.186035, consuming time:61.2245 s\n",
      "[epoch 9626]: training loss: 1001.461182, consuming time:61.5074 s\n",
      "[epoch 9627]: training loss: 781.666504, consuming time:61.7144 s\n",
      "[epoch 9628]: training loss: 1161.771362, consuming time:61.5972 s\n",
      "[epoch 9629]: training loss: 753.844910, consuming time:61.4363 s\n",
      "[epoch 9630]: training loss: 697.417480, consuming time:61.2699 s\n",
      "[epoch 9631]: training loss: 1298.729248, consuming time:61.6681 s\n",
      "[epoch 9632]: training loss: 707.124512, consuming time:61.7162 s\n",
      "[epoch 9633]: training loss: 1088.980103, consuming time:62.6771 s\n",
      "[epoch 9634]: training loss: 882.035889, consuming time:60.2701 s\n",
      "[epoch 9635]: training loss: 738.471924, consuming time:59.9635 s\n",
      "[epoch 9636]: training loss: 1041.048584, consuming time:59.8122 s\n",
      "[epoch 9637]: training loss: 999.778809, consuming time:60.3071 s\n",
      "[epoch 9638]: training loss: 912.099060, consuming time:60.6930 s\n",
      "[epoch 9639]: training loss: 1365.376709, consuming time:60.6787 s\n",
      "[epoch 9640]: training loss: 859.800049, consuming time:60.8259 s\n",
      "[epoch 9641]: training loss: 719.075134, consuming time:61.1342 s\n",
      "[epoch 9642]: training loss: 816.024170, consuming time:61.4906 s\n",
      "[epoch 9643]: training loss: 871.783691, consuming time:62.0207 s\n",
      "[epoch 9644]: training loss: 1208.730225, consuming time:61.3448 s\n",
      "[epoch 9645]: training loss: 826.773804, consuming time:62.6657 s\n",
      "[epoch 9646]: training loss: 987.188721, consuming time:61.9070 s\n",
      "[epoch 9647]: training loss: 889.561096, consuming time:61.7179 s\n",
      "[epoch 9648]: training loss: 763.069153, consuming time:61.8219 s\n",
      "[epoch 9649]: training loss: 913.516724, consuming time:61.2859 s\n",
      "[epoch 9650]: training loss: 862.273193, consuming time:61.4142 s\n",
      "[epoch 9651]: training loss: 885.256592, consuming time:61.3364 s\n",
      "[epoch 9652]: training loss: 868.610596, consuming time:61.3199 s\n",
      "[epoch 9653]: training loss: 961.566162, consuming time:61.0561 s\n",
      "[epoch 9654]: training loss: 748.790894, consuming time:61.6173 s\n",
      "[epoch 9655]: training loss: 867.981689, consuming time:61.5050 s\n",
      "[epoch 9656]: training loss: 827.974854, consuming time:61.7201 s\n",
      "[epoch 9657]: training loss: 937.153442, consuming time:61.0532 s\n",
      "[epoch 9658]: training loss: 1014.723877, consuming time:61.5843 s\n",
      "[epoch 9659]: training loss: 721.033203, consuming time:61.1032 s\n",
      "[epoch 9660]: training loss: 804.995361, consuming time:62.1495 s\n",
      "[epoch 9661]: training loss: 891.897339, consuming time:61.2761 s\n",
      "[epoch 9662]: training loss: 735.001709, consuming time:61.5953 s\n",
      "[epoch 9663]: training loss: 966.114380, consuming time:62.0121 s\n",
      "[epoch 9664]: training loss: 987.803589, consuming time:61.7719 s\n",
      "[epoch 9665]: training loss: 774.905701, consuming time:62.5546 s\n",
      "[epoch 9666]: training loss: 919.295349, consuming time:61.7099 s\n",
      "[epoch 9667]: training loss: 1139.320801, consuming time:61.5064 s\n",
      "[epoch 9668]: training loss: 978.031006, consuming time:61.2604 s\n",
      "[epoch 9669]: training loss: 1036.999268, consuming time:61.3870 s\n",
      "[epoch 9670]: training loss: 862.598389, consuming time:61.7772 s\n",
      "[epoch 9671]: training loss: 1193.152222, consuming time:61.7625 s\n",
      "[epoch 9672]: training loss: 883.498962, consuming time:61.6983 s\n",
      "[epoch 9673]: training loss: 878.477234, consuming time:62.0024 s\n",
      "[epoch 9674]: training loss: 1082.938232, consuming time:61.5659 s\n",
      "[epoch 9675]: training loss: 938.033203, consuming time:61.9179 s\n",
      "[epoch 9676]: training loss: 1280.042236, consuming time:61.6945 s\n",
      "[epoch 9677]: training loss: 1014.139404, consuming time:61.8499 s\n",
      "[epoch 9678]: training loss: 808.159790, consuming time:61.9488 s\n",
      "[epoch 9679]: training loss: 688.295898, consuming time:62.0753 s\n",
      "[epoch 9680]: training loss: 725.035278, consuming time:62.1612 s\n",
      "[epoch 9681]: training loss: 995.025024, consuming time:61.6482 s\n",
      "[epoch 9682]: training loss: 999.485596, consuming time:61.6058 s\n",
      "[epoch 9683]: training loss: 879.827271, consuming time:61.5383 s\n",
      "[epoch 9684]: training loss: 948.656433, consuming time:61.4060 s\n",
      "[epoch 9685]: training loss: 775.145264, consuming time:61.6577 s\n",
      "[epoch 9686]: training loss: 837.286621, consuming time:61.8554 s\n",
      "[epoch 9687]: training loss: 873.161377, consuming time:61.8322 s\n",
      "[epoch 9688]: training loss: 949.173950, consuming time:61.9735 s\n",
      "[epoch 9689]: training loss: 846.657593, consuming time:61.8292 s\n",
      "[epoch 9690]: training loss: 892.824707, consuming time:61.7449 s\n",
      "[epoch 9691]: training loss: 970.787231, consuming time:61.8583 s\n",
      "[epoch 9692]: training loss: 1005.442627, consuming time:61.9471 s\n",
      "[epoch 9693]: training loss: 1043.031006, consuming time:61.9753 s\n",
      "[epoch 9694]: training loss: 803.422119, consuming time:61.8338 s\n",
      "[epoch 9695]: training loss: 737.382385, consuming time:61.8827 s\n",
      "[epoch 9696]: training loss: 1149.793213, consuming time:61.9601 s\n",
      "[epoch 9697]: training loss: 1047.243164, consuming time:62.0866 s\n",
      "[epoch 9698]: training loss: 947.544495, consuming time:62.0115 s\n",
      "[epoch 9699]: training loss: 902.678101, consuming time:61.5586 s\n",
      "[epoch 9700]: training loss: 1111.861938, consuming time:61.8383 s\n",
      "[epoch 9701]: training loss: 777.034424, consuming time:62.0150 s\n",
      "[epoch 9702]: training loss: 934.742920, consuming time:62.3420 s\n",
      "[epoch 9703]: training loss: 804.285156, consuming time:63.7955 s\n",
      "[epoch 9704]: training loss: 842.606812, consuming time:61.5846 s\n",
      "[epoch 9705]: training loss: 1107.907227, consuming time:61.1532 s\n",
      "[epoch 9706]: training loss: 913.278137, consuming time:61.8519 s\n",
      "[epoch 9707]: training loss: 951.201538, consuming time:61.8881 s\n",
      "[epoch 9708]: training loss: 1024.655640, consuming time:61.5637 s\n",
      "[epoch 9709]: training loss: 1100.267822, consuming time:61.4886 s\n",
      "[epoch 9710]: training loss: 918.617798, consuming time:61.2832 s\n",
      "[epoch 9711]: training loss: 760.668823, consuming time:61.2006 s\n",
      "[epoch 9712]: training loss: 1097.430298, consuming time:60.7724 s\n",
      "[epoch 9713]: training loss: 1001.927002, consuming time:61.2709 s\n",
      "[epoch 9714]: training loss: 989.893433, consuming time:61.1377 s\n",
      "[epoch 9715]: training loss: 1075.784912, consuming time:60.8166 s\n",
      "[epoch 9716]: training loss: 954.360596, consuming time:60.5185 s\n",
      "[epoch 9717]: training loss: 1047.011841, consuming time:60.6168 s\n",
      "[epoch 9718]: training loss: 992.933594, consuming time:60.8033 s\n",
      "[epoch 9719]: training loss: 865.521606, consuming time:61.7112 s\n",
      "[epoch 9720]: training loss: 1081.047729, consuming time:64.9114 s\n",
      "[epoch 9721]: training loss: 1110.411865, consuming time:63.6456 s\n",
      "[epoch 9722]: training loss: 897.340454, consuming time:63.4433 s\n",
      "[epoch 9723]: training loss: 1230.060547, consuming time:63.7243 s\n",
      "[epoch 9724]: training loss: 997.656311, consuming time:64.2820 s\n",
      "[epoch 9725]: training loss: 828.183105, consuming time:60.7360 s\n",
      "[epoch 9726]: training loss: 926.999390, consuming time:61.1332 s\n",
      "[epoch 9727]: training loss: 924.508606, consuming time:60.8352 s\n",
      "[epoch 9728]: training loss: 814.964478, consuming time:60.8191 s\n",
      "[epoch 9729]: training loss: 959.106201, consuming time:61.2865 s\n",
      "[epoch 9730]: training loss: 1046.617188, consuming time:60.4619 s\n",
      "[epoch 9731]: training loss: 904.352661, consuming time:60.6363 s\n",
      "[epoch 9732]: training loss: 1040.202393, consuming time:62.5167 s\n",
      "[epoch 9733]: training loss: 788.471436, consuming time:62.6061 s\n",
      "[epoch 9734]: training loss: 902.167603, consuming time:62.1740 s\n",
      "[epoch 9735]: training loss: 1197.982910, consuming time:61.9672 s\n",
      "[epoch 9736]: training loss: 850.891296, consuming time:61.8741 s\n",
      "[epoch 9737]: training loss: 918.886169, consuming time:62.0670 s\n",
      "[epoch 9738]: training loss: 1082.469482, consuming time:62.1050 s\n",
      "[epoch 9739]: training loss: 865.870728, consuming time:62.0206 s\n",
      "[epoch 9740]: training loss: 1048.321289, consuming time:61.8599 s\n",
      "[epoch 9741]: training loss: 922.955322, consuming time:61.6625 s\n",
      "[epoch 9742]: training loss: 964.646240, consuming time:61.6518 s\n",
      "[epoch 9743]: training loss: 1080.212158, consuming time:61.5842 s\n",
      "[epoch 9744]: training loss: 1232.762329, consuming time:62.0374 s\n",
      "[epoch 9745]: training loss: 980.299255, consuming time:62.4101 s\n",
      "[epoch 9746]: training loss: 768.912720, consuming time:61.7642 s\n",
      "[epoch 9747]: training loss: 987.561218, consuming time:61.8319 s\n",
      "[epoch 9748]: training loss: 882.146606, consuming time:61.6469 s\n",
      "[epoch 9749]: training loss: 577.053772, consuming time:61.9096 s\n",
      "[epoch 9750]: training loss: 929.061523, consuming time:62.0646 s\n",
      "[epoch 9751]: training loss: 1148.252808, consuming time:61.9251 s\n",
      "[epoch 9752]: training loss: 880.213013, consuming time:61.8539 s\n",
      "[epoch 9753]: training loss: 1071.533447, consuming time:61.9352 s\n",
      "[epoch 9754]: training loss: 759.688599, consuming time:61.7970 s\n",
      "[epoch 9755]: training loss: 1124.418579, consuming time:61.9830 s\n",
      "[epoch 9756]: training loss: 890.092346, consuming time:61.8955 s\n",
      "[epoch 9757]: training loss: 832.931030, consuming time:61.7449 s\n",
      "[epoch 9758]: training loss: 1151.942749, consuming time:62.0553 s\n",
      "[epoch 9759]: training loss: 757.592896, consuming time:61.9484 s\n",
      "[epoch 9760]: training loss: 919.026489, consuming time:62.2446 s\n",
      "[epoch 9761]: training loss: 796.195129, consuming time:61.7012 s\n",
      "[epoch 9762]: training loss: 1116.355835, consuming time:65.2937 s\n",
      "[epoch 9763]: training loss: 864.919128, consuming time:63.3998 s\n",
      "[epoch 9764]: training loss: 687.806885, consuming time:63.5966 s\n",
      "[epoch 9765]: training loss: 837.440613, consuming time:65.0666 s\n",
      "[epoch 9766]: training loss: 760.148438, consuming time:64.6498 s\n",
      "[epoch 9767]: training loss: 808.991943, consuming time:64.1554 s\n",
      "[epoch 9768]: training loss: 854.128784, consuming time:65.5076 s\n",
      "[epoch 9769]: training loss: 839.917847, consuming time:63.3922 s\n",
      "[epoch 9770]: training loss: 727.335449, consuming time:63.6662 s\n",
      "[epoch 9771]: training loss: 1085.038574, consuming time:64.3092 s\n",
      "[epoch 9772]: training loss: 1019.376770, consuming time:67.3232 s\n",
      "[epoch 9773]: training loss: 980.187805, consuming time:66.1143 s\n",
      "[epoch 9774]: training loss: 857.651245, consuming time:64.8898 s\n",
      "[epoch 9775]: training loss: 556.538940, consuming time:64.1023 s\n",
      "[epoch 9776]: training loss: 864.707031, consuming time:64.8583 s\n",
      "[epoch 9777]: training loss: 967.333923, consuming time:64.6483 s\n",
      "[epoch 9778]: training loss: 908.940308, consuming time:61.9856 s\n",
      "[epoch 9779]: training loss: 949.745605, consuming time:61.5147 s\n",
      "[epoch 9780]: training loss: 778.133545, consuming time:62.2644 s\n",
      "[epoch 9781]: training loss: 841.466614, consuming time:63.1535 s\n",
      "[epoch 9782]: training loss: 1133.098022, consuming time:62.1835 s\n",
      "[epoch 9783]: training loss: 1020.756287, consuming time:61.7253 s\n",
      "[epoch 9784]: training loss: 717.469482, consuming time:60.9598 s\n",
      "[epoch 9785]: training loss: 797.778625, consuming time:61.5046 s\n",
      "[epoch 9786]: training loss: 915.483032, consuming time:60.7940 s\n",
      "[epoch 9787]: training loss: 757.072998, consuming time:60.3809 s\n",
      "[epoch 9788]: training loss: 997.679138, consuming time:61.0247 s\n",
      "[epoch 9789]: training loss: 1112.112061, consuming time:60.7963 s\n",
      "[epoch 9790]: training loss: 963.731567, consuming time:61.0550 s\n",
      "[epoch 9791]: training loss: 708.289062, consuming time:61.1701 s\n",
      "[epoch 9792]: training loss: 837.624146, consuming time:61.7966 s\n",
      "[epoch 9793]: training loss: 764.168457, consuming time:61.9045 s\n",
      "[epoch 9794]: training loss: 941.045166, consuming time:61.7643 s\n",
      "[epoch 9795]: training loss: 889.965942, consuming time:61.6634 s\n",
      "[epoch 9796]: training loss: 962.918335, consuming time:61.9285 s\n",
      "[epoch 9797]: training loss: 1034.729004, consuming time:61.6679 s\n",
      "[epoch 9798]: training loss: 971.198120, consuming time:63.2988 s\n",
      "[epoch 9799]: training loss: 1053.161743, consuming time:63.8224 s\n",
      "[epoch 9800]: training loss: 1079.941162, consuming time:62.6292 s\n",
      "[epoch 9801]: training loss: 934.933411, consuming time:62.4428 s\n",
      "[epoch 9802]: training loss: 953.577637, consuming time:62.7920 s\n",
      "[epoch 9803]: training loss: 911.880615, consuming time:65.1067 s\n",
      "[epoch 9804]: training loss: 762.856567, consuming time:63.5844 s\n",
      "[epoch 9805]: training loss: 1054.280640, consuming time:63.9831 s\n",
      "[epoch 9806]: training loss: 838.287903, consuming time:63.5530 s\n",
      "[epoch 9807]: training loss: 734.663757, consuming time:63.9722 s\n",
      "[epoch 9808]: training loss: 1555.359619, consuming time:62.4640 s\n",
      "[epoch 9809]: training loss: 814.250977, consuming time:62.4933 s\n",
      "[epoch 9810]: training loss: 1101.524048, consuming time:62.7198 s\n",
      "[epoch 9811]: training loss: 914.945312, consuming time:63.0860 s\n",
      "[epoch 9812]: training loss: 1042.130859, consuming time:63.5332 s\n",
      "[epoch 9813]: training loss: 1054.903320, consuming time:62.1381 s\n",
      "[epoch 9814]: training loss: 998.356873, consuming time:62.0242 s\n",
      "[epoch 9815]: training loss: 1064.075562, consuming time:61.7146 s\n",
      "[epoch 9816]: training loss: 944.484985, consuming time:61.6828 s\n",
      "[epoch 9817]: training loss: 798.523438, consuming time:61.6242 s\n",
      "[epoch 9818]: training loss: 1096.998291, consuming time:61.6661 s\n",
      "[epoch 9819]: training loss: 806.434143, consuming time:61.8957 s\n",
      "[epoch 9820]: training loss: 925.598633, consuming time:61.6112 s\n",
      "[epoch 9821]: training loss: 1001.147888, consuming time:61.7694 s\n",
      "[epoch 9822]: training loss: 902.125916, consuming time:61.7035 s\n",
      "[epoch 9823]: training loss: 881.832947, consuming time:61.5804 s\n",
      "[epoch 9824]: training loss: 960.644653, consuming time:61.9715 s\n",
      "[epoch 9825]: training loss: 825.969360, consuming time:61.7918 s\n",
      "[epoch 9826]: training loss: 1114.107300, consuming time:61.6639 s\n",
      "[epoch 9827]: training loss: 907.278015, consuming time:61.5204 s\n",
      "[epoch 9828]: training loss: 768.982483, consuming time:61.7106 s\n",
      "[epoch 9829]: training loss: 838.867188, consuming time:61.6086 s\n",
      "[epoch 9830]: training loss: 989.766052, consuming time:61.7503 s\n",
      "[epoch 9831]: training loss: 1051.614014, consuming time:61.8228 s\n",
      "[epoch 9832]: training loss: 651.414368, consuming time:61.7993 s\n",
      "[epoch 9833]: training loss: 843.923950, consuming time:61.7313 s\n",
      "[epoch 9834]: training loss: 849.359497, consuming time:61.7748 s\n",
      "[epoch 9835]: training loss: 890.855591, consuming time:61.6692 s\n",
      "[epoch 9836]: training loss: 963.115112, consuming time:61.7914 s\n",
      "[epoch 9837]: training loss: 989.161926, consuming time:61.7288 s\n",
      "[epoch 9838]: training loss: 725.820557, consuming time:61.5121 s\n",
      "[epoch 9839]: training loss: 854.537537, consuming time:61.5064 s\n",
      "[epoch 9840]: training loss: 825.367310, consuming time:61.6924 s\n",
      "[epoch 9841]: training loss: 1353.820068, consuming time:61.6141 s\n",
      "[epoch 9842]: training loss: 966.354370, consuming time:61.8335 s\n",
      "[epoch 9843]: training loss: 977.099976, consuming time:61.7314 s\n",
      "[epoch 9844]: training loss: 760.985718, consuming time:61.4467 s\n",
      "[epoch 9845]: training loss: 783.943726, consuming time:61.5589 s\n",
      "[epoch 9846]: training loss: 906.258728, consuming time:61.5906 s\n",
      "[epoch 9847]: training loss: 1011.859070, consuming time:61.7830 s\n",
      "[epoch 9848]: training loss: 885.687500, consuming time:61.7357 s\n",
      "[epoch 9849]: training loss: 829.443481, consuming time:61.8417 s\n",
      "[epoch 9850]: training loss: 761.388184, consuming time:61.5989 s\n",
      "[epoch 9851]: training loss: 880.970703, consuming time:61.5118 s\n",
      "[epoch 9852]: training loss: 847.982422, consuming time:61.7603 s\n",
      "[epoch 9853]: training loss: 993.655762, consuming time:61.8338 s\n",
      "[epoch 9854]: training loss: 977.448853, consuming time:61.8710 s\n",
      "[epoch 9855]: training loss: 912.937866, consuming time:61.8010 s\n",
      "[epoch 9856]: training loss: 796.940186, consuming time:61.7514 s\n",
      "[epoch 9857]: training loss: 1003.744751, consuming time:61.7880 s\n",
      "[epoch 9858]: training loss: 733.296509, consuming time:61.8129 s\n",
      "[epoch 9859]: training loss: 1033.417236, consuming time:61.9214 s\n",
      "[epoch 9860]: training loss: 877.117676, consuming time:61.7680 s\n",
      "[epoch 9861]: training loss: 1290.967773, consuming time:61.4775 s\n",
      "[epoch 9862]: training loss: 1166.724487, consuming time:62.1367 s\n",
      "[epoch 9863]: training loss: 830.471680, consuming time:64.2587 s\n",
      "[epoch 9864]: training loss: 858.629272, consuming time:62.8234 s\n",
      "[epoch 9865]: training loss: 1023.988586, consuming time:64.5594 s\n",
      "[epoch 9866]: training loss: 833.541687, consuming time:64.8377 s\n",
      "[epoch 9867]: training loss: 865.673401, consuming time:67.4439 s\n",
      "[epoch 9868]: training loss: 938.839844, consuming time:69.2052 s\n",
      "[epoch 9869]: training loss: 766.724609, consuming time:67.6371 s\n",
      "[epoch 9870]: training loss: 878.940674, consuming time:65.1120 s\n",
      "[epoch 9871]: training loss: 1127.600342, consuming time:65.1866 s\n",
      "[epoch 9872]: training loss: 888.948059, consuming time:70.0495 s\n",
      "[epoch 9873]: training loss: 924.753662, consuming time:65.5470 s\n",
      "[epoch 9874]: training loss: 841.475891, consuming time:64.1530 s\n",
      "[epoch 9875]: training loss: 1039.104248, consuming time:62.7602 s\n",
      "[epoch 9876]: training loss: 955.531921, consuming time:62.4887 s\n",
      "[epoch 9877]: training loss: 787.469116, consuming time:62.7747 s\n",
      "[epoch 9878]: training loss: 997.674561, consuming time:62.8737 s\n",
      "[epoch 9879]: training loss: 821.642883, consuming time:63.8521 s\n",
      "[epoch 9880]: training loss: 973.982056, consuming time:63.7125 s\n",
      "[epoch 9881]: training loss: 828.131531, consuming time:62.5014 s\n",
      "[epoch 9882]: training loss: 878.178101, consuming time:62.5411 s\n",
      "[epoch 9883]: training loss: 876.041992, consuming time:61.5877 s\n",
      "[epoch 9884]: training loss: 963.801208, consuming time:61.6646 s\n",
      "[epoch 9885]: training loss: 729.316956, consuming time:61.3382 s\n",
      "[epoch 9886]: training loss: 823.223694, consuming time:61.6972 s\n",
      "[epoch 9887]: training loss: 900.263367, consuming time:62.4408 s\n",
      "[epoch 9888]: training loss: 827.953430, consuming time:61.6291 s\n",
      "[epoch 9889]: training loss: 926.530823, consuming time:61.5786 s\n",
      "[epoch 9890]: training loss: 803.054443, consuming time:61.4906 s\n",
      "[epoch 9891]: training loss: 784.743774, consuming time:61.5319 s\n",
      "[epoch 9892]: training loss: 911.270508, consuming time:61.5504 s\n",
      "[epoch 9893]: training loss: 1173.309326, consuming time:61.6176 s\n",
      "[epoch 9894]: training loss: 1015.229675, consuming time:63.4988 s\n",
      "[epoch 9895]: training loss: 1078.601685, consuming time:61.7169 s\n",
      "[epoch 9896]: training loss: 897.357178, consuming time:62.1631 s\n",
      "[epoch 9897]: training loss: 993.224121, consuming time:61.5551 s\n",
      "[epoch 9898]: training loss: 1169.069336, consuming time:62.2218 s\n",
      "[epoch 9899]: training loss: 1158.277588, consuming time:62.0556 s\n",
      "[epoch 9900]: training loss: 1077.485840, consuming time:62.3475 s\n",
      "[epoch 9901]: training loss: 1025.496338, consuming time:61.7498 s\n",
      "[epoch 9902]: training loss: 1021.713745, consuming time:61.4999 s\n",
      "[epoch 9903]: training loss: 853.594482, consuming time:62.0496 s\n",
      "[epoch 9904]: training loss: 1217.326050, consuming time:61.9298 s\n",
      "[epoch 9905]: training loss: 923.207153, consuming time:61.9231 s\n",
      "[epoch 9906]: training loss: 1001.348511, consuming time:61.8907 s\n",
      "[epoch 9907]: training loss: 1244.140259, consuming time:61.7778 s\n",
      "[epoch 9908]: training loss: 844.276611, consuming time:61.8583 s\n",
      "[epoch 9909]: training loss: 713.362305, consuming time:62.1626 s\n",
      "[epoch 9910]: training loss: 811.706299, consuming time:62.1846 s\n",
      "[epoch 9911]: training loss: 956.433838, consuming time:61.5880 s\n",
      "[epoch 9912]: training loss: 871.521118, consuming time:61.3925 s\n",
      "[epoch 9913]: training loss: 801.659607, consuming time:61.5949 s\n",
      "[epoch 9914]: training loss: 1284.938477, consuming time:61.7312 s\n",
      "[epoch 9915]: training loss: 1152.406006, consuming time:61.4975 s\n",
      "[epoch 9916]: training loss: 1011.695312, consuming time:61.9356 s\n",
      "[epoch 9917]: training loss: 1039.439941, consuming time:62.0967 s\n",
      "[epoch 9918]: training loss: 836.992065, consuming time:61.9127 s\n",
      "[epoch 9919]: training loss: 1061.794189, consuming time:61.6304 s\n",
      "[epoch 9920]: training loss: 925.484131, consuming time:62.3690 s\n",
      "[epoch 9921]: training loss: 950.813599, consuming time:62.1173 s\n",
      "[epoch 9922]: training loss: 820.814026, consuming time:61.6588 s\n",
      "[epoch 9923]: training loss: 790.974365, consuming time:61.9069 s\n",
      "[epoch 9924]: training loss: 1065.666504, consuming time:61.9908 s\n",
      "[epoch 9925]: training loss: 977.047241, consuming time:61.9846 s\n",
      "[epoch 9926]: training loss: 839.217346, consuming time:62.2379 s\n",
      "[epoch 9927]: training loss: 969.698608, consuming time:62.2937 s\n",
      "[epoch 9928]: training loss: 736.266479, consuming time:62.2062 s\n",
      "[epoch 9929]: training loss: 991.476440, consuming time:61.9240 s\n",
      "[epoch 9930]: training loss: 1140.095947, consuming time:61.7202 s\n",
      "[epoch 9931]: training loss: 820.155762, consuming time:61.0597 s\n",
      "[epoch 9932]: training loss: 1409.229126, consuming time:61.8917 s\n",
      "[epoch 9933]: training loss: 838.742676, consuming time:61.7272 s\n",
      "[epoch 9934]: training loss: 1360.097900, consuming time:61.9858 s\n",
      "[epoch 9935]: training loss: 915.962158, consuming time:62.2240 s\n",
      "[epoch 9936]: training loss: 744.619873, consuming time:61.8561 s\n",
      "[epoch 9937]: training loss: 1151.516846, consuming time:61.8116 s\n",
      "[epoch 9938]: training loss: 915.417847, consuming time:62.5632 s\n",
      "[epoch 9939]: training loss: 801.734009, consuming time:62.0752 s\n",
      "[epoch 9940]: training loss: 701.354004, consuming time:61.9579 s\n",
      "[epoch 9941]: training loss: 1138.286133, consuming time:61.8059 s\n",
      "[epoch 9942]: training loss: 860.600525, consuming time:62.3600 s\n",
      "[epoch 9943]: training loss: 1335.789062, consuming time:62.1941 s\n",
      "[epoch 9944]: training loss: 505.761658, consuming time:62.1588 s\n",
      "[epoch 9945]: training loss: 805.898682, consuming time:61.6517 s\n",
      "[epoch 9946]: training loss: 765.977600, consuming time:62.2472 s\n",
      "[epoch 9947]: training loss: 884.272034, consuming time:62.3911 s\n",
      "[epoch 9948]: training loss: 1140.399902, consuming time:63.1430 s\n",
      "[epoch 9949]: training loss: 1045.480469, consuming time:62.4367 s\n",
      "[epoch 9950]: training loss: 1037.557495, consuming time:62.5809 s\n",
      "[epoch 9951]: training loss: 809.881592, consuming time:62.1635 s\n",
      "[epoch 9952]: training loss: 884.057983, consuming time:61.2148 s\n",
      "[epoch 9953]: training loss: 855.366455, consuming time:61.3157 s\n",
      "[epoch 9954]: training loss: 939.044495, consuming time:62.0979 s\n",
      "[epoch 9955]: training loss: 887.993286, consuming time:62.1906 s\n",
      "[epoch 9956]: training loss: 1049.365356, consuming time:62.4449 s\n",
      "[epoch 9957]: training loss: 811.484375, consuming time:62.0838 s\n",
      "[epoch 9958]: training loss: 1107.034180, consuming time:61.6739 s\n",
      "[epoch 9959]: training loss: 1266.654053, consuming time:61.8884 s\n",
      "[epoch 9960]: training loss: 974.512695, consuming time:61.7312 s\n",
      "[epoch 9961]: training loss: 863.557129, consuming time:62.0317 s\n",
      "[epoch 9962]: training loss: 668.924133, consuming time:62.4531 s\n",
      "[epoch 9963]: training loss: 891.901978, consuming time:61.8984 s\n",
      "[epoch 9964]: training loss: 1147.296143, consuming time:61.8792 s\n",
      "[epoch 9965]: training loss: 781.184570, consuming time:61.6627 s\n",
      "[epoch 9966]: training loss: 961.088928, consuming time:61.9586 s\n",
      "[epoch 9967]: training loss: 875.866943, consuming time:61.5886 s\n",
      "[epoch 9968]: training loss: 1186.422485, consuming time:62.0638 s\n",
      "[epoch 9969]: training loss: 821.903198, consuming time:62.1323 s\n",
      "[epoch 9970]: training loss: 1055.715210, consuming time:61.8197 s\n",
      "[epoch 9971]: training loss: 1136.805176, consuming time:61.8526 s\n",
      "[epoch 9972]: training loss: 1042.915527, consuming time:61.6534 s\n",
      "[epoch 9973]: training loss: 873.359070, consuming time:62.0875 s\n",
      "[epoch 9974]: training loss: 835.880005, consuming time:62.0431 s\n",
      "[epoch 9975]: training loss: 828.301025, consuming time:62.0505 s\n",
      "[epoch 9976]: training loss: 915.044861, consuming time:63.0687 s\n",
      "[epoch 9977]: training loss: 864.112244, consuming time:62.6309 s\n",
      "[epoch 9978]: training loss: 926.118408, consuming time:61.7361 s\n",
      "[epoch 9979]: training loss: 893.786255, consuming time:61.8786 s\n",
      "[epoch 9980]: training loss: 1117.876831, consuming time:61.8364 s\n",
      "[epoch 9981]: training loss: 764.727173, consuming time:62.0454 s\n",
      "[epoch 9982]: training loss: 759.023926, consuming time:61.7774 s\n",
      "[epoch 9983]: training loss: 1245.394287, consuming time:61.9347 s\n",
      "[epoch 9984]: training loss: 719.588379, consuming time:61.8661 s\n",
      "[epoch 9985]: training loss: 969.771790, consuming time:61.7258 s\n",
      "[epoch 9986]: training loss: 548.176392, consuming time:61.9961 s\n",
      "[epoch 9987]: training loss: 1275.802368, consuming time:62.3431 s\n",
      "[epoch 9988]: training loss: 773.773499, consuming time:62.1681 s\n",
      "[epoch 9989]: training loss: 1103.709595, consuming time:62.1808 s\n",
      "[epoch 9990]: training loss: 969.268127, consuming time:62.0081 s\n",
      "[epoch 9991]: training loss: 1085.420532, consuming time:61.9094 s\n",
      "[epoch 9992]: training loss: 871.624268, consuming time:61.7073 s\n",
      "[epoch 9993]: training loss: 785.752686, consuming time:65.9315 s\n",
      "[epoch 9994]: training loss: 1014.724365, consuming time:61.6425 s\n",
      "[epoch 9995]: training loss: 1305.357178, consuming time:60.3725 s\n",
      "[epoch 9996]: training loss: 823.684998, consuming time:60.6492 s\n",
      "[epoch 9997]: training loss: 814.031860, consuming time:60.6482 s\n",
      "[epoch 9998]: training loss: 1162.202881, consuming time:61.2481 s\n",
      "[epoch 9999]: training loss: 634.973999, consuming time:61.6549 s\n",
      "[epoch 10000]: training loss: 944.033997, consuming time:63.4552 s\n"
     ]
    }
   ],
   "source": [
    "# 加载需要的数据\n",
    "data_path = r\"./data/Inputs_mnist_train_64\"\n",
    "label_path = r\"./data/Labels_mnist_train\"\n",
    "mydata = MyDataset(data_path,label_path)\n",
    "bath_size = 8\n",
    "train_iter = data.DataLoader(mydata, batch_size=bath_size, shuffle=True,num_workers=5)\n",
    "# 模型\n",
    "net = nn.Sequential(FirstNet(64,32),SecondNet(32,32),LastNet(128,1)) \n",
    "# 训练参数\n",
    "lr, num_epochs = 0.001, 10000\n",
    "train(net, train_iter, num_epochs, lr, try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(net,'./net/net_10000.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "n1 = torch.load(\"net.pth\")\n",
    "test_iter = data.DataLoader(mydata, batch_size=1, shuffle=False)\n",
    "plt.figure(1)\n",
    "for i,(X,y) in enumerate(test_iter):\n",
    "    n1.to('cpu')\n",
    "    n1.eval()\n",
    "    y_hat = n1(X)\n",
    "    ax = plt.subplot(241+i)\n",
    "    ax.imshow(y[0,0,:,:].detach().numpy(),'gray')\n",
    "    ax.axis('off')\n",
    "    ax = plt.subplot(241+i+4)\n",
    "    ax.imshow(y_hat[0,0,:,:].detach().numpy(),'gray')\n",
    "    ax.axis('off')\n",
    "    if i== 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e975ec227eb6d18a9f767734e5cfce713416b69b247c84ff69398720ad42309"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
